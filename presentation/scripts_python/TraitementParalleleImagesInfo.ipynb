{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (1882, 56, 56), (1882, 1), (1882, 1), (1882, 1), (1882, 1), (1882, 1))\n",
      "('Validation set', (764, 56, 56), (764, 1), (764, 1), (764, 1), (764, 1), (764, 1))\n"
     ]
    }
   ],
   "source": [
    "# Now with tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "pickle_file = 'housingNantesAllInfoFullImages.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_prices_labels = save['train_prices_labels']\n",
    "  train_nbRooms_labels = save['train_nbRooms_labels']\n",
    "  train_surfaceHouse_labels = save['train_surfaceHouse_labels']\n",
    "  train_surfaceLand_labels = save['train_surfaceLand_labels']\n",
    "  train_nbPictures_labels = save['train_nbPictures_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_prices_labels = save['valid_prices_labels']\n",
    "  valid_nbRooms_labels = save['valid_nbRooms_labels']\n",
    "  valid_surfaceHouse_labels = save['valid_surfaceHouse_labels']\n",
    "  valid_surfaceLand_labels = save['valid_surfaceLand_labels']\n",
    "  valid_nbPictures_labels = save['valid_nbPictures_labels']\n",
    "\n",
    "\n",
    "\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_prices_labels.shape,train_nbRooms_labels.shape,train_surfaceHouse_labels.shape,train_surfaceLand_labels.shape,train_nbPictures_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_prices_labels.shape,valid_nbRooms_labels.shape,valid_surfaceHouse_labels.shape,valid_surfaceLand_labels.shape,valid_nbPictures_labels.shape)\n",
    "\n",
    "image_size = 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# clean prices == 0\n",
    "\n",
    "price_equals_zero_list = []\n",
    "\n",
    "for i,price in enumerate(train_prices_labels):\n",
    "    if price == 0:\n",
    "        price_equals_zero_list.append(i)  \n",
    "\n",
    "print len(price_equals_zero_list)   \n",
    "\n",
    "new_dim = train_prices_labels.shape[0]-len(price_equals_zero_list)\n",
    "\n",
    "train_labels_prices_no_zeros = np.zeros((new_dim, 1))\n",
    "\n",
    "train_labels_nbRooms_no_zeros = np.zeros((new_dim, 1))\n",
    "train_labels_surfaceHouse_no_zeros = np.zeros((new_dim, 1))\n",
    "train_labels_surfaceLand_no_zeros = np.zeros((new_dim, 1))\n",
    "train_labels_nbPictures_no_zeros = np.zeros((new_dim, 1))\n",
    "\n",
    "train_images_no_zeros = np.zeros((new_dim, image_size,image_size))\n",
    "\n",
    "j = 0\n",
    "for i,price in enumerate(train_prices_labels):\n",
    "    if price != 0:\n",
    "        train_labels_prices_no_zeros[j][0] = price  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,image in enumerate(train_dataset):\n",
    "    if i not in price_equals_zero_list:\n",
    "        train_images_no_zeros[j,:,:] = image  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,nbRooms in enumerate(train_nbRooms_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        train_labels_nbRooms_no_zeros[j][0] = nbRooms  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceHouse in enumerate(train_surfaceHouse_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        train_labels_surfaceHouse_no_zeros[j][0] = surfaceHouse  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceLand in enumerate(train_surfaceLand_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        train_labels_surfaceLand_no_zeros[j][0] = surfaceLand  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,nbPictures in enumerate(train_nbPictures_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        train_labels_nbPictures_no_zeros[j][0] = nbPictures  \n",
    "        j += 1\n",
    "        \n",
    "# for validation:\n",
    "\n",
    "price_equals_zero_list = []\n",
    "\n",
    "for i,price in enumerate(valid_prices_labels):\n",
    "    if price == 0:\n",
    "        price_equals_zero_list.append(i)  \n",
    "\n",
    "print len(price_equals_zero_list)   \n",
    "\n",
    "new_dim = valid_prices_labels.shape[0]-len(price_equals_zero_list) \n",
    "\n",
    "valid_labels_prices_no_zeros = np.zeros((new_dim, 1))\n",
    "valid_labels_nbRooms_no_zeros = np.zeros((new_dim, 1))\n",
    "valid_labels_surfaceHouse_no_zeros = np.zeros((new_dim, 1))\n",
    "valid_labels_surfaceLand_no_zeros = np.zeros((new_dim, 1))\n",
    "valid_labels_nbPictures_no_zeros = np.zeros((new_dim, 1))\n",
    "\n",
    "valid_images_no_zeros = np.zeros((new_dim, image_size,image_size))\n",
    "\n",
    "j = 0\n",
    "for i,price in enumerate(valid_prices_labels):\n",
    "    if price != 0:\n",
    "        valid_labels_prices_no_zeros[j][0] = price  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,image in enumerate(valid_dataset):\n",
    "    if i not in price_equals_zero_list:\n",
    "        valid_images_no_zeros[j,:,:] = image  \n",
    "        j += 1    \n",
    "\n",
    "j = 0        \n",
    "for i,nbRooms in enumerate(valid_nbRooms_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        valid_labels_nbRooms_no_zeros[j][0] = nbRooms  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceHouse in enumerate(valid_surfaceHouse_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        valid_labels_surfaceHouse_no_zeros[j][0] = surfaceHouse  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceLand in enumerate(valid_surfaceLand_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        valid_labels_surfaceLand_no_zeros[j][0] = surfaceLand  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,nbPictures in enumerate(valid_nbPictures_labels):\n",
    "    if i not in price_equals_zero_list:\n",
    "        valid_labels_nbPictures_no_zeros[j][0] = nbPictures  \n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# clean surfaceHouse == 0\n",
    "\n",
    "surface_equals_zero_list = []\n",
    "\n",
    "for i,surface in enumerate(train_labels_surfaceHouse_no_zeros):\n",
    "    if surface == float(0):\n",
    "        surface_equals_zero_list.append(i)  \n",
    "\n",
    "print len(surface_equals_zero_list)   \n",
    "\n",
    "new_dim = train_labels_prices_no_zeros.shape[0]-len(surface_equals_zero_list) \n",
    "\n",
    "train_labels_prices_surface_clean = np.zeros((new_dim, 1))\n",
    "\n",
    "train_labels_nbRooms_surface_clean = np.zeros((new_dim, 1))\n",
    "train_labels_surfaceHouse_surface_clean = np.zeros((new_dim, 1))\n",
    "train_labels_surfaceLand_surface_clean = np.zeros((new_dim, 1))\n",
    "train_labels_nbPictures_surface_clean = np.zeros((new_dim, 1))\n",
    "\n",
    "train_images_surface_clean = np.zeros((new_dim, image_size,image_size))\n",
    "\n",
    "j = 0\n",
    "for i,price in enumerate(train_labels_prices_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_labels_prices_surface_clean[j][0] = price  \n",
    "        j += 1\n",
    "        \n",
    "j = 0\n",
    "for i,image in enumerate(train_images_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_images_surface_clean[j,:,:] = image  \n",
    "        j += 1    \n",
    "        \n",
    "j = 0\n",
    "for i,nbRooms in enumerate(train_labels_nbRooms_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_labels_nbRooms_surface_clean[j][0] = nbRooms  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceHouse in enumerate(train_labels_surfaceHouse_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_labels_surfaceHouse_surface_clean[j][0] = surfaceHouse  \n",
    "        j += 1\n",
    "        \n",
    "j = 0\n",
    "for i,surfaceLand in enumerate(train_labels_surfaceLand_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_labels_surfaceLand_surface_clean[j][0] = surfaceLand  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,nbPictures in enumerate(train_labels_nbPictures_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        train_labels_nbPictures_surface_clean[j][0] = nbPictures  \n",
    "        j += 1 \n",
    "\n",
    "# validation\n",
    "surface_equals_zero_list = []\n",
    "\n",
    "for i,surface in enumerate(valid_labels_surfaceHouse_no_zeros):\n",
    "    if surface == float(0):\n",
    "        surface_equals_zero_list.append(i)  \n",
    "\n",
    "print len(surface_equals_zero_list)   \n",
    "\n",
    "new_dim = valid_labels_prices_no_zeros.shape[0]-len(surface_equals_zero_list) \n",
    "\n",
    "valid_labels_prices_surface_clean = np.zeros((new_dim, 1))\n",
    "\n",
    "valid_labels_nbRooms_surface_clean = np.zeros((new_dim, 1))\n",
    "valid_labels_surfaceHouse_surface_clean = np.zeros((new_dim, 1))\n",
    "valid_labels_surfaceLand_surface_clean = np.zeros((new_dim, 1))\n",
    "valid_labels_nbPictures_surface_clean = np.zeros((new_dim, 1))\n",
    "\n",
    "valid_images_surface_clean = np.zeros((new_dim, image_size,image_size))\n",
    "\n",
    "\n",
    "j = 0\n",
    "for i,price in enumerate(valid_labels_prices_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_labels_prices_surface_clean[j][0] = price  \n",
    "        j += 1\n",
    "        \n",
    "j = 0\n",
    "for i,image in enumerate(valid_images_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_images_surface_clean[j,:,:] = image  \n",
    "        j += 1    \n",
    "        \n",
    "j = 0\n",
    "for i,nbRooms in enumerate(valid_labels_nbRooms_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_labels_nbRooms_surface_clean[j][0] = nbRooms  \n",
    "        j += 1\n",
    "\n",
    "j = 0\n",
    "for i,surfaceHouse in enumerate(valid_labels_surfaceHouse_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_labels_surfaceHouse_surface_clean[j][0] = surfaceHouse  \n",
    "        j += 1\n",
    "        \n",
    "j = 0\n",
    "for i,surfaceLand in enumerate(valid_labels_surfaceLand_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_labels_surfaceLand_surface_clean[j][0] = surfaceLand  \n",
    "        j += 1\n",
    "        \n",
    "j = 0\n",
    "for i,nbPictures in enumerate(valid_labels_nbPictures_no_zeros):\n",
    "    if i not in surface_equals_zero_list:\n",
    "        valid_labels_nbPictures_surface_clean[j][0] = nbPictures  \n",
    "        j += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reformat prices to have then between 0 and 1\n",
    "\n",
    "max_value = max(max(train_labels_prices_surface_clean),max(valid_labels_prices_surface_clean))\n",
    "\n",
    "train_prices_labels = (train_labels_prices_surface_clean - max_value / 2) / max_value\n",
    "\n",
    "valid_prices_labels = (valid_labels_prices_surface_clean - max_value / 2) / max_value\n",
    "\n",
    "\n",
    "train_nbRooms_labels = train_labels_nbRooms_surface_clean\n",
    "train_surfaceHouse_labels = train_labels_surfaceHouse_surface_clean\n",
    "train_surfaceLand_labels = train_labels_surfaceLand_surface_clean\n",
    "train_nbPictures_labels = train_labels_nbPictures_surface_clean\n",
    "valid_nbRooms_labels = valid_labels_nbRooms_surface_clean\n",
    "valid_surfaceHouse_labels = train_labels_surfaceHouse_surface_clean\n",
    "valid_surfaceLand_labels = valid_labels_surfaceLand_surface_clean\n",
    "valid_nbPictures_labels = valid_labels_nbPictures_surface_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "# Separate image tensors and info tensors\n",
    "\n",
    "# reformat images to 2d array 56*56\n",
    "train_images = train_images_surface_clean.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "valid_images = valid_images_surface_clean.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "\n",
    "\n",
    "# build the dataset for complementary info from json files\n",
    "nb_variables = 4\n",
    "\n",
    "train_info = np.zeros((train_images.shape[0],nb_variables))\n",
    "\n",
    "# Reformat the variables to have them between -1 and 1\n",
    "nbRooms_max = max(max(train_nbRooms_labels), max(valid_nbRooms_labels))\n",
    "surface_max = max(max(train_surfaceHouse_labels), max(valid_surfaceHouse_labels))\n",
    "surfaceLand_max = max(max(train_surfaceLand_labels), max(valid_surfaceLand_labels))\n",
    "nbPictures_max = max(max(train_nbPictures_labels), max(valid_nbPictures_labels))\n",
    "\n",
    "for i in range(train_images.shape[0]):\n",
    "        train_info[i][0] = ( train_nbRooms_labels[i] - nbRooms_max /2 ) / nbRooms_max\n",
    "        train_info[i][1] = ( train_surfaceHouse_labels[i] - surface_max /2 ) / surface_max\n",
    "        train_info[i][2] = ( train_surfaceLand_labels[i] - surfaceLand_max /2 ) / surfaceLand_max\n",
    "        train_info[i][3] = ( train_nbPictures_labels[i] - nbPictures_max /2 ) / nbPictures_max\n",
    "\n",
    "    \n",
    "valid_info = np.zeros((valid_images.shape[0],nb_variables))\n",
    "\n",
    "for i in range(valid_images.shape[0]):\n",
    "        valid_info[i][0] = ( valid_nbRooms_labels[i] - nbRooms_max /2 ) / nbRooms_max\n",
    "        valid_info[i][1] = ( valid_surfaceHouse_labels[i] - surface_max /2 ) / surface_max\n",
    "        valid_info[i][2] = ( valid_surfaceLand_labels[i] - surfaceLand_max /2 ) / surfaceLand_max\n",
    "        valid_info[i][3] = ( valid_nbPictures_labels[i] - nbPictures_max /2 ) / nbPictures_max\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parameters to the function graph\n",
    "num_labels= 1\n",
    "dtype = tf.float32\n",
    "\n",
    "batch_size = 40\n",
    "num_hidden_info_labels = 20\n",
    "num_hidden_image_labels = 40\n",
    "\n",
    "regularization_beta = 5e-4\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation into constants that are\n",
    "  # attached to the graph.\n",
    "    tf_train_info = tf.placeholder(dtype, shape=(batch_size,train_info.shape[1]))\n",
    "    tf_train_price = tf.placeholder(dtype, shape=(batch_size,train_prices_labels.shape[1]))\n",
    "    tf_valid_info = tf.placeholder(dtype, shape=[None,train_info.shape[1]])\n",
    "    \n",
    "    tf_train_images = tf.placeholder(dtype, shape=[batch_size,image_size * image_size])\n",
    "    tf_valid_images = tf.placeholder(dtype, shape=[None,image_size * image_size])\n",
    "    \n",
    "  # Variables.\n",
    "    \n",
    "    # info layer\n",
    "    \n",
    "    weights_info_layer = tf.Variable(tf.truncated_normal([nb_variables, num_hidden_info_labels]))\n",
    "    biases_info_layer = tf.Variable(tf.zeros([num_hidden_info_labels]))\n",
    "    info_layer = tf.nn.relu(tf.matmul(tf_train_info , weights_info_layer) + biases_info_layer)\n",
    "    \n",
    "    # image layer\n",
    "    \n",
    "    weights_image_layer = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_image_labels]))\n",
    "    biases_image_layer = tf.Variable(tf.zeros([num_hidden_image_labels]))\n",
    "    image_layer = tf.nn.relu(tf.matmul(tf_train_images , weights_image_layer) + biases_image_layer)\n",
    "        \n",
    "    # merged_layer \n",
    "    \n",
    "    merged_layer = tf.concat([info_layer, image_layer],1)\n",
    "    \n",
    "    # output layer\n",
    "    \n",
    "    weights_output = tf.Variable(tf.truncated_normal([num_hidden_info_labels+num_hidden_image_labels,num_labels]))\n",
    "    biases_output = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    price_estimation = tf.matmul(merged_layer, weights_output) + biases_output\n",
    "    \n",
    "    loss = tf.reduce_sum(tf.square(price_estimation - tf_train_price)) /train_prices_labels.shape[0]\n",
    "  \n",
    "    # L2 regularization\n",
    "    \n",
    "    regularizers = (tf.nn.l2_loss(weights_image_layer) + tf.nn.l2_loss(biases_image_layer) +\n",
    "                   tf.nn.l2_loss(weights_info_layer) + tf.nn.l2_loss(biases_info_layer) +\n",
    "                    tf.nn.l2_loss(weights_output) + tf.nn.l2_loss(biases_output)\n",
    "                   )\n",
    "    \n",
    "    loss = loss + regularization_beta * regularizers\n",
    " \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using AdagradOptimizer.\n",
    "  #  AdagradOptimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(loss)\n",
    "    #optimizer = tf.train.MomentumOptimizer(5, 0.001).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training and validation.\n",
    "  # Predictions\n",
    "    \n",
    "    train_prediction = price_estimation\n",
    "    \n",
    "    info_validation = tf.nn.relu(tf.matmul(tf_valid_info, weights_info_layer) + biases_info_layer)\n",
    "    images_validation = tf.nn.relu(tf.matmul(tf_valid_images, weights_image_layer) + biases_image_layer)\n",
    "    \n",
    "    merged_validation = tf.concat([info_validation, images_validation],1)   \n",
    "    \n",
    "    valid_prediction = tf.matmul(merged_validation, weights_output) + biases_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 282.600616\n",
      "Training accuracy: 6407.0%\n",
      "Validation accuracy: 878.5%\n",
      "Loss at step 1000: 197.355362\n",
      "Training accuracy: 207.0%\n",
      "Validation accuracy: -58.0%\n",
      "Loss at step 2000: 164.115173\n",
      "Training accuracy: 1839.3%\n",
      "Validation accuracy: -42.5%\n",
      "Loss at step 3000: 138.820923\n",
      "Training accuracy: 219.8%\n",
      "Validation accuracy: -66.8%\n",
      "Loss at step 4000: 118.640144\n",
      "Training accuracy: -600.6%\n",
      "Validation accuracy: -78.8%\n",
      "Loss at step 5000: 102.351860\n",
      "Training accuracy: -182.4%\n",
      "Validation accuracy: -54.2%\n",
      "Loss at step 6000: 88.784950\n",
      "Training accuracy: 169.2%\n",
      "Validation accuracy: -54.4%\n",
      "Loss at step 7000: 77.347748\n",
      "Training accuracy: 355.8%\n",
      "Validation accuracy: -50.6%\n",
      "Loss at step 8000: 67.668884\n",
      "Training accuracy: -60.3%\n",
      "Validation accuracy: -28.2%\n",
      "Loss at step 9000: 59.473392\n",
      "Training accuracy: 332.6%\n",
      "Validation accuracy: -11.9%\n",
      "Loss at step 10000: 52.449345\n",
      "Training accuracy: 319.9%\n",
      "Validation accuracy: -3.9%\n",
      "Loss at step 11000: 46.371601\n",
      "Training accuracy: 238.4%\n",
      "Validation accuracy: 8.4%\n",
      "Loss at step 12000: 41.108677\n",
      "Training accuracy: 494.3%\n",
      "Validation accuracy: 13.7%\n",
      "Loss at step 13000: 36.508541\n",
      "Training accuracy: -14.3%\n",
      "Validation accuracy: 13.2%\n",
      "Loss at step 14000: 32.535519\n",
      "Training accuracy: -30.7%\n",
      "Validation accuracy: 20.1%\n",
      "Loss at step 15000: 29.045784\n",
      "Training accuracy: 102.0%\n",
      "Validation accuracy: 23.8%\n",
      "Loss at step 16000: 25.950052\n",
      "Training accuracy: 158.8%\n",
      "Validation accuracy: 26.9%\n",
      "Loss at step 17000: 23.241060\n",
      "Training accuracy: 163.7%\n",
      "Validation accuracy: 31.7%\n",
      "Loss at step 18000: 20.845676\n",
      "Training accuracy: 150.5%\n",
      "Validation accuracy: 36.1%\n",
      "Loss at step 19000: 18.732792\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 41.9%\n",
      "Loss at step 20000: 16.869417\n",
      "Training accuracy: 160.6%\n",
      "Validation accuracy: 45.9%\n",
      "Loss at step 21000: 15.193830\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 48.5%\n",
      "Loss at step 22000: 13.701168\n",
      "Training accuracy: 14.4%\n",
      "Validation accuracy: 51.8%\n",
      "Loss at step 23000: 12.378360\n",
      "Training accuracy: 147.9%\n",
      "Validation accuracy: 51.3%\n",
      "Loss at step 24000: 11.195026\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 55.2%\n",
      "Loss at step 25000: 10.134684\n",
      "Training accuracy: -212.1%\n",
      "Validation accuracy: 56.2%\n",
      "Loss at step 26000: 9.190052\n",
      "Training accuracy: 134.3%\n",
      "Validation accuracy: 58.0%\n",
      "Loss at step 27000: 8.335349\n",
      "Training accuracy: 150.6%\n",
      "Validation accuracy: 61.1%\n",
      "Loss at step 28000: 7.570783\n",
      "Training accuracy: 54.0%\n",
      "Validation accuracy: 62.3%\n",
      "Loss at step 29000: 6.880760\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 64.4%\n",
      "Loss at step 30000: 6.256661\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 67.2%\n",
      "Loss at step 31000: 5.696314\n",
      "Training accuracy: 141.3%\n",
      "Validation accuracy: 69.0%\n",
      "Loss at step 32000: 5.190460\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 69.1%\n",
      "Loss at step 33000: 4.732057\n",
      "Training accuracy: -19.7%\n",
      "Validation accuracy: 69.6%\n",
      "Loss at step 34000: 4.319973\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 71.5%\n",
      "Loss at step 35000: 3.944915\n",
      "Training accuracy: 119.7%\n",
      "Validation accuracy: 71.5%\n",
      "Loss at step 36000: 3.605549\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 72.9%\n",
      "Loss at step 37000: 3.297247\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 73.9%\n",
      "Loss at step 38000: 3.018213\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 39000: 2.764453\n",
      "Training accuracy: 126.7%\n",
      "Validation accuracy: 76.4%\n",
      "Loss at step 40000: 2.534455\n",
      "Training accuracy: 113.5%\n",
      "Validation accuracy: 78.1%\n",
      "Loss at step 41000: 2.323510\n",
      "Training accuracy: 186.8%\n",
      "Validation accuracy: 79.2%\n",
      "Loss at step 42000: 2.131665\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 78.9%\n",
      "Loss at step 43000: 1.958109\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 80.5%\n",
      "Loss at step 44000: 1.799347\n",
      "Training accuracy: 118.1%\n",
      "Validation accuracy: 80.9%\n",
      "Loss at step 45000: 1.653806\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 81.9%\n",
      "Loss at step 46000: 1.521792\n",
      "Training accuracy: 111.3%\n",
      "Validation accuracy: 82.6%\n",
      "Loss at step 47000: 1.400678\n",
      "Training accuracy: 171.0%\n",
      "Validation accuracy: 83.0%\n",
      "Loss at step 48000: 1.290414\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 83.7%\n",
      "Loss at step 49000: 1.190375\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 84.1%\n",
      "Loss at step 50000: 1.097212\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 85.1%\n",
      "Loss at step 51000: 1.012352\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 85.3%\n",
      "Loss at step 52000: 0.935205\n",
      "Training accuracy: 124.4%\n",
      "Validation accuracy: 85.4%\n",
      "Loss at step 53000: 0.864397\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 86.2%\n",
      "Loss at step 54000: 0.799204\n",
      "Training accuracy: -68.5%\n",
      "Validation accuracy: 86.6%\n",
      "Loss at step 55000: 0.739752\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 87.3%\n",
      "Loss at step 56000: 0.685129\n",
      "Training accuracy: 108.2%\n",
      "Validation accuracy: 87.4%\n",
      "Loss at step 57000: 0.634892\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 88.0%\n",
      "Loss at step 58000: 0.588810\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.3%\n",
      "Loss at step 59000: 0.546396\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 60000: 0.507548\n",
      "Training accuracy: 118.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 61000: 0.471576\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 62000: 0.438576\n",
      "Training accuracy: 3.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 63000: 0.408320\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 64000: 0.380572\n",
      "Training accuracy: 111.9%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 65000: 0.354837\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 66000: 0.331357\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 67000: 0.309822\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 68000: 0.289364\n",
      "Training accuracy: 37.1%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 69000: 0.271334\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 70000: 0.253735\n",
      "Training accuracy: 131.8%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 71000: 0.237803\n",
      "Training accuracy: 100.5%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 72000: 0.223353\n",
      "Training accuracy: 17.0%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 73000: 0.209766\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 74000: 0.197397\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 75000: 0.185841\n",
      "Training accuracy: 105.3%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 76000: 0.175416\n",
      "Training accuracy: 132.0%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 77000: 0.165624\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 78000: 0.156469\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 79000: 0.147918\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 80000: 0.140059\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 81000: 0.132908\n",
      "Training accuracy: 112.5%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 82000: 0.126435\n",
      "Training accuracy: 11.9%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 83000: 0.119986\n",
      "Training accuracy: 4.5%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 84000: 0.114307\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 85000: 0.109043\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 86000: 0.104025\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 87000: 0.099395\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 88000: 0.095175\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 89000: 0.091290\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 90000: 0.087491\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 91000: 0.084081\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 92000: 0.080844\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 93000: 0.078011\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 94000: 0.075202\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 95000: 0.072743\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 96000: 0.070363\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 97000: 0.067996\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 95.8%\n",
      "Loss at step 98000: 0.066100\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 99000: 0.064075\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 100000: 0.062050\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 101000: 0.060497\n",
      "Training accuracy: 1.2%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 102000: 0.058770\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 103000: 0.057318\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 104000: 0.055863\n",
      "Training accuracy: 106.7%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 105000: 0.054733\n",
      "Training accuracy: 125.1%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 106000: 0.053547\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 107000: 0.052341\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 108000: 0.051147\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 109000: 0.050108\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 110000: 0.049233\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 111000: 0.048536\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 112000: 0.047460\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 95.7%\n",
      "Loss at step 113000: 0.046703\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 114000: 0.046041\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 115000: 0.045258\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 116000: 0.044606\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 96.5%\n",
      "Loss at step 117000: 0.043932\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 96.8%\n",
      "Loss at step 118000: 0.043461\n",
      "Training accuracy: 127.5%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 119000: 0.042842\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 120000: 0.042263\n",
      "Training accuracy: 30.0%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 121000: 0.041735\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 122000: 0.041320\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 96.5%\n",
      "Loss at step 123000: 0.040866\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 124000: 0.040565\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 125000: 0.040067\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 126000: 0.039653\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 96.8%\n",
      "Loss at step 127000: 0.039358\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 128000: 0.039034\n",
      "Training accuracy: 111.5%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 129000: 0.038533\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 130000: 0.038235\n",
      "Training accuracy: 11.1%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 131000: 0.037939\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 132000: 0.037592\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 133000: 0.037263\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 134000: 0.037085\n",
      "Training accuracy: 112.4%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 135000: 0.036997\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 96.5%\n",
      "Loss at step 136000: 0.036580\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 96.8%\n",
      "Loss at step 137000: 0.036223\n",
      "Training accuracy: 93.4%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 138000: 0.035976\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 139000: 0.035817\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 95.7%\n",
      "Loss at step 140000: 0.035627\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 96.5%\n",
      "Loss at step 141000: 0.035328\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 95.8%\n",
      "Loss at step 142000: 0.035146\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 143000: 0.035010\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 144000: 0.034807\n",
      "Training accuracy: 98.1%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 145000: 0.034609\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 96.5%\n",
      "Loss at step 146000: 0.034356\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 96.8%\n",
      "Loss at step 147000: 0.034311\n",
      "Training accuracy: 99.8%\n",
      "Validation accuracy: 97.0%\n",
      "Loss at step 148000: 0.033995\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 149000: 0.033848\n",
      "Training accuracy: 28.8%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 150000: 0.033684\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 151000: 0.033529\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 96.6%\n",
      "Loss at step 152000: 0.033434\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 153000: 0.033400\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 154000: 0.033084\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 96.6%\n",
      "Loss at step 155000: 0.032976\n",
      "Training accuracy: 54.9%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 156000: 0.032879\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 97.1%\n",
      "Loss at step 157000: 0.032704\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 158000: 0.032531\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 159000: 0.032349\n",
      "Training accuracy: 28.0%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 160000: 0.032307\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 161000: 0.032086\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 95.7%\n",
      "Loss at step 162000: 0.031888\n",
      "Training accuracy: 106.6%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 163000: 0.031839\n",
      "Training accuracy: 108.2%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 164000: 0.031888\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 165000: 0.031582\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 96.9%\n",
      "Loss at step 166000: 0.031362\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 96.8%\n",
      "Loss at step 167000: 0.031258\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 168000: 0.031239\n",
      "Training accuracy: 44.5%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 169000: 0.031121\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 96.4%\n",
      "Loss at step 170000: 0.030971\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 171000: 0.030804\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 172000: 0.030718\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 173000: 0.030778\n",
      "Training accuracy: 93.4%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 174000: 0.030556\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 96.1%\n",
      "Loss at step 175000: 0.030492\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 176000: 0.030407\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 177000: 0.030145\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 95.8%\n",
      "Loss at step 178000: 0.030092\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 179000: 0.029982\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 95.7%\n",
      "Loss at step 180000: 0.029842\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 95.7%\n",
      "Loss at step 181000: 0.029842\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 182000: 0.029832\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 183000: 0.029564\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 184000: 0.029490\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 185000: 0.029466\n",
      "Training accuracy: 93.0%\n",
      "Validation accuracy: 96.3%\n",
      "Loss at step 186000: 0.029311\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 187000: 0.029193\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 188000: 0.029052\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 189000: 0.029064\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 190000: 0.028880\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 191000: 0.028720\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 192000: 0.028675\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 193000: 0.028802\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 194000: 0.028466\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 96.0%\n",
      "Loss at step 195000: 0.028310\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 196000: 0.028245\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 197000: 0.028216\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 198000: 0.028123\n",
      "Training accuracy: 36.7%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 199000: 0.028090\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 200000: 0.027847\n",
      "Training accuracy: 93.0%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 201000: 0.027831\n",
      "Training accuracy: 110.4%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 202000: 0.027960\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 203000: 0.027726\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 204000: 0.027768\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 96.2%\n",
      "Loss at step 205000: 0.027504\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 95.9%\n",
      "Loss at step 206000: 0.027385\n",
      "Training accuracy: 102.4%\n",
      "Validation accuracy: 95.4%\n",
      "Loss at step 207000: 0.027399\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 208000: 0.027258\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 209000: 0.027144\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 210000: 0.027062\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 211000: 0.026980\n",
      "Training accuracy: 115.8%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 212000: 0.026913\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 213000: 0.026815\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 214000: 0.026685\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 95.8%\n",
      "Loss at step 215000: 0.026635\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 216000: 0.026570\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 217000: 0.026516\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 218000: 0.026464\n",
      "Training accuracy: 42.7%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 219000: 0.026328\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 220000: 0.026182\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 221000: 0.026168\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 222000: 0.026234\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 223000: 0.025964\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 224000: 0.025851\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 95.6%\n",
      "Loss at step 225000: 0.025793\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 226000: 0.025764\n",
      "Training accuracy: 30.2%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 227000: 0.025641\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 228000: 0.025707\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 229000: 0.025418\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 230000: 0.025490\n",
      "Training accuracy: 103.6%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 231000: 0.025640\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 232000: 0.025340\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 233000: 0.025370\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 95.8%\n",
      "Loss at step 234000: 0.025190\n",
      "Training accuracy: 106.4%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 235000: 0.025036\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 236000: 0.025011\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 237000: 0.024934\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 238000: 0.024831\n",
      "Training accuracy: 93.2%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 239000: 0.024746\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 240000: 0.024724\n",
      "Training accuracy: 133.3%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 241000: 0.024609\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 242000: 0.024551\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 243000: 0.024434\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 244000: 0.024333\n",
      "Training accuracy: 104.8%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 245000: 0.024344\n",
      "Training accuracy: 95.1%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 246000: 0.024287\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 247000: 0.024209\n",
      "Training accuracy: 36.1%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 248000: 0.024115\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 249000: 0.024048\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 250000: 0.023949\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 251000: 0.023968\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 252000: 0.023769\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 253000: 0.023679\n",
      "Training accuracy: 106.4%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 254000: 0.023638\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 255000: 0.023581\n",
      "Training accuracy: 44.3%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 256000: 0.023496\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 257000: 0.023584\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 258000: 0.023354\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 259000: 0.023479\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 260000: 0.023564\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 261000: 0.023245\n",
      "Training accuracy: 49.5%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 262000: 0.023304\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 95.5%\n",
      "Loss at step 263000: 0.023182\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 95.2%\n",
      "Loss at step 264000: 0.022941\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 265000: 0.022994\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 266000: 0.022882\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 267000: 0.022793\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 268000: 0.022684\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 269000: 0.022703\n",
      "Training accuracy: 126.4%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 270000: 0.022662\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 271000: 0.022621\n",
      "Training accuracy: 54.5%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 272000: 0.022405\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 95.1%\n",
      "Loss at step 273000: 0.022320\n",
      "Training accuracy: 101.2%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 274000: 0.022392\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 275000: 0.022320\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 276000: 0.022181\n",
      "Training accuracy: 36.9%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 277000: 0.022180\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 278000: 0.022113\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 279000: 0.022022\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 280000: 0.021977\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 281000: 0.021850\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 282000: 0.021882\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 94.9%\n",
      "Loss at step 283000: 0.021710\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 284000: 0.021668\n",
      "Training accuracy: -2.3%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 285000: 0.021583\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 286000: 0.021686\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 287000: 0.021489\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 288000: 0.021655\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 289000: 0.021631\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 290000: 0.021405\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 291000: 0.021593\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 95.3%\n",
      "Loss at step 292000: 0.021264\n",
      "Training accuracy: 103.6%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 293000: 0.021128\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 294000: 0.021198\n",
      "Training accuracy: 17.0%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 295000: 0.021031\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 296000: 0.020913\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 297000: 0.020823\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 298000: 0.020832\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 299000: 0.020901\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 300000: 0.020799\n",
      "Training accuracy: 60.5%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 301000: 0.020597\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 302000: 0.020527\n",
      "Training accuracy: 104.8%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 303000: 0.020571\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 304000: 0.020616\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 305000: 0.020402\n",
      "Training accuracy: 30.3%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 306000: 0.020354\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 307000: 0.020367\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 308000: 0.020230\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 309000: 0.020192\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 310000: 0.020129\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 94.8%\n",
      "Loss at step 311000: 0.020158\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 312000: 0.020039\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 313000: 0.019954\n",
      "Training accuracy: 10.5%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 314000: 0.019880\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 315000: 0.019928\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 316000: 0.019801\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 317000: 0.019946\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 318000: 0.019882\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 319000: 0.019685\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 320000: 0.019782\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 95.0%\n",
      "Loss at step 321000: 0.019634\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 322000: 0.019456\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 323000: 0.019547\n",
      "Training accuracy: -5.5%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 324000: 0.019337\n",
      "Training accuracy: 107.5%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 325000: 0.019253\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 326000: 0.019160\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 327000: 0.019228\n",
      "Training accuracy: 125.1%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 328000: 0.019278\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 329000: 0.019191\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 330000: 0.018969\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 94.6%\n",
      "Loss at step 331000: 0.018907\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 332000: 0.018958\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 333000: 0.018998\n",
      "Training accuracy: 28.1%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 334000: 0.018822\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 335000: 0.018740\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 336000: 0.018794\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 337000: 0.018640\n",
      "Training accuracy: 91.6%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 338000: 0.018673\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 339000: 0.018532\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 340000: 0.018573\n",
      "Training accuracy: 125.5%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 341000: 0.018460\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 342000: 0.018395\n",
      "Training accuracy: 13.3%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 343000: 0.018358\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 344000: 0.018330\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 345000: 0.018265\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 346000: 0.018405\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 347000: 0.018226\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 348000: 0.018148\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 94.4%\n",
      "Loss at step 349000: 0.018176\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 94.7%\n",
      "Loss at step 350000: 0.018107\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 351000: 0.017957\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 352000: 0.017959\n",
      "Training accuracy: -6.6%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 353000: 0.017892\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 354000: 0.017794\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 355000: 0.017665\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 356000: 0.017779\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 357000: 0.017917\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 358000: 0.017687\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 359000: 0.017483\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 360000: 0.017436\n",
      "Training accuracy: 101.2%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 361000: 0.017487\n",
      "Training accuracy: 93.4%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 362000: 0.017457\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 94.2%\n",
      "Loss at step 363000: 0.017346\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 364000: 0.017265\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 365000: 0.017322\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 366000: 0.017276\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 367000: 0.017230\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 368000: 0.017110\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 369000: 0.017195\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 370000: 0.016996\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 371000: 0.016995\n",
      "Training accuracy: 6.4%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 372000: 0.016953\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 373000: 0.016913\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 374000: 0.016921\n",
      "Training accuracy: 89.6%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 375000: 0.017002\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 376000: 0.016788\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 377000: 0.016779\n",
      "Training accuracy: 60.7%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 378000: 0.016782\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 94.3%\n",
      "Loss at step 379000: 0.016673\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 380000: 0.016628\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 381000: 0.016566\n",
      "Training accuracy: 2.8%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 382000: 0.016590\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 383000: 0.016429\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 384000: 0.016301\n",
      "Training accuracy: 107.3%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 385000: 0.016366\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 386000: 0.016581\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 387000: 0.016297\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 388000: 0.016116\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 389000: 0.016126\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 390000: 0.016183\n",
      "Training accuracy: 15.0%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 391000: 0.016093\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 392000: 0.016084\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 393000: 0.015922\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 394000: 0.015917\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 395000: 0.016088\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 396000: 0.015915\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 397000: 0.015955\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 398000: 0.015915\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 399000: 0.015701\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 400000: 0.015729\n",
      "Training accuracy: -3.0%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 401000: 0.015712\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 402000: 0.015602\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 403000: 0.015569\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 404000: 0.015593\n",
      "Training accuracy: 103.4%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 405000: 0.015508\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 406000: 0.015456\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 407000: 0.015518\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 94.0%\n",
      "Loss at step 408000: 0.015342\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 409000: 0.015357\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 410000: 0.015283\n",
      "Training accuracy: 7.2%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 411000: 0.015353\n",
      "Training accuracy: 24.6%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 412000: 0.015186\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 413000: 0.015071\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 414000: 0.015114\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 415000: 0.015333\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 416000: 0.014998\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 417000: 0.014910\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 418000: 0.014881\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 419000: 0.014927\n",
      "Training accuracy: 4.1%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 420000: 0.014869\n",
      "Training accuracy: 11.2%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 421000: 0.014947\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 422000: 0.014658\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 423000: 0.014705\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 424000: 0.015015\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 425000: 0.014727\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 426000: 0.014851\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 427000: 0.014608\n",
      "Training accuracy: 103.3%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 428000: 0.014524\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 429000: 0.014621\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 430000: 0.014545\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 431000: 0.014433\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 432000: 0.014368\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 433000: 0.014357\n",
      "Training accuracy: 119.9%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 434000: 0.014355\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 435000: 0.014283\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 436000: 0.014173\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 93.9%\n",
      "Loss at step 437000: 0.014194\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 438000: 0.014172\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 439000: 0.014180\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 440000: 0.014166\n",
      "Training accuracy: 28.8%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 441000: 0.014055\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 442000: 0.014043\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 443000: 0.013993\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 444000: 0.014027\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 445000: 0.013870\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 446000: 0.013795\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 447000: 0.013788\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 448000: 0.013778\n",
      "Training accuracy: 2.7%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 449000: 0.013716\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 450000: 0.013858\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 451000: 0.013542\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 452000: 0.013715\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 453000: 0.014011\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 454000: 0.013646\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 455000: 0.013764\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 94.1%\n",
      "Loss at step 456000: 0.013583\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 457000: 0.013397\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 458000: 0.013523\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 459000: 0.013461\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 460000: 0.013358\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 461000: 0.013260\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 462000: 0.013345\n",
      "Training accuracy: 145.6%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 463000: 0.013237\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 464000: 0.013261\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 465000: 0.013136\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 466000: 0.013058\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 467000: 0.013127\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 468000: 0.013144\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 469000: 0.013108\n",
      "Training accuracy: 20.9%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 470000: 0.013017\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 471000: 0.013039\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 472000: 0.012927\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 473000: 0.012961\n",
      "Training accuracy: 69.0%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 474000: 0.012802\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 475000: 0.012744\n",
      "Training accuracy: 111.4%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 476000: 0.012764\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 477000: 0.012722\n",
      "Training accuracy: 17.9%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 478000: 0.012692\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 479000: 0.012840\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 480000: 0.012630\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 481000: 0.012803\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 482000: 0.013039\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 483000: 0.012656\n",
      "Training accuracy: 41.1%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 484000: 0.012774\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 93.8%\n",
      "Loss at step 485000: 0.012657\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 486000: 0.012404\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 487000: 0.012545\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 488000: 0.012442\n",
      "Training accuracy: 103.6%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 489000: 0.012376\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 490000: 0.012282\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 491000: 0.012372\n",
      "Training accuracy: 136.8%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 492000: 0.012450\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 493000: 0.012376\n",
      "Training accuracy: 42.7%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 494000: 0.012135\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 93.6%\n",
      "Loss at step 495000: 0.012073\n",
      "Training accuracy: 100.5%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 496000: 0.012171\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 497000: 0.012205\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 498000: 0.012066\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 499000: 0.012048\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 500000: 0.012106\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 501000: 0.011956\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 502000: 0.011990\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 503000: 0.011911\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 93.5%\n",
      "Loss at step 504000: 0.011970\n",
      "Training accuracy: 109.2%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 505000: 0.011815\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 506000: 0.011795\n",
      "Training accuracy: -62.4%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 507000: 0.011756\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 508000: 0.011944\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 509000: 0.011730\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 510000: 0.011970\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 511000: 0.012024\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 512000: 0.011790\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 513000: 0.012053\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 93.7%\n",
      "Loss at step 514000: 0.011651\n",
      "Training accuracy: 106.0%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 515000: 0.011545\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 516000: 0.011697\n",
      "Training accuracy: -0.9%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 517000: 0.011531\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 518000: 0.011389\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 519000: 0.011309\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 520000: 0.011396\n",
      "Training accuracy: 127.1%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 521000: 0.011556\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 522000: 0.011469\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 523000: 0.011229\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 524000: 0.011190\n",
      "Training accuracy: 105.3%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 525000: 0.011308\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 526000: 0.011409\n",
      "Training accuracy: -8.3%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 527000: 0.011176\n",
      "Training accuracy: 14.9%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 528000: 0.011155\n",
      "Training accuracy: 73.5%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 529000: 0.011253\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 530000: 0.011080\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 531000: 0.011078\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 532000: 0.011051\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 93.3%\n",
      "Loss at step 533000: 0.011113\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 534000: 0.011000\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 535000: 0.010960\n",
      "Training accuracy: -29.5%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 536000: 0.010901\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 537000: 0.011017\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 538000: 0.010909\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 539000: 0.011106\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 540000: 0.011106\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 541000: 0.010872\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 542000: 0.011041\n",
      "Training accuracy: 71.6%\n",
      "Validation accuracy: 93.4%\n",
      "Loss at step 543000: 0.010900\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 544000: 0.010706\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 545000: 0.010889\n",
      "Training accuracy: -28.3%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 546000: 0.010737\n",
      "Training accuracy: 106.1%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 547000: 0.010569\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 548000: 0.010472\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 549000: 0.010617\n",
      "Training accuracy: 130.6%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 550000: 0.010766\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 551000: 0.010681\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 552000: 0.010413\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 553000: 0.010367\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 554000: 0.010477\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 555000: 0.010588\n",
      "Training accuracy: 12.9%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 556000: 0.010388\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 557000: 0.010325\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 558000: 0.010473\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 559000: 0.010292\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 560000: 0.010386\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 561000: 0.010248\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 562000: 0.010389\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 93.1%\n",
      "Loss at step 563000: 0.010223\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 564000: 0.010180\n",
      "Training accuracy: -23.5%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 565000: 0.010191\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 566000: 0.010141\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 567000: 0.010137\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 568000: 0.010315\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 569000: 0.010144\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 570000: 0.010101\n",
      "Training accuracy: 53.3%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 571000: 0.010148\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 93.2%\n",
      "Loss at step 572000: 0.010113\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 573000: 0.010040\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 574000: 0.010015\n",
      "Training accuracy: -29.6%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 575000: 0.009961\n",
      "Training accuracy: 93.2%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 576000: 0.009851\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 577000: 0.009713\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 578000: 0.009911\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 579000: 0.010162\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 580000: 0.009890\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 581000: 0.009655\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 93.0%\n",
      "Loss at step 582000: 0.009627\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 583000: 0.009732\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 584000: 0.009742\n",
      "Training accuracy: 17.3%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 585000: 0.009736\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 586000: 0.009570\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 587000: 0.009591\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 588000: 0.009650\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 589000: 0.009611\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 590000: 0.009531\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 591000: 0.009657\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 592000: 0.009455\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 593000: 0.009500\n",
      "Training accuracy: -33.3%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 594000: 0.009492\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 595000: 0.009435\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 596000: 0.009496\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 597000: 0.009611\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 598000: 0.009389\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 599000: 0.009414\n",
      "Training accuracy: 49.5%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 600000: 0.009438\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 92.9%\n",
      "Loss at step 601000: 0.009338\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 602000: 0.009298\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 603000: 0.009249\n",
      "Training accuracy: -16.1%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 604000: 0.009392\n",
      "Training accuracy: 4.2%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 605000: 0.009169\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 606000: 0.009033\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 607000: 0.009147\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 608000: 0.009486\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 609000: 0.009129\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 610000: 0.008935\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 611000: 0.008983\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 612000: 0.009106\n",
      "Training accuracy: -19.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 613000: 0.009005\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 614000: 0.009055\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 615000: 0.008862\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 616000: 0.008878\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 617000: 0.009277\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 618000: 0.008989\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 619000: 0.009087\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 620000: 0.009008\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 621000: 0.008785\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 622000: 0.008853\n",
      "Training accuracy: -39.6%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 623000: 0.008912\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 624000: 0.008764\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 625000: 0.008730\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 626000: 0.008804\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 627000: 0.008712\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 628000: 0.008661\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 629000: 0.008783\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 630000: 0.008609\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 631000: 0.008669\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 632000: 0.008602\n",
      "Training accuracy: -9.8%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 633000: 0.008705\n",
      "Training accuracy: 4.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 634000: 0.008523\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 635000: 0.008384\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 636000: 0.008569\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 637000: 0.008698\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 638000: 0.008411\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 639000: 0.008333\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 640000: 0.008335\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 641000: 0.008435\n",
      "Training accuracy: -21.6%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 642000: 0.008384\n",
      "Training accuracy: -16.1%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 643000: 0.008548\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 644000: 0.008164\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 645000: 0.008332\n",
      "Training accuracy: 110.1%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 646000: 0.008744\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 647000: 0.008375\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 648000: 0.008552\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 92.8%\n",
      "Loss at step 649000: 0.008268\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 650000: 0.008173\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 651000: 0.008349\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 652000: 0.008320\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 653000: 0.008140\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 654000: 0.008091\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 655000: 0.008164\n",
      "Training accuracy: 121.8%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 656000: 0.008133\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 657000: 0.008052\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 658000: 0.007951\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 659000: 0.008030\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 660000: 0.008020\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 661000: 0.008056\n",
      "Training accuracy: 41.3%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 662000: 0.008101\n",
      "Training accuracy: 2.4%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 663000: 0.007960\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 664000: 0.008043\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 665000: 0.007940\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 666000: 0.007978\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 667000: 0.007836\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 92.6%\n",
      "Loss at step 668000: 0.007770\n",
      "Training accuracy: 104.3%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 669000: 0.007813\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 670000: 0.007762\n",
      "Training accuracy: -3.5%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 671000: 0.007765\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 672000: 0.007981\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 673000: 0.007580\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 674000: 0.007826\n",
      "Training accuracy: 107.3%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 675000: 0.008292\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 676000: 0.007829\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 677000: 0.007990\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 92.7%\n",
      "Loss at step 678000: 0.007747\n",
      "Training accuracy: 113.8%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 679000: 0.007563\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 680000: 0.007780\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 681000: 0.007757\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 682000: 0.007599\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 683000: 0.007494\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 684000: 0.007639\n",
      "Training accuracy: 151.7%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 685000: 0.007519\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 686000: 0.007573\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 687000: 0.007442\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 688000: 0.007358\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 689000: 0.007498\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 690000: 0.007539\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 691000: 0.007417\n",
      "Training accuracy: -0.9%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 692000: 0.007401\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 693000: 0.007510\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 694000: 0.007386\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 695000: 0.007423\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 696000: 0.007296\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 92.4%\n",
      "Loss at step 697000: 0.007207\n",
      "Training accuracy: 111.4%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 698000: 0.007278\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 699000: 0.007220\n",
      "Training accuracy: -4.8%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 700000: 0.007212\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 701000: 0.007454\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 702000: 0.007204\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 703000: 0.007405\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 704000: 0.007792\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 705000: 0.007357\n",
      "Training accuracy: 25.1%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 706000: 0.007466\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 92.5%\n",
      "Loss at step 707000: 0.007212\n",
      "Training accuracy: 111.2%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 708000: 0.007083\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 709000: 0.007303\n",
      "Training accuracy: -10.7%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 710000: 0.007175\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 711000: 0.007092\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 712000: 0.006984\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 713000: 0.007178\n",
      "Training accuracy: 140.2%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 714000: 0.007277\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 715000: 0.007170\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 716000: 0.006897\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 717000: 0.006837\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 718000: 0.006930\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 719000: 0.007079\n",
      "Training accuracy: -10.8%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 720000: 0.006923\n",
      "Training accuracy: 3.2%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 721000: 0.006917\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 722000: 0.007040\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 723000: 0.006847\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 724000: 0.006900\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 725000: 0.006863\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 92.2%\n",
      "Loss at step 726000: 0.006914\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 727000: 0.006779\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 728000: 0.006760\n",
      "Training accuracy: -109.0%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 729000: 0.006732\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 730000: 0.006875\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 731000: 0.006734\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 732000: 0.007030\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 733000: 0.007197\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 734000: 0.006890\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 735000: 0.007230\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 92.3%\n",
      "Loss at step 736000: 0.006725\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 737000: 0.006607\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 738000: 0.006823\n",
      "Training accuracy: -9.2%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 739000: 0.006702\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 740000: 0.006505\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 741000: 0.006419\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 742000: 0.006580\n",
      "Training accuracy: 134.7%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 743000: 0.006787\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 744000: 0.006696\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 745000: 0.006414\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 746000: 0.006394\n",
      "Training accuracy: 105.0%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 747000: 0.006569\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 748000: 0.006689\n",
      "Training accuracy: -22.2%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 749000: 0.006428\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 750000: 0.006418\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 751000: 0.006582\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 752000: 0.006381\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 753000: 0.006507\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 754000: 0.006420\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 755000: 0.006469\n",
      "Training accuracy: 122.6%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 756000: 0.006400\n",
      "Training accuracy: 73.7%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 757000: 0.006322\n",
      "Training accuracy: -61.7%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 758000: 0.006271\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 759000: 0.006448\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 760000: 0.006335\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 761000: 0.006563\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 762000: 0.006484\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 763000: 0.006372\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 764000: 0.006549\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 92.1%\n",
      "Loss at step 765000: 0.006406\n",
      "Training accuracy: 115.8%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 766000: 0.006175\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 767000: 0.006442\n",
      "Training accuracy: -43.8%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 768000: 0.006267\n",
      "Training accuracy: 106.9%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 769000: 0.006066\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 770000: 0.005967\n",
      "Training accuracy: 106.9%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 771000: 0.006175\n",
      "Training accuracy: 135.1%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 772000: 0.006401\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 773000: 0.006235\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 774000: 0.005988\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 775000: 0.005968\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 776000: 0.006113\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 777000: 0.006255\n",
      "Training accuracy: 4.7%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 778000: 0.006034\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 779000: 0.005973\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 780000: 0.006210\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 781000: 0.005908\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 782000: 0.006116\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 783000: 0.005991\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 784000: 0.006131\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 785000: 0.005999\n",
      "Training accuracy: 72.4%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 786000: 0.005915\n",
      "Training accuracy: -52.3%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 787000: 0.005974\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 788000: 0.005913\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 789000: 0.005941\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 790000: 0.006064\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 791000: 0.005960\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 792000: 0.005974\n",
      "Training accuracy: 44.5%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 793000: 0.005977\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 92.0%\n",
      "Loss at step 794000: 0.005956\n",
      "Training accuracy: 105.1%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 795000: 0.005923\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 796000: 0.005911\n",
      "Training accuracy: -44.9%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 797000: 0.005977\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 798000: 0.005718\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 799000: 0.005573\n",
      "Training accuracy: 107.0%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 800000: 0.005806\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 801000: 0.006184\n",
      "Training accuracy: 53.2%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 802000: 0.005844\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 803000: 0.005575\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 804000: 0.005559\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 805000: 0.005729\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 806000: 0.005725\n",
      "Training accuracy: 11.6%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 807000: 0.005766\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 808000: 0.005545\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 809000: 0.005545\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 810000: 0.005698\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 811000: 0.005754\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 812000: 0.005615\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 813000: 0.005728\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 814000: 0.005542\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 815000: 0.005593\n",
      "Training accuracy: -63.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 816000: 0.005768\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 817000: 0.005543\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 818000: 0.005648\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 819000: 0.005756\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 820000: 0.005539\n",
      "Training accuracy: 60.4%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 821000: 0.005621\n",
      "Training accuracy: 41.7%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 822000: 0.005601\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 823000: 0.005518\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 824000: 0.005497\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 825000: 0.005452\n",
      "Training accuracy: -26.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 826000: 0.005650\n",
      "Training accuracy: -13.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 827000: 0.005370\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 828000: 0.005227\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 829000: 0.005391\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 830000: 0.005815\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 831000: 0.005406\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 91.8%\n",
      "Loss at step 832000: 0.005178\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 833000: 0.005255\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 834000: 0.005429\n",
      "Training accuracy: -39.8%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 835000: 0.005279\n",
      "Training accuracy: 11.6%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 836000: 0.005415\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 837000: 0.005168\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 838000: 0.005209\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 839000: 0.005737\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 840000: 0.005384\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 841000: 0.005469\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 91.9%\n",
      "Loss at step 842000: 0.005390\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 843000: 0.005192\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 844000: 0.005267\n",
      "Training accuracy: -71.5%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 845000: 0.005395\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 846000: 0.005187\n",
      "Training accuracy: 71.6%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 847000: 0.005150\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 848000: 0.005245\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 849000: 0.005177\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 850000: 0.005096\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 851000: 0.005271\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 852000: 0.005070\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 853000: 0.005147\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 854000: 0.005078\n",
      "Training accuracy: 39.2%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 855000: 0.005257\n",
      "Training accuracy: -9.8%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 856000: 0.005022\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 857000: 0.004890\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 858000: 0.005065\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 859000: 0.005243\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 860000: 0.004951\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 861000: 0.004882\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 862000: 0.004903\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 863000: 0.005051\n",
      "Training accuracy: -45.9%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 864000: 0.004937\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 865000: 0.005249\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 866000: 0.004737\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 867000: 0.004955\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 868000: 0.005540\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 869000: 0.005064\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 870000: 0.005292\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 91.7%\n",
      "Loss at step 871000: 0.004926\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 872000: 0.004872\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 873000: 0.005049\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 874000: 0.005066\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 875000: 0.004830\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 876000: 0.004795\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 877000: 0.004889\n",
      "Training accuracy: 127.8%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 878000: 0.004868\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 879000: 0.004785\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 880000: 0.004656\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 881000: 0.004783\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 882000: 0.004783\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 883000: 0.004848\n",
      "Training accuracy: 32.9%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 884000: 0.004906\n",
      "Training accuracy: -12.5%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 885000: 0.004745\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 886000: 0.004894\n",
      "Training accuracy: 100.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 887000: 0.004731\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 888000: 0.004780\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 889000: 0.004626\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 890000: 0.004574\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 891000: 0.004650\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 892000: 0.004602\n",
      "Training accuracy: -23.8%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 893000: 0.004610\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 894000: 0.004949\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 895000: 0.004423\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 896000: 0.004745\n",
      "Training accuracy: 109.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 897000: 0.005329\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 898000: 0.004646\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 899000: 0.004955\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 900000: 0.004671\n",
      "Training accuracy: 118.9%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 901000: 0.004477\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 902000: 0.004760\n",
      "Training accuracy: 69.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 903000: 0.004755\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 904000: 0.004577\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 905000: 0.004451\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 906000: 0.004609\n",
      "Training accuracy: 161.8%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 907000: 0.004532\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 908000: 0.004679\n",
      "Training accuracy: 31.8%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 909000: 0.004421\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 910000: 0.004336\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 911000: 0.004521\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 912000: 0.004591\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 913000: 0.004461\n",
      "Training accuracy: -19.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 914000: 0.004438\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 915000: 0.004635\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 916000: 0.004460\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 917000: 0.004485\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 918000: 0.004389\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 919000: 0.004269\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 920000: 0.004365\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 921000: 0.004351\n",
      "Training accuracy: -127.2%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 922000: 0.004293\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 923000: 0.004623\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 924000: 0.004363\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 925000: 0.004647\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 926000: 0.005097\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 927000: 0.004578\n",
      "Training accuracy: 16.6%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 928000: 0.004674\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 91.6%\n",
      "Loss at step 929000: 0.004369\n",
      "Training accuracy: 115.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 930000: 0.004243\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 931000: 0.004598\n",
      "Training accuracy: -28.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 932000: 0.004373\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 933000: 0.004296\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 934000: 0.004167\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 935000: 0.004414\n",
      "Training accuracy: 136.9%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 936000: 0.004570\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 937000: 0.004443\n",
      "Training accuracy: 30.0%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 938000: 0.004115\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 939000: 0.004054\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 940000: 0.004169\n",
      "Training accuracy: 107.1%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 941000: 0.004373\n",
      "Training accuracy: -18.6%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 942000: 0.004038\n",
      "Training accuracy: -10.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 943000: 0.004197\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 944000: 0.004392\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 945000: 0.004116\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 946000: 0.004194\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 947000: 0.004211\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 948000: 0.004235\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 949000: 0.004126\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 950000: 0.004098\n",
      "Training accuracy: -147.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 951000: 0.004051\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 952000: 0.004249\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 953000: 0.004110\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 954000: 0.004428\n",
      "Training accuracy: 93.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 955000: 0.004679\n",
      "Training accuracy: 53.4%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 956000: 0.004249\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 957000: 0.004688\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 91.5%\n",
      "Loss at step 958000: 0.004193\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 959000: 0.003987\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 960000: 0.004279\n",
      "Training accuracy: -22.2%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 961000: 0.003956\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 962000: 0.003921\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 963000: 0.003807\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 964000: 0.004013\n",
      "Training accuracy: 144.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 965000: 0.004316\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 966000: 0.004187\n",
      "Training accuracy: 49.5%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 967000: 0.003843\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 968000: 0.003838\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 969000: 0.004067\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 970000: 0.004185\n",
      "Training accuracy: -32.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 971000: 0.003898\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 972000: 0.003891\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 973000: 0.004140\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 974000: 0.003888\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 975000: 0.004056\n",
      "Training accuracy: 60.8%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 976000: 0.003951\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 977000: 0.004007\n",
      "Training accuracy: 126.0%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 978000: 0.003973\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 979000: 0.003855\n",
      "Training accuracy: -88.6%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 980000: 0.003817\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 981000: 0.004021\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 982000: 0.003908\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 983000: 0.004148\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 984000: 0.004080\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 985000: 0.003984\n",
      "Training accuracy: 39.5%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 986000: 0.004188\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 91.4%\n",
      "Loss at step 987000: 0.004016\n",
      "Training accuracy: 117.5%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 988000: 0.003761\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 989000: 0.004119\n",
      "Training accuracy: -62.2%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 990000: 0.003908\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 991000: 0.003694\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 992000: 0.003571\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 993000: 0.003820\n",
      "Training accuracy: 143.6%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 994000: 0.004088\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 995000: 0.003907\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 996000: 0.003612\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 997000: 0.003606\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 998000: 0.003774\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 999000: 0.003974\n",
      "Training accuracy: -3.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1000000: 0.003709\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1001000: 0.003654\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1002000: 0.003966\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1003000: 0.003585\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1004000: 0.003864\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1005000: 0.003748\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 1006000: 0.003863\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1007000: 0.003768\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1008000: 0.003645\n",
      "Training accuracy: -76.4%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1009000: 0.003764\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1010000: 0.003640\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1011000: 0.003717\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1012000: 0.003839\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1013000: 0.003699\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1014000: 0.003771\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1015000: 0.003714\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 91.3%\n",
      "Loss at step 1016000: 0.003733\n",
      "Training accuracy: 105.6%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1017000: 0.003707\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1018000: 0.003725\n",
      "Training accuracy: -59.6%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1019000: 0.003821\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1020000: 0.003528\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1021000: 0.003352\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1022000: 0.003618\n",
      "Training accuracy: 120.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1023000: 0.004044\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1024000: 0.003703\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1025000: 0.003384\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1026000: 0.003377\n",
      "Training accuracy: 98.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1027000: 0.003588\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1028000: 0.003578\n",
      "Training accuracy: 4.3%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1029000: 0.003662\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1030000: 0.003385\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1031000: 0.003419\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1032000: 0.003606\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1033000: 0.003699\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1034000: 0.003532\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1035000: 0.003649\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1036000: 0.003465\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1037000: 0.003512\n",
      "Training accuracy: -85.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1038000: 0.003739\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1039000: 0.003423\n",
      "Training accuracy: 53.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1040000: 0.003597\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1041000: 0.003711\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1042000: 0.003477\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1043000: 0.003601\n",
      "Training accuracy: 34.6%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1044000: 0.003550\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1045000: 0.003464\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1046000: 0.003468\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1047000: 0.003414\n",
      "Training accuracy: -42.6%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1048000: 0.003685\n",
      "Training accuracy: -32.6%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1049000: 0.003327\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1050000: 0.003185\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1051000: 0.003382\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1052000: 0.003842\n",
      "Training accuracy: 44.3%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1053000: 0.003346\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1054000: 0.003152\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1055000: 0.003247\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1056000: 0.003463\n",
      "Training accuracy: -57.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1057000: 0.003295\n",
      "Training accuracy: -10.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1058000: 0.003473\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1059000: 0.003168\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1060000: 0.003244\n",
      "Training accuracy: 110.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1061000: 0.003964\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1062000: 0.003495\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1063000: 0.003773\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 91.2%\n",
      "Loss at step 1064000: 0.003480\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1065000: 0.003283\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1066000: 0.003309\n",
      "Training accuracy: 20.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1067000: 0.003543\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1068000: 0.003272\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1069000: 0.003201\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1070000: 0.003280\n",
      "Training accuracy: 110.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1071000: 0.003231\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1072000: 0.003190\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1073000: 0.003374\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1074000: 0.003157\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1075000: 0.003263\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1076000: 0.003223\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1077000: 0.003437\n",
      "Training accuracy: -28.0%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1078000: 0.003161\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1079000: 0.003011\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1080000: 0.003211\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1081000: 0.003367\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1082000: 0.003105\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1083000: 0.003020\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1084000: 0.003093\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1085000: 0.003250\n",
      "Training accuracy: -59.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1086000: 0.003112\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1087000: 0.003516\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1088000: 0.002886\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1089000: 0.003211\n",
      "Training accuracy: 111.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1090000: 0.003850\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1091000: 0.003313\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1092000: 0.003525\n",
      "Training accuracy: 40.2%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1093000: 0.003179\n",
      "Training accuracy: 123.4%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1094000: 0.003076\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1095000: 0.003306\n",
      "Training accuracy: 16.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1096000: 0.003360\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1097000: 0.003052\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1098000: 0.002999\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1099000: 0.003131\n",
      "Training accuracy: 134.6%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1100000: 0.003099\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1101000: 0.003175\n",
      "Training accuracy: 42.6%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1102000: 0.002873\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1103000: 0.002928\n",
      "Training accuracy: 101.9%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1104000: 0.003056\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1105000: 0.003131\n",
      "Training accuracy: 23.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1106000: 0.003233\n",
      "Training accuracy: -31.8%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1107000: 0.003003\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1108000: 0.003241\n",
      "Training accuracy: 101.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1109000: 0.003024\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1110000: 0.003040\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1111000: 0.002921\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1112000: 0.002853\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1113000: 0.002956\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1114000: 0.002902\n",
      "Training accuracy: -36.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1115000: 0.002930\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1116000: 0.003340\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1117000: 0.002718\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1118000: 0.003087\n",
      "Training accuracy: 112.0%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1119000: 0.003773\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1120000: 0.002966\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1121000: 0.003320\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 91.1%\n",
      "Loss at step 1122000: 0.003027\n",
      "Training accuracy: 116.6%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1123000: 0.002822\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1124000: 0.003212\n",
      "Training accuracy: 40.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1125000: 0.003168\n",
      "Training accuracy: 104.4%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1126000: 0.002953\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1127000: 0.002794\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1128000: 0.003016\n",
      "Training accuracy: 168.8%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1129000: 0.002872\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1130000: 0.003096\n",
      "Training accuracy: 28.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1131000: 0.002764\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1132000: 0.002684\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1133000: 0.002930\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1134000: 0.003017\n",
      "Training accuracy: 62.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1135000: 0.002880\n",
      "Training accuracy: -36.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1136000: 0.002910\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1137000: 0.003118\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1138000: 0.002878\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1139000: 0.002895\n",
      "Training accuracy: 58.2%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1140000: 0.002827\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1141000: 0.002687\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1142000: 0.002823\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1143000: 0.002793\n",
      "Training accuracy: -151.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1144000: 0.002709\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1145000: 0.003176\n",
      "Training accuracy: 61.3%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1146000: 0.002802\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1147000: 0.003109\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1148000: 0.003693\n",
      "Training accuracy: 40.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1149000: 0.003084\n",
      "Training accuracy: 8.4%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1150000: 0.003151\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 91.0%\n",
      "Loss at step 1151000: 0.002818\n",
      "Training accuracy: 113.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1152000: 0.002713\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1153000: 0.003149\n",
      "Training accuracy: -38.4%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1154000: 0.002880\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1155000: 0.002795\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1156000: 0.002619\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1157000: 0.002945\n",
      "Training accuracy: 142.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1158000: 0.003102\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1159000: 0.002991\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1160000: 0.002598\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1161000: 0.002540\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1162000: 0.002660\n",
      "Training accuracy: 111.1%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1163000: 0.002902\n",
      "Training accuracy: -32.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1164000: 0.002595\n",
      "Training accuracy: -29.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1165000: 0.002719\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1166000: 0.003003\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1167000: 0.002649\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1168000: 0.002727\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1169000: 0.002825\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1170000: 0.002789\n",
      "Training accuracy: 111.2%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1171000: 0.002693\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1172000: 0.002657\n",
      "Training accuracy: -171.8%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1173000: 0.002604\n",
      "Training accuracy: 54.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1174000: 0.002863\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1175000: 0.002687\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1176000: 0.003025\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1177000: 0.003338\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1178000: 0.002842\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1179000: 0.003317\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 90.9%\n",
      "Loss at step 1180000: 0.002840\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1181000: 0.002566\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1182000: 0.002995\n",
      "Training accuracy: -56.3%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1183000: 0.002548\n",
      "Training accuracy: 110.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1184000: 0.002519\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1185000: 0.002374\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1186000: 0.002625\n",
      "Training accuracy: 149.9%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1187000: 0.002960\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1188000: 0.002836\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1189000: 0.002438\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1190000: 0.002439\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1191000: 0.002692\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1192000: 0.002858\n",
      "Training accuracy: -36.4%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1193000: 0.002514\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1194000: 0.002535\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1195000: 0.002851\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1196000: 0.002562\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1197000: 0.002735\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1198000: 0.002629\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1199000: 0.002668\n",
      "Training accuracy: 129.4%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1200000: 0.002645\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1201000: 0.002544\n",
      "Training accuracy: -100.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1202000: 0.002457\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1203000: 0.002762\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1204000: 0.002603\n",
      "Training accuracy: 89.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1205000: 0.002828\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1206000: 0.002676\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1207000: 0.002675\n",
      "Training accuracy: 34.9%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1208000: 0.002659\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 90.8%\n",
      "Loss at step 1209000: 0.002719\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 90.7%\n",
      "Loss at step 1210000: 0.002601\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1211000: 0.002873\n",
      "Training accuracy: -73.3%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1212000: 0.002597\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1213000: 0.002393\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1214000: 0.002257\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1215000: 0.002539\n",
      "Training accuracy: 147.6%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1216000: 0.002938\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1217000: 0.002700\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1218000: 0.002299\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1219000: 0.002323\n",
      "Training accuracy: 106.5%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1220000: 0.002513\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1221000: 0.002606\n",
      "Training accuracy: -7.7%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1222000: 0.002435\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1223000: 0.002368\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1224000: 0.002781\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1225000: 0.002315\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1226000: 0.002645\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1227000: 0.002540\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1228000: 0.002635\n",
      "Training accuracy: 105.1%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1229000: 0.002530\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1230000: 0.002401\n",
      "Training accuracy: -88.7%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1231000: 0.002535\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1232000: 0.002420\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1233000: 0.002500\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1234000: 0.002570\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1235000: 0.002489\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1236000: 0.002571\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1237000: 0.002495\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1238000: 0.002474\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1239000: 0.002549\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1240000: 0.002536\n",
      "Training accuracy: -69.0%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1241000: 0.002653\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1242000: 0.002316\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1243000: 0.002132\n",
      "Training accuracy: 105.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1244000: 0.002443\n",
      "Training accuracy: 119.2%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1245000: 0.002885\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1246000: 0.002361\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1247000: 0.002183\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1248000: 0.002182\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1249000: 0.002473\n",
      "Training accuracy: -27.1%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1250000: 0.002401\n",
      "Training accuracy: -5.7%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1251000: 0.002515\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1252000: 0.002205\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1253000: 0.002280\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1254000: 0.002474\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1255000: 0.002575\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1256000: 0.002704\n",
      "Training accuracy: 53.2%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1257000: 0.002528\n",
      "Training accuracy: 103.7%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1258000: 0.002340\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1259000: 0.002374\n",
      "Training accuracy: -98.0%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1260000: 0.002642\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1261000: 0.002285\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1262000: 0.002489\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1263000: 0.002605\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1264000: 0.002349\n",
      "Training accuracy: 44.5%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1265000: 0.002518\n",
      "Training accuracy: 30.4%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1266000: 0.002418\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1267000: 0.002320\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1268000: 0.002371\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1269000: 0.002253\n",
      "Training accuracy: -25.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1270000: 0.002621\n",
      "Training accuracy: -43.3%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1271000: 0.002205\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1272000: 0.002075\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1273000: 0.002284\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1274000: 0.002782\n",
      "Training accuracy: 38.5%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1275000: 0.002264\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1276000: 0.002032\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1277000: 0.002157\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1278000: 0.002450\n",
      "Training accuracy: -80.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1279000: 0.002187\n",
      "Training accuracy: -22.1%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1280000: 0.002653\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1281000: 0.001991\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1282000: 0.002145\n",
      "Training accuracy: 123.9%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1283000: 0.003056\n",
      "Training accuracy: 70.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1284000: 0.002457\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1285000: 0.002783\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 90.6%\n",
      "Loss at step 1286000: 0.002432\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1287000: 0.002226\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1288000: 0.002255\n",
      "Training accuracy: 14.8%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1289000: 0.002545\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1290000: 0.002173\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1291000: 0.002129\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1292000: 0.002253\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1293000: 0.002154\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1294000: 0.002152\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1295000: 0.002327\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1296000: 0.002103\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1297000: 0.002235\n",
      "Training accuracy: 95.1%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1298000: 0.002278\n",
      "Training accuracy: 18.9%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1299000: 0.002434\n",
      "Training accuracy: -37.6%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1300000: 0.002135\n",
      "Training accuracy: 103.3%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1301000: 0.001973\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1302000: 0.002211\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1303000: 0.002308\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1304000: 0.002094\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1305000: 0.001996\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1306000: 0.002086\n",
      "Training accuracy: 73.5%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1307000: 0.002271\n",
      "Training accuracy: -66.9%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1308000: 0.002106\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1309000: 0.002579\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1310000: 0.001862\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1311000: 0.002204\n",
      "Training accuracy: 116.2%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1312000: 0.002930\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1313000: 0.002352\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1314000: 0.002547\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 90.5%\n",
      "Loss at step 1315000: 0.002243\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1316000: 0.002106\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1317000: 0.002387\n",
      "Training accuracy: -1.6%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1318000: 0.002427\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1319000: 0.002066\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1320000: 0.002006\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1321000: 0.002180\n",
      "Training accuracy: 135.8%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1322000: 0.002118\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1323000: 0.002247\n",
      "Training accuracy: 37.4%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1324000: 0.001883\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1325000: 0.001868\n",
      "Training accuracy: 108.6%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1326000: 0.002108\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1327000: 0.002112\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1328000: 0.002300\n",
      "Training accuracy: -50.4%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1329000: 0.002054\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1330000: 0.002339\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1331000: 0.002122\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1332000: 0.002060\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1333000: 0.001995\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1334000: 0.001888\n",
      "Training accuracy: 117.6%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1335000: 0.002030\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1336000: 0.001966\n",
      "Training accuracy: -43.5%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1337000: 0.001973\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1338000: 0.002473\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1339000: 0.001773\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1340000: 0.002157\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1341000: 0.002920\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1342000: 0.002037\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1343000: 0.002428\n",
      "Training accuracy: 44.8%\n",
      "Validation accuracy: 90.4%\n",
      "Loss at step 1344000: 0.002125\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1345000: 0.001917\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1346000: 0.002330\n",
      "Training accuracy: 34.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1347000: 0.002311\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1348000: 0.002027\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1349000: 0.001861\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1350000: 0.002137\n",
      "Training accuracy: 170.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1351000: 0.002053\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1352000: 0.002240\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1353000: 0.001862\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1354000: 0.001747\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1355000: 0.001865\n",
      "Training accuracy: 121.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1356000: 0.002155\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1357000: 0.002006\n",
      "Training accuracy: -45.3%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1358000: 0.002037\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1359000: 0.002270\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1360000: 0.002013\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1361000: 0.002010\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1362000: 0.001917\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1363000: 0.002076\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1364000: 0.001961\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1365000: 0.001948\n",
      "Training accuracy: -162.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1366000: 0.001843\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1367000: 0.002377\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1368000: 0.001952\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1369000: 0.002266\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1370000: 0.002946\n",
      "Training accuracy: 37.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1371000: 0.002247\n",
      "Training accuracy: 5.9%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1372000: 0.002633\n",
      "Training accuracy: 37.9%\n",
      "Validation accuracy: 90.3%\n",
      "Loss at step 1373000: 0.001955\n",
      "Training accuracy: 111.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1374000: 0.001858\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1375000: 0.002364\n",
      "Training accuracy: -43.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1376000: 0.002060\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1377000: 0.001937\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1378000: 0.001772\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1379000: 0.002128\n",
      "Training accuracy: 145.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1380000: 0.002294\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1381000: 0.002204\n",
      "Training accuracy: 20.7%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1382000: 0.001749\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1383000: 0.001724\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1384000: 0.001801\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1385000: 0.002177\n",
      "Training accuracy: -55.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1386000: 0.001777\n",
      "Training accuracy: -38.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1387000: 0.001909\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1388000: 0.002236\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1389000: 0.001833\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1390000: 0.001906\n",
      "Training accuracy: 54.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1391000: 0.002098\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1392000: 0.001978\n",
      "Training accuracy: 115.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1393000: 0.001901\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1394000: 0.001814\n",
      "Training accuracy: -47.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1395000: 0.001804\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1396000: 0.002099\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1397000: 0.001914\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1398000: 0.002218\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1399000: 0.002607\n",
      "Training accuracy: 42.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1400000: 0.002066\n",
      "Training accuracy: 30.5%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1401000: 0.002260\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1402000: 0.002041\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1403000: 0.001786\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1404000: 0.002267\n",
      "Training accuracy: -69.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1405000: 0.001746\n",
      "Training accuracy: 111.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1406000: 0.001756\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1407000: 0.001559\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1408000: 0.001866\n",
      "Training accuracy: 151.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1409000: 0.002220\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1410000: 0.002095\n",
      "Training accuracy: 41.0%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1411000: 0.001668\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1412000: 0.001663\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1413000: 0.001954\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1414000: 0.002127\n",
      "Training accuracy: -48.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1415000: 0.001778\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1416000: 0.001773\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1417000: 0.002137\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1418000: 0.001812\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1419000: 0.001980\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1420000: 0.001924\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1421000: 0.001920\n",
      "Training accuracy: 134.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1422000: 0.001917\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1423000: 0.001762\n",
      "Training accuracy: -92.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1424000: 0.001703\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1425000: 0.001799\n",
      "Training accuracy: 40.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1426000: 0.001873\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1427000: 0.002109\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1428000: 0.001869\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1429000: 0.001955\n",
      "Training accuracy: 33.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1430000: 0.001925\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 90.2%\n",
      "Loss at step 1431000: 0.001988\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1432000: 0.001895\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1433000: 0.002201\n",
      "Training accuracy: -80.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1434000: 0.001862\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1435000: 0.001671\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1436000: 0.001526\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1437000: 0.001842\n",
      "Training accuracy: 149.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1438000: 0.002256\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1439000: 0.002021\n",
      "Training accuracy: 41.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1440000: 0.001573\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1441000: 0.001614\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1442000: 0.001827\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1443000: 0.001811\n",
      "Training accuracy: -17.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1444000: 0.001716\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1445000: 0.001657\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1446000: 0.002155\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1447000: 0.001600\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1448000: 0.001976\n",
      "Training accuracy: 53.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1449000: 0.001894\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1450000: 0.001953\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1451000: 0.001858\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1452000: 0.001717\n",
      "Training accuracy: -96.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1453000: 0.001856\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1454000: 0.001674\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1455000: 0.001904\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1456000: 0.001891\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1457000: 0.001810\n",
      "Training accuracy: 52.9%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1458000: 0.001906\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1459000: 0.001831\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1460000: 0.001756\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1461000: 0.001891\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1462000: 0.001865\n",
      "Training accuracy: -64.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1463000: 0.002025\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1464000: 0.001639\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1465000: 0.001460\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1466000: 0.001773\n",
      "Training accuracy: 118.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1467000: 0.002240\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1468000: 0.001708\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1469000: 0.001508\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1470000: 0.001521\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1471000: 0.001848\n",
      "Training accuracy: -29.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1472000: 0.001742\n",
      "Training accuracy: -22.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1473000: 0.001869\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1474000: 0.001567\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1475000: 0.001645\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1476000: 0.001799\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1477000: 0.001966\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1478000: 0.002124\n",
      "Training accuracy: 48.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1479000: 0.001885\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1480000: 0.001722\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1481000: 0.001740\n",
      "Training accuracy: -104.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1482000: 0.002031\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1483000: 0.001668\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1484000: 0.001858\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1485000: 0.002018\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1486000: 0.001697\n",
      "Training accuracy: 40.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1487000: 0.001921\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1488000: 0.001797\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1489000: 0.001677\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1490000: 0.001763\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1491000: 0.001637\n",
      "Training accuracy: -36.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1492000: 0.002050\n",
      "Training accuracy: -53.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1493000: 0.001567\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1494000: 0.001469\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1495000: 0.001671\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1496000: 0.002125\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1497000: 0.001666\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1498000: 0.001390\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1499000: 0.001550\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1500000: 0.001892\n",
      "Training accuracy: -87.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1501000: 0.001563\n",
      "Training accuracy: -46.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1502000: 0.002081\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1503000: 0.001372\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1504000: 0.001553\n",
      "Training accuracy: 127.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1505000: 0.002503\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1506000: 0.001857\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1507000: 0.002250\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1508000: 0.001568\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1509000: 0.001645\n",
      "Training accuracy: 94.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1510000: 0.001898\n",
      "Training accuracy: 9.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1511000: 0.001987\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1512000: 0.001571\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1513000: 0.001530\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1514000: 0.001707\n",
      "Training accuracy: 140.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1515000: 0.001588\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1516000: 0.001596\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1517000: 0.001414\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1518000: 0.001518\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1519000: 0.001666\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1520000: 0.001717\n",
      "Training accuracy: 13.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1521000: 0.001911\n",
      "Training accuracy: -49.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1522000: 0.001548\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1523000: 0.001391\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1524000: 0.001629\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1525000: 0.001720\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1526000: 0.001535\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1527000: 0.001427\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1528000: 0.001600\n",
      "Training accuracy: 60.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1529000: 0.001758\n",
      "Training accuracy: -72.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1530000: 0.001456\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1531000: 0.002042\n",
      "Training accuracy: 69.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1532000: 0.001284\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1533000: 0.001652\n",
      "Training accuracy: 117.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1534000: 0.002421\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1535000: 0.001802\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1536000: 0.001984\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1537000: 0.001692\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1538000: 0.001568\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1539000: 0.001870\n",
      "Training accuracy: -3.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1540000: 0.001906\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1541000: 0.001597\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1542000: 0.001443\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1543000: 0.001659\n",
      "Training accuracy: 139.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1544000: 0.001567\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1545000: 0.001718\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1546000: 0.001329\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1547000: 0.001310\n",
      "Training accuracy: 109.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1548000: 0.001585\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1549000: 0.001575\n",
      "Training accuracy: 48.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1550000: 0.001807\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1551000: 0.001489\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1552000: 0.001840\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1553000: 0.001566\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1554000: 0.001534\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1555000: 0.001484\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1556000: 0.001343\n",
      "Training accuracy: 121.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1557000: 0.001511\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1558000: 0.001456\n",
      "Training accuracy: -46.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1559000: 0.001421\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1560000: 0.001947\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1561000: 0.001253\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1562000: 0.001630\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1563000: 0.002486\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1564000: 0.001516\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1565000: 0.001927\n",
      "Training accuracy: 37.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1566000: 0.001618\n",
      "Training accuracy: 115.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1567000: 0.001406\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1568000: 0.001841\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1569000: 0.001835\n",
      "Training accuracy: 103.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1570000: 0.001529\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1571000: 0.001344\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1572000: 0.001656\n",
      "Training accuracy: 165.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1573000: 0.001565\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1574000: 0.001766\n",
      "Training accuracy: 20.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1575000: 0.001343\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1576000: 0.001231\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1577000: 0.001351\n",
      "Training accuracy: 119.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1578000: 0.001654\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1579000: 0.001525\n",
      "Training accuracy: -42.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1580000: 0.001535\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1581000: 0.001835\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1582000: 0.001524\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1583000: 0.001502\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1584000: 0.001441\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1585000: 0.001584\n",
      "Training accuracy: 119.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1586000: 0.001480\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1587000: 0.001457\n",
      "Training accuracy: -165.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1588000: 0.001361\n",
      "Training accuracy: 60.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1589000: 0.001948\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1590000: 0.001488\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1591000: 0.001765\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1592000: 0.002519\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1593000: 0.001768\n",
      "Training accuracy: 6.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1594000: 0.002185\n",
      "Training accuracy: 34.1%\n",
      "Validation accuracy: 90.1%\n",
      "Loss at step 1595000: 0.001460\n",
      "Training accuracy: 111.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1596000: 0.001379\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1597000: 0.001929\n",
      "Training accuracy: -48.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1598000: 0.001598\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1599000: 0.001451\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1600000: 0.001222\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1601000: 0.001688\n",
      "Training accuracy: 143.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1602000: 0.001835\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1603000: 0.001800\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1604000: 0.001258\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1605000: 0.001256\n",
      "Training accuracy: 103.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1606000: 0.001346\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1607000: 0.001711\n",
      "Training accuracy: -64.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1608000: 0.001326\n",
      "Training accuracy: -46.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1609000: 0.001451\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1610000: 0.001830\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1611000: 0.001366\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1612000: 0.001347\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1613000: 0.001702\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1614000: 0.001531\n",
      "Training accuracy: 119.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1615000: 0.001444\n",
      "Training accuracy: 72.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1616000: 0.001433\n",
      "Training accuracy: -57.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1617000: 0.001348\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1618000: 0.001681\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1619000: 0.001455\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1620000: 0.001791\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1621000: 0.002300\n",
      "Training accuracy: 31.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1622000: 0.001607\n",
      "Training accuracy: 30.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1623000: 0.001814\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1624000: 0.001598\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1625000: 0.001327\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1626000: 0.001905\n",
      "Training accuracy: -95.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1627000: 0.001303\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1628000: 0.001287\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1629000: 0.001136\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1630000: 0.001426\n",
      "Training accuracy: 150.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1631000: 0.001836\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1632000: 0.001715\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1633000: 0.001203\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1634000: 0.001229\n",
      "Training accuracy: 107.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1635000: 0.001532\n",
      "Training accuracy: 102.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1636000: 0.001708\n",
      "Training accuracy: -48.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1637000: 0.001336\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1638000: 0.001346\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1639000: 0.001776\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1640000: 0.001381\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1641000: 0.001564\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1642000: 0.001539\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1643000: 0.001500\n",
      "Training accuracy: 138.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1644000: 0.001513\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1645000: 0.001309\n",
      "Training accuracy: -91.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1646000: 0.001282\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1647000: 0.001393\n",
      "Training accuracy: 33.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1648000: 0.001462\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1649000: 0.001690\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1650000: 0.001465\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1651000: 0.001546\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1652000: 0.001501\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1653000: 0.001575\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1654000: 0.001525\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1655000: 0.001574\n",
      "Training accuracy: -87.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1656000: 0.001454\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1657000: 0.001257\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1658000: 0.001107\n",
      "Training accuracy: 102.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1659000: 0.001417\n",
      "Training accuracy: 119.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1660000: 0.001954\n",
      "Training accuracy: 37.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1661000: 0.001613\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1662000: 0.001156\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1663000: 0.001214\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1664000: 0.001444\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1665000: 0.001395\n",
      "Training accuracy: -23.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1666000: 0.001291\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1667000: 0.001260\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1668000: 0.001824\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1669000: 0.001195\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1670000: 0.001605\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1671000: 0.001533\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1672000: 0.001558\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1673000: 0.001390\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1674000: 0.001290\n",
      "Training accuracy: -97.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1675000: 0.001485\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1676000: 0.001307\n",
      "Training accuracy: 44.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1677000: 0.001559\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1678000: 0.001499\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1679000: 0.001495\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1680000: 0.001627\n",
      "Training accuracy: 24.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1681000: 0.001434\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1682000: 0.001374\n",
      "Training accuracy: 102.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1683000: 0.001532\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1684000: 0.001499\n",
      "Training accuracy: -73.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1685000: 0.001657\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1686000: 0.001171\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1687000: 0.001065\n",
      "Training accuracy: 104.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1688000: 0.001384\n",
      "Training accuracy: 116.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1689000: 0.001875\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1690000: 0.001355\n",
      "Training accuracy: 58.9%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1691000: 0.001122\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1692000: 0.001146\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1693000: 0.001571\n",
      "Training accuracy: -34.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1694000: 0.001355\n",
      "Training accuracy: -28.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1695000: 0.001499\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1696000: 0.001198\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1697000: 0.001279\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1698000: 0.001709\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1699000: 0.001604\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1700000: 0.001802\n",
      "Training accuracy: 45.7%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1701000: 0.001545\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1702000: 0.001386\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1703000: 0.001357\n",
      "Training accuracy: -107.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1704000: 0.001691\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1705000: 0.001303\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1706000: 0.001503\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1707000: 0.001698\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1708000: 0.001312\n",
      "Training accuracy: 58.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1709000: 0.001573\n",
      "Training accuracy: 28.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1710000: 0.001419\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1711000: 0.001294\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1712000: 0.001431\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1713000: 0.001361\n",
      "Training accuracy: -64.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1714000: 0.001698\n",
      "Training accuracy: -58.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1715000: 0.001181\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1716000: 0.001097\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1717000: 0.001313\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1718000: 0.001773\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1719000: 0.001340\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1720000: 0.001066\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1721000: 0.001199\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1722000: 0.001565\n",
      "Training accuracy: -85.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1723000: 0.001212\n",
      "Training accuracy: -55.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1724000: 0.001754\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1725000: 0.001020\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1726000: 0.001209\n",
      "Training accuracy: 123.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1727000: 0.002181\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1728000: 0.001540\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1729000: 0.001939\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 90.0%\n",
      "Loss at step 1730000: 0.001241\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1731000: 0.001303\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1732000: 0.001581\n",
      "Training accuracy: 8.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1733000: 0.001670\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1734000: 0.001203\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1735000: 0.001180\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1736000: 0.001396\n",
      "Training accuracy: 142.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1737000: 0.001233\n",
      "Training accuracy: 73.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1738000: 0.001265\n",
      "Training accuracy: 55.6%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1739000: 0.001049\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1740000: 0.001174\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1741000: 0.001342\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1742000: 0.001397\n",
      "Training accuracy: 7.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1743000: 0.001598\n",
      "Training accuracy: -54.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1744000: 0.001203\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1745000: 0.001075\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1746000: 0.001291\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1747000: 0.001389\n",
      "Training accuracy: 61.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1748000: 0.001214\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1749000: 0.001090\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1750000: 0.001271\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1751000: 0.001466\n",
      "Training accuracy: -71.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1752000: 0.001123\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1753000: 0.001783\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1754000: 0.000963\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1755000: 0.001323\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1756000: 0.002279\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1757000: 0.001508\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1758000: 0.001654\n",
      "Training accuracy: 40.8%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1759000: 0.001391\n",
      "Training accuracy: 116.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1760000: 0.001266\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1761000: 0.001492\n",
      "Training accuracy: 1.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1762000: 0.001619\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1763000: 0.001286\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1764000: 0.001124\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1765000: 0.001356\n",
      "Training accuracy: 142.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1766000: 0.001183\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1767000: 0.001428\n",
      "Training accuracy: 34.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1768000: 0.001083\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1769000: 0.000984\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1770000: 0.001299\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1771000: 0.001217\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1772000: 0.001516\n",
      "Training accuracy: -60.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1773000: 0.001179\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1774000: 0.001526\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1775000: 0.001237\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1776000: 0.001213\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1777000: 0.001149\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1778000: 0.001045\n",
      "Training accuracy: 124.2%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1779000: 0.001231\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1780000: 0.001153\n",
      "Training accuracy: -45.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1781000: 0.001120\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1782000: 0.001683\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1783000: 0.000964\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1784000: 0.001309\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1785000: 0.002251\n",
      "Training accuracy: 34.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1786000: 0.001252\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1787000: 0.001637\n",
      "Training accuracy: 35.1%\n",
      "Validation accuracy: 89.9%\n",
      "Loss at step 1788000: 0.001342\n",
      "Training accuracy: 115.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1789000: 0.001113\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1790000: 0.001567\n",
      "Training accuracy: 30.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1791000: 0.001572\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1792000: 0.001237\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1793000: 0.001048\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1794000: 0.001386\n",
      "Training accuracy: 165.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1795000: 0.001277\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1796000: 0.001509\n",
      "Training accuracy: 18.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1797000: 0.001053\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1798000: 0.000924\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1799000: 0.001125\n",
      "Training accuracy: 112.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1800000: 0.001344\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1801000: 0.001249\n",
      "Training accuracy: -43.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1802000: 0.001248\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1803000: 0.001585\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1804000: 0.001238\n",
      "Training accuracy: 52.9%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1805000: 0.001186\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1806000: 0.001184\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1807000: 0.001315\n",
      "Training accuracy: 121.1%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1808000: 0.001200\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1809000: 0.001174\n",
      "Training accuracy: -164.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1810000: 0.001072\n",
      "Training accuracy: 58.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1811000: 0.001724\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1812000: 0.001232\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1813000: 0.001573\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1814000: 0.002334\n",
      "Training accuracy: 27.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1815000: 0.001506\n",
      "Training accuracy: 5.3%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1816000: 0.001923\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1817000: 0.001171\n",
      "Training accuracy: 109.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1818000: 0.001119\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1819000: 0.001672\n",
      "Training accuracy: -51.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1820000: 0.001334\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1821000: 0.001162\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1822000: 0.000911\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1823000: 0.001438\n",
      "Training accuracy: 141.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1824000: 0.001516\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1825000: 0.001451\n",
      "Training accuracy: 27.5%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1826000: 0.000990\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1827000: 0.000969\n",
      "Training accuracy: 103.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1828000: 0.001078\n",
      "Training accuracy: 109.2%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1829000: 0.001427\n",
      "Training accuracy: -74.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1830000: 0.001048\n",
      "Training accuracy: -45.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1831000: 0.001177\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1832000: 0.001593\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1833000: 0.001093\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1834000: 0.001061\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1835000: 0.001465\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1836000: 0.001298\n",
      "Training accuracy: 116.8%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1837000: 0.001183\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1838000: 0.001116\n",
      "Training accuracy: -118.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1839000: 0.001081\n",
      "Training accuracy: 44.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1840000: 0.001451\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1841000: 0.001211\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1842000: 0.001539\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1843000: 0.001814\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1844000: 0.001372\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1845000: 0.001542\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 89.8%\n",
      "Loss at step 1846000: 0.001319\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1847000: 0.001062\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1848000: 0.001689\n",
      "Training accuracy: -102.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1849000: 0.001044\n",
      "Training accuracy: 111.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1850000: 0.001053\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1851000: 0.000871\n",
      "Training accuracy: 101.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1852000: 0.001169\n",
      "Training accuracy: 150.1%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1853000: 0.001593\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1854000: 0.001498\n",
      "Training accuracy: 13.6%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1855000: 0.000956\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1856000: 0.000969\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1857000: 0.001296\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1858000: 0.001385\n",
      "Training accuracy: -40.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1859000: 0.001110\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1860000: 0.001102\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1861000: 0.001574\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1862000: 0.001115\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1863000: 0.001315\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1864000: 0.001314\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1865000: 0.001216\n",
      "Training accuracy: 156.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1866000: 0.001360\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1867000: 0.001057\n",
      "Training accuracy: -94.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1868000: 0.001033\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1869000: 0.001184\n",
      "Training accuracy: 27.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1870000: 0.001234\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1871000: 0.001456\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1872000: 0.001231\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1873000: 0.001315\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1874000: 0.001268\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1875000: 0.001312\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1876000: 0.001322\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1877000: 0.001343\n",
      "Training accuracy: -93.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1878000: 0.001219\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1879000: 0.001033\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1880000: 0.000859\n",
      "Training accuracy: 101.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1881000: 0.001181\n",
      "Training accuracy: 117.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1882000: 0.001750\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1883000: 0.001409\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1884000: 0.000916\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1885000: 0.000964\n",
      "Training accuracy: 103.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1886000: 0.001254\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1887000: 0.001152\n",
      "Training accuracy: -34.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1888000: 0.001063\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1889000: 0.001027\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1890000: 0.001596\n",
      "Training accuracy: 53.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1891000: 0.001253\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1892000: 0.001392\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1893000: 0.001314\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1894000: 0.001371\n",
      "Training accuracy: 111.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1895000: 0.001198\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1896000: 0.001045\n",
      "Training accuracy: -99.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1897000: 0.001278\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1898000: 0.001149\n",
      "Training accuracy: 48.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1899000: 0.001335\n",
      "Training accuracy: 59.0%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1900000: 0.001573\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1901000: 0.001162\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1902000: 0.001428\n",
      "Training accuracy: 23.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1903000: 0.001195\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1904000: 0.001134\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1905000: 0.001324\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1906000: 0.001283\n",
      "Training accuracy: -76.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1907000: 0.001475\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1908000: 0.000953\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1909000: 0.000832\n",
      "Training accuracy: 104.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1910000: 0.001172\n",
      "Training accuracy: 114.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1911000: 0.001648\n",
      "Training accuracy: 34.4%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1912000: 0.001157\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1913000: 0.000847\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1914000: 0.000911\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1915000: 0.001383\n",
      "Training accuracy: -34.5%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1916000: 0.001152\n",
      "Training accuracy: -38.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1917000: 0.001301\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1918000: 0.000981\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1919000: 0.001096\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1920000: 0.001534\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1921000: 0.001425\n",
      "Training accuracy: 44.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1922000: 0.001631\n",
      "Training accuracy: 44.3%\n",
      "Validation accuracy: 89.7%\n",
      "Loss at step 1923000: 0.001362\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1924000: 0.001170\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1925000: 0.001140\n",
      "Training accuracy: -111.3%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1926000: 0.001482\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1927000: 0.001091\n",
      "Training accuracy: 35.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1928000: 0.001293\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1929000: 0.001501\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1930000: 0.001117\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1931000: 0.001376\n",
      "Training accuracy: 20.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1932000: 0.001194\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1933000: 0.001068\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1934000: 0.001241\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1935000: 0.001161\n",
      "Training accuracy: -67.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1936000: 0.001518\n",
      "Training accuracy: -62.2%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1937000: 0.000965\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1938000: 0.000884\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1939000: 0.001110\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1940000: 0.001569\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1941000: 0.001146\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1942000: 0.000863\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1943000: 0.000988\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1944000: 0.001322\n",
      "Training accuracy: -76.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1945000: 0.001024\n",
      "Training accuracy: -65.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1946000: 0.001590\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1947000: 0.000822\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1948000: 0.001004\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1949000: 0.002022\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1950000: 0.001351\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1951000: 0.001796\n",
      "Training accuracy: 24.7%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1952000: 0.001101\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 89.6%\n",
      "Loss at step 1953000: 0.001120\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1954000: 0.001396\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1955000: 0.001504\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1956000: 0.001023\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1957000: 0.000979\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1958000: 0.001133\n",
      "Training accuracy: 147.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1959000: 0.001032\n",
      "Training accuracy: 72.4%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1960000: 0.001092\n",
      "Training accuracy: 53.9%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1961000: 0.000834\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1962000: 0.001000\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1963000: 0.001144\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1964000: 0.001213\n",
      "Training accuracy: 4.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1965000: 0.001421\n",
      "Training accuracy: -53.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1966000: 0.001021\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1967000: 0.000881\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1968000: 0.001113\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1969000: 0.001064\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1970000: 0.001052\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1971000: 0.000893\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1972000: 0.001077\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1973000: 0.001290\n",
      "Training accuracy: -75.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1974000: 0.000956\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1975000: 0.001649\n",
      "Training accuracy: 53.0%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1976000: 0.000812\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1977000: 0.001136\n",
      "Training accuracy: 117.4%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1978000: 0.002149\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1979000: 0.001341\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1980000: 0.001469\n",
      "Training accuracy: 38.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1981000: 0.001197\n",
      "Training accuracy: 116.1%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1982000: 0.001112\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1983000: 0.001286\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1984000: 0.001451\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1985000: 0.001115\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1986000: 0.000936\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1987000: 0.001228\n",
      "Training accuracy: 177.4%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1988000: 0.000991\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1989000: 0.001254\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1990000: 0.000904\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 1991000: 0.000792\n",
      "Training accuracy: 109.6%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1992000: 0.001129\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1993000: 0.001137\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1994000: 0.001357\n",
      "Training accuracy: -61.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1995000: 0.000992\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1996000: 0.001373\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1997000: 0.001070\n",
      "Training accuracy: 69.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1998000: 0.001079\n",
      "Training accuracy: 53.8%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 1999000: 0.000964\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 2000000: 0.000866\n",
      "Training accuracy: 125.5%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2001000: 0.001058\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2002000: 0.000975\n",
      "Training accuracy: -49.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2003000: 0.000943\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2004000: 0.001511\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2005000: 0.001071\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2006000: 0.001288\n",
      "Training accuracy: 104.8%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2007000: 0.002138\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2008000: 0.001124\n",
      "Training accuracy: 4.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2009000: 0.001468\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 89.5%\n",
      "Loss at step 2010000: 0.001268\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2011000: 0.000866\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2012000: 0.001413\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2013000: 0.001422\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2014000: 0.001060\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2015000: 0.000877\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2016000: 0.001237\n",
      "Training accuracy: 167.5%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2017000: 0.001172\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2018000: 0.001381\n",
      "Training accuracy: 16.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2019000: 0.000855\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2020000: 0.000743\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2021000: 0.000944\n",
      "Training accuracy: 114.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2022000: 0.001171\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2023000: 0.001106\n",
      "Training accuracy: -44.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2024000: 0.001087\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2025000: 0.001471\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2026000: 0.001089\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2027000: 0.001022\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2028000: 0.001276\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2029000: 0.001170\n",
      "Training accuracy: 121.9%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2030000: 0.001039\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2031000: 0.000988\n",
      "Training accuracy: -178.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2032000: 0.000910\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2033000: 0.001583\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2034000: 0.001083\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2035000: 0.001396\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2036000: 0.001927\n",
      "Training accuracy: 28.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2037000: 0.001349\n",
      "Training accuracy: 4.6%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2038000: 0.001752\n",
      "Training accuracy: 30.1%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2039000: 0.000996\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2040000: 0.000932\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2041000: 0.001532\n",
      "Training accuracy: -48.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2042000: 0.001190\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2043000: 0.000910\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2044000: 0.000748\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2045000: 0.001019\n",
      "Training accuracy: 145.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2046000: 0.001369\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2047000: 0.001308\n",
      "Training accuracy: 26.3%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2048000: 0.000825\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2049000: 0.000799\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2050000: 0.001181\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2051000: 0.001275\n",
      "Training accuracy: -83.3%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2052000: 0.000980\n",
      "Training accuracy: -51.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2053000: 0.001024\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2054000: 0.001482\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2055000: 0.000937\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2056000: 0.000903\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2057000: 0.001334\n",
      "Training accuracy: 72.2%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2058000: 0.001145\n",
      "Training accuracy: 141.4%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2059000: 0.001296\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2060000: 0.000943\n",
      "Training accuracy: -123.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2061000: 0.000931\n",
      "Training accuracy: 42.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2062000: 0.001320\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2063000: 0.001089\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2064000: 0.001366\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2065000: 0.001705\n",
      "Training accuracy: 27.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2066000: 0.001211\n",
      "Training accuracy: 35.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2067000: 0.001375\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 89.4%\n",
      "Loss at step 2068000: 0.001184\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2069000: 0.000917\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2070000: 0.001571\n",
      "Training accuracy: -99.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2071000: 0.000901\n",
      "Training accuracy: 110.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2072000: 0.000913\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2073000: 0.000723\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2074000: 0.001025\n",
      "Training accuracy: 151.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2075000: 0.001462\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2076000: 0.001357\n",
      "Training accuracy: 12.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2077000: 0.000806\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2078000: 0.000825\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2079000: 0.001165\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2080000: 0.001241\n",
      "Training accuracy: -47.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2081000: 0.000966\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2082000: 0.000954\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2083000: 0.001473\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2084000: 0.000970\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2085000: 0.001260\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2086000: 0.001199\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2087000: 0.001079\n",
      "Training accuracy: 158.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2088000: 0.001235\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2089000: 0.000892\n",
      "Training accuracy: -103.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2090000: 0.001171\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2091000: 0.001034\n",
      "Training accuracy: 27.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2092000: 0.001094\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2093000: 0.001309\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2094000: 0.001110\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2095000: 0.001197\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2096000: 0.001074\n",
      "Training accuracy: 72.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2097000: 0.001127\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2098000: 0.001207\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2099000: 0.001212\n",
      "Training accuracy: -95.8%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2100000: 0.001084\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2101000: 0.000888\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2102000: 0.000708\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2103000: 0.001158\n",
      "Training accuracy: 111.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2104000: 0.001622\n",
      "Training accuracy: 34.9%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2105000: 0.001280\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2106000: 0.000765\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2107000: 0.000801\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2108000: 0.001139\n",
      "Training accuracy: 113.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2109000: 0.001014\n",
      "Training accuracy: -40.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2110000: 0.000917\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2111000: 0.000877\n",
      "Training accuracy: 60.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2112000: 0.001487\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2113000: 0.001113\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2114000: 0.001262\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2115000: 0.001213\n",
      "Training accuracy: 53.7%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2116000: 0.001266\n",
      "Training accuracy: 112.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2117000: 0.001083\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2118000: 0.000917\n",
      "Training accuracy: -106.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2119000: 0.001154\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2120000: 0.001001\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2121000: 0.001191\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2122000: 0.001455\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2123000: 0.001039\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2124000: 0.001298\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2125000: 0.001046\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2126000: 0.000996\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2127000: 0.001187\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2128000: 0.001156\n",
      "Training accuracy: -76.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2129000: 0.001365\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2130000: 0.000833\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2131000: 0.000702\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2132000: 0.001020\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2133000: 0.001541\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2134000: 0.001134\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2135000: 0.000703\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2136000: 0.000883\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2137000: 0.001265\n",
      "Training accuracy: -35.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2138000: 0.000930\n",
      "Training accuracy: -40.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2139000: 0.001188\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2140000: 0.000841\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2141000: 0.000970\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2142000: 0.001431\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2143000: 0.001247\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2144000: 0.001533\n",
      "Training accuracy: 41.5%\n",
      "Validation accuracy: 89.3%\n",
      "Loss at step 2145000: 0.001260\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2146000: 0.001065\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2147000: 0.001029\n",
      "Training accuracy: -118.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2148000: 0.001372\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2149000: 0.000966\n",
      "Training accuracy: 34.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2150000: 0.000897\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2151000: 0.001220\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2152000: 0.000980\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2153000: 0.001226\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2154000: 0.001050\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2155000: 0.000869\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2156000: 0.001140\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2157000: 0.001046\n",
      "Training accuracy: -65.8%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2158000: 0.001407\n",
      "Training accuracy: -61.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2159000: 0.000853\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2160000: 0.000761\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2161000: 0.000989\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2162000: 0.001371\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2163000: 0.001021\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2164000: 0.000772\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2165000: 0.000874\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2166000: 0.001210\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2167000: 0.000924\n",
      "Training accuracy: -73.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2168000: 0.001485\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2169000: 0.000686\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2170000: 0.000874\n",
      "Training accuracy: 124.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2171000: 0.001931\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2172000: 0.001248\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2173000: 0.001427\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2174000: 0.001010\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2175000: 0.001019\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2176000: 0.001283\n",
      "Training accuracy: 2.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2177000: 0.001407\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2178000: 0.000906\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2179000: 0.000865\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2180000: 0.001036\n",
      "Training accuracy: 149.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2181000: 0.000927\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2182000: 0.000979\n",
      "Training accuracy: 53.4%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2183000: 0.000713\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2184000: 0.000887\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2185000: 0.001042\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2186000: 0.001105\n",
      "Training accuracy: -1.4%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2187000: 0.001303\n",
      "Training accuracy: -52.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2188000: 0.000887\n",
      "Training accuracy: 107.0%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2189000: 0.001049\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2190000: 0.001001\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2191000: 0.000915\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2192000: 0.000976\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2193000: 0.000778\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2194000: 0.000962\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2195000: 0.000919\n",
      "Training accuracy: -72.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2196000: 0.000837\n",
      "Training accuracy: 61.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2197000: 0.001444\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2198000: 0.000683\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2199000: 0.001035\n",
      "Training accuracy: 117.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2200000: 0.002076\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2201000: 0.001263\n",
      "Training accuracy: 70.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2202000: 0.001363\n",
      "Training accuracy: 36.9%\n",
      "Validation accuracy: 89.2%\n",
      "Loss at step 2203000: 0.001083\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2204000: 0.000847\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2205000: 0.001299\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2206000: 0.001341\n",
      "Training accuracy: 101.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2207000: 0.001005\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2208000: 0.000802\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2209000: 0.001137\n",
      "Training accuracy: 178.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2210000: 0.000871\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2211000: 0.001175\n",
      "Training accuracy: 24.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2212000: 0.000799\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2213000: 0.000657\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2214000: 0.001045\n",
      "Training accuracy: 114.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2215000: 0.001026\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2216000: 0.001257\n",
      "Training accuracy: -60.4%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2217000: 0.000875\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2218000: 0.001342\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2219000: 0.000969\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2220000: 0.000953\n",
      "Training accuracy: 53.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2221000: 0.000869\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2222000: 0.000772\n",
      "Training accuracy: 126.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2223000: 0.000951\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2224000: 0.000864\n",
      "Training accuracy: -50.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2225000: 0.000836\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2226000: 0.001415\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2227000: 0.000987\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2228000: 0.001170\n",
      "Training accuracy: 105.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2229000: 0.002087\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2230000: 0.001217\n",
      "Training accuracy: 5.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2231000: 0.001360\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2232000: 0.001178\n",
      "Training accuracy: 106.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2233000: 0.000761\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2234000: 0.001315\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2235000: 0.001135\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2236000: 0.000956\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2237000: 0.000787\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2238000: 0.001148\n",
      "Training accuracy: 168.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2239000: 0.001322\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2240000: 0.001382\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2241000: 0.000776\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2242000: 0.000664\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2243000: 0.000836\n",
      "Training accuracy: 116.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2244000: 0.001070\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2245000: 0.001013\n",
      "Training accuracy: -43.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2246000: 0.000967\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2247000: 0.001406\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2248000: 0.000878\n",
      "Training accuracy: 55.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2249000: 0.000932\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2250000: 0.001221\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2251000: 0.001083\n",
      "Training accuracy: 122.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2252000: 0.000950\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2253000: 0.000873\n",
      "Training accuracy: -197.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2254000: 0.000815\n",
      "Training accuracy: 53.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2255000: 0.001527\n",
      "Training accuracy: 40.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2256000: 0.001003\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2257000: 0.001301\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2258000: 0.001865\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2259000: 0.001298\n",
      "Training accuracy: 19.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2260000: 0.001624\n",
      "Training accuracy: 48.8%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2261000: 0.000898\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2262000: 0.000834\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2263000: 0.001438\n",
      "Training accuracy: -48.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2264000: 0.001083\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2265000: 0.000803\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2266000: 0.000628\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2267000: 0.000932\n",
      "Training accuracy: 146.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2268000: 0.001278\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2269000: 0.001238\n",
      "Training accuracy: 22.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2270000: 0.000740\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2271000: 0.000698\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2272000: 0.001140\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2273000: 0.001179\n",
      "Training accuracy: -89.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2274000: 0.000905\n",
      "Training accuracy: -52.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2275000: 0.000915\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2276000: 0.001420\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2277000: 0.000858\n",
      "Training accuracy: 59.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2278000: 0.000806\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2279000: 0.001170\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2280000: 0.001066\n",
      "Training accuracy: 142.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2281000: 0.001120\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2282000: 0.000860\n",
      "Training accuracy: -130.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2283000: 0.000823\n",
      "Training accuracy: 41.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2284000: 0.001234\n",
      "Training accuracy: 45.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2285000: 0.001007\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2286000: 0.001275\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2287000: 0.001628\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2288000: 0.001141\n",
      "Training accuracy: 34.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2289000: 0.001278\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2290000: 0.001096\n",
      "Training accuracy: 112.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2291000: 0.000820\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2292000: 0.001482\n",
      "Training accuracy: -100.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2293000: 0.001069\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2294000: 0.000834\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2295000: 0.000634\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2296000: 0.000950\n",
      "Training accuracy: 152.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2297000: 0.001395\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2298000: 0.001283\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2299000: 0.000720\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2300000: 0.000716\n",
      "Training accuracy: 106.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2301000: 0.001084\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2302000: 0.001152\n",
      "Training accuracy: -56.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2303000: 0.000878\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2304000: 0.000836\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2305000: 0.001415\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2306000: 0.000904\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2307000: 0.001182\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2308000: 0.001134\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2309000: 0.001229\n",
      "Training accuracy: 118.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2310000: 0.001172\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2311000: 0.000830\n",
      "Training accuracy: -111.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2312000: 0.001089\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2313000: 0.000938\n",
      "Training accuracy: 26.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2314000: 0.001012\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2315000: 0.001218\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2316000: 0.001032\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2317000: 0.001122\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2318000: 0.000989\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2319000: 0.001023\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2320000: 0.001151\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2321000: 0.001136\n",
      "Training accuracy: -97.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2322000: 0.000999\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2323000: 0.000823\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2324000: 0.000613\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2325000: 0.001085\n",
      "Training accuracy: 110.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2326000: 0.001550\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2327000: 0.001236\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2328000: 0.000684\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2329000: 0.000702\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2330000: 0.001071\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2331000: 0.000935\n",
      "Training accuracy: -46.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2332000: 0.001146\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2333000: 0.000802\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2334000: 0.001148\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2335000: 0.001020\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2336000: 0.001090\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2337000: 0.001130\n",
      "Training accuracy: 43.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2338000: 0.001205\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2339000: 0.001022\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2340000: 0.000938\n",
      "Training accuracy: -119.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2341000: 0.001077\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2342000: 0.000916\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2343000: 0.001114\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2344000: 0.001367\n",
      "Training accuracy: 101.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2345000: 0.000960\n",
      "Training accuracy: 30.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2346000: 0.001222\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2347000: 0.000960\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2348000: 0.000913\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2349000: 0.001025\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2350000: 0.000979\n",
      "Training accuracy: -72.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2351000: 0.001350\n",
      "Training accuracy: -61.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2352000: 0.000764\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2353000: 0.000674\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2354000: 0.000944\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2355000: 0.001457\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2356000: 0.001072\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2357000: 0.000614\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2358000: 0.000801\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2359000: 0.001192\n",
      "Training accuracy: -68.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2360000: 0.000849\n",
      "Training accuracy: -45.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2361000: 0.001128\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2362000: 0.000765\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2363000: 0.000843\n",
      "Training accuracy: 109.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2364000: 0.001673\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2365000: 0.001168\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2366000: 0.001472\n",
      "Training accuracy: 39.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2367000: 0.001192\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2368000: 0.000998\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2369000: 0.000952\n",
      "Training accuracy: -123.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2370000: 0.001311\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2371000: 0.000902\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2372000: 0.000825\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2373000: 0.001165\n",
      "Training accuracy: 107.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2374000: 0.000892\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2375000: 0.000920\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2376000: 0.000964\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2377000: 0.000791\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2378000: 0.001085\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2379000: 0.000979\n",
      "Training accuracy: -67.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2380000: 0.001256\n",
      "Training accuracy: -56.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2381000: 0.000780\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2382000: 0.000674\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2383000: 0.001069\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2384000: 0.001039\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2385000: 0.000866\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2386000: 0.000679\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2387000: 0.000773\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2388000: 0.001146\n",
      "Training accuracy: -77.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2389000: 0.000897\n",
      "Training accuracy: -95.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2390000: 0.001425\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2391000: 0.000623\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2392000: 0.000881\n",
      "Training accuracy: 120.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2393000: 0.001880\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2394000: 0.001180\n",
      "Training accuracy: 71.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2395000: 0.001360\n",
      "Training accuracy: 22.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2396000: 0.000955\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2397000: 0.000949\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2398000: 0.001221\n",
      "Training accuracy: -0.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2399000: 0.001349\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2400000: 0.000812\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2401000: 0.000784\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2402000: 0.001066\n",
      "Training accuracy: 144.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2403000: 0.000849\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2404000: 0.000873\n",
      "Training accuracy: 38.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2405000: 0.000642\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2406000: 0.000831\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2407000: 0.000988\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2408000: 0.001039\n",
      "Training accuracy: -2.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2409000: 0.001244\n",
      "Training accuracy: -58.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2410000: 0.000854\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2411000: 0.001274\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2412000: 0.000935\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2413000: 0.000855\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2414000: 0.000905\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2415000: 0.000703\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2416000: 0.000918\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2417000: 0.000821\n",
      "Training accuracy: -54.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2418000: 0.000793\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2419000: 0.001394\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2420000: 0.000622\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2421000: 0.000975\n",
      "Training accuracy: 117.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2422000: 0.002035\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2423000: 0.001208\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2424000: 0.001298\n",
      "Training accuracy: 35.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2425000: 0.001010\n",
      "Training accuracy: 113.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2426000: 0.000777\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2427000: 0.001248\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2428000: 0.001281\n",
      "Training accuracy: 101.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2429000: 0.000934\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2430000: 0.000732\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2431000: 0.001088\n",
      "Training accuracy: 179.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2432000: 0.000800\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2433000: 0.001120\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2434000: 0.000736\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2435000: 0.000584\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2436000: 0.001003\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2437000: 0.000975\n",
      "Training accuracy: 54.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2438000: 0.000936\n",
      "Training accuracy: -57.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2439000: 0.000810\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2440000: 0.001294\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2441000: 0.000909\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2442000: 0.000891\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2443000: 0.000945\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2444000: 0.000713\n",
      "Training accuracy: 127.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2445000: 0.000904\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2446000: 0.000810\n",
      "Training accuracy: -51.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2447000: 0.000763\n",
      "Training accuracy: 54.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2448000: 0.001368\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2449000: 0.000944\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2450000: 0.001100\n",
      "Training accuracy: 107.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2451000: 0.002077\n",
      "Training accuracy: 5.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2452000: 0.001253\n",
      "Training accuracy: 0.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2453000: 0.001300\n",
      "Training accuracy: 29.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2454000: 0.000866\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2455000: 0.000774\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2456000: 0.001299\n",
      "Training accuracy: -47.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2457000: 0.001057\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2458000: 0.000900\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2459000: 0.000729\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2460000: 0.001248\n",
      "Training accuracy: 164.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2461000: 0.001282\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2462000: 0.001328\n",
      "Training accuracy: 10.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2463000: 0.000707\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2464000: 0.000600\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2465000: 0.000769\n",
      "Training accuracy: 118.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2466000: 0.001061\n",
      "Training accuracy: -81.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2467000: 0.000962\n",
      "Training accuracy: -44.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2468000: 0.000893\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2469000: 0.001390\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2470000: 0.000809\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2471000: 0.000868\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2472000: 0.001161\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2473000: 0.001026\n",
      "Training accuracy: 124.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2474000: 0.000893\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2475000: 0.000821\n",
      "Training accuracy: -202.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2476000: 0.000781\n",
      "Training accuracy: 40.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2477000: 0.001175\n",
      "Training accuracy: 41.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2478000: 0.000938\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2479000: 0.001246\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2480000: 0.001846\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2481000: 0.001239\n",
      "Training accuracy: 22.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2482000: 0.001571\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2483000: 0.000834\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2484000: 0.000773\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2485000: 0.001311\n",
      "Training accuracy: -42.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2486000: 0.001044\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2487000: 0.000750\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2488000: 0.000577\n",
      "Training accuracy: 89.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2489000: 0.000897\n",
      "Training accuracy: 150.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2490000: 0.001222\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2491000: 0.001214\n",
      "Training accuracy: 34.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2492000: 0.000683\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2493000: 0.000651\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2494000: 0.001091\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2495000: 0.001117\n",
      "Training accuracy: -93.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2496000: 0.000812\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2497000: 0.000840\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2498000: 0.001335\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2499000: 0.000799\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2500000: 0.001063\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2501000: 0.001115\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2502000: 0.001020\n",
      "Training accuracy: 144.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2503000: 0.001070\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2504000: 0.000797\n",
      "Training accuracy: -133.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2505000: 0.000769\n",
      "Training accuracy: 41.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2506000: 0.001174\n",
      "Training accuracy: 43.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2507000: 0.000954\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2508000: 0.001219\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2509000: 0.001275\n",
      "Training accuracy: 29.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2510000: 0.001094\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2511000: 0.001223\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2512000: 0.001050\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2513000: 0.000778\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2514000: 0.001439\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2515000: 0.001006\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2516000: 0.000754\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2517000: 0.000569\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2518000: 0.000902\n",
      "Training accuracy: 152.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2519000: 0.001353\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2520000: 0.001201\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2521000: 0.000676\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2522000: 0.000664\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2523000: 0.001039\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2524000: 0.001120\n",
      "Training accuracy: -59.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2525000: 0.000823\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2526000: 0.000785\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2527000: 0.001394\n",
      "Training accuracy: 72.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2528000: 0.000712\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2529000: 0.001151\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2530000: 0.001087\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2531000: 0.001198\n",
      "Training accuracy: 118.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2532000: 0.001125\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2533000: 0.000779\n",
      "Training accuracy: -115.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2534000: 0.000995\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2535000: 0.000889\n",
      "Training accuracy: 24.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2536000: 0.000960\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2537000: 0.001130\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2538000: 0.000976\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2539000: 0.001077\n",
      "Training accuracy: 29.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2540000: 0.000919\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2541000: 0.000958\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2542000: 0.001111\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2543000: 0.001087\n",
      "Training accuracy: -98.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2544000: 0.001248\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2545000: 0.000770\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2546000: 0.000567\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2547000: 0.000951\n",
      "Training accuracy: 117.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2548000: 0.001511\n",
      "Training accuracy: 34.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2549000: 0.001202\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2550000: 0.000637\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2551000: 0.000627\n",
      "Training accuracy: 93.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2552000: 0.001037\n",
      "Training accuracy: 115.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2553000: 0.000886\n",
      "Training accuracy: -50.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2554000: 0.001099\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2555000: 0.000714\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2556000: 0.000816\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2557000: 0.000975\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2558000: 0.001175\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2559000: 0.001095\n",
      "Training accuracy: 42.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2560000: 0.001169\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2561000: 0.000966\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2562000: 0.000879\n",
      "Training accuracy: -122.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2563000: 0.001322\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2564000: 0.000838\n",
      "Training accuracy: 44.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2565000: 0.001066\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2566000: 0.001317\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2567000: 0.000900\n",
      "Training accuracy: 30.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2568000: 0.001176\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2569000: 0.000908\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2570000: 0.000864\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2571000: 0.000992\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2572000: 0.000927\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2573000: 0.001314\n",
      "Training accuracy: -64.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2574000: 0.000727\n",
      "Training accuracy: 98.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2575000: 0.000631\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2576000: 0.000893\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2577000: 0.001430\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2578000: 0.001040\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2579000: 0.000565\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2580000: 0.000745\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2581000: 0.001152\n",
      "Training accuracy: -68.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2582000: 0.000790\n",
      "Training accuracy: -48.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2583000: 0.001097\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2584000: 0.000713\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2585000: 0.000813\n",
      "Training accuracy: 109.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2586000: 0.001723\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2587000: 0.001125\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2588000: 0.001292\n",
      "Training accuracy: 43.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2589000: 0.001156\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2590000: 0.000950\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2591000: 0.000902\n",
      "Training accuracy: -127.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2592000: 0.001278\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2593000: 0.000853\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2594000: 0.000769\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2595000: 0.001127\n",
      "Training accuracy: 106.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2596000: 0.000804\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2597000: 0.000783\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2598000: 0.000928\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2599000: 0.000725\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2600000: 0.000999\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2601000: 0.000889\n",
      "Training accuracy: 5.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2602000: 0.001226\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2603000: 0.000741\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2604000: 0.000632\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2605000: 0.000879\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2606000: 0.000983\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2607000: 0.000823\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2608000: 0.000657\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2609000: 0.000724\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2610000: 0.001103\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2611000: 0.000791\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2612000: 0.001396\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2613000: 0.000578\n",
      "Training accuracy: 69.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2614000: 0.000820\n",
      "Training accuracy: 121.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2615000: 0.001867\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2616000: 0.001136\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2617000: 0.001342\n",
      "Training accuracy: 19.7%\n",
      "Validation accuracy: 89.1%\n",
      "Loss at step 2618000: 0.000920\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2619000: 0.000933\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2620000: 0.001194\n",
      "Training accuracy: -2.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2621000: 0.001277\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2622000: 0.000769\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2623000: 0.000740\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2624000: 0.001041\n",
      "Training accuracy: 144.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2625000: 0.000834\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2626000: 0.000822\n",
      "Training accuracy: 34.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2627000: 0.000595\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2628000: 0.000787\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2629000: 0.000949\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2630000: 0.000979\n",
      "Training accuracy: -2.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2631000: 0.001193\n",
      "Training accuracy: -58.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2632000: 0.000811\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2633000: 0.001241\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2634000: 0.000878\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2635000: 0.000811\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2636000: 0.000831\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2637000: 0.000659\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2638000: 0.000864\n",
      "Training accuracy: 70.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2639000: 0.000780\n",
      "Training accuracy: -54.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2640000: 0.000752\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2641000: 0.001387\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2642000: 0.000596\n",
      "Training accuracy: 58.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2643000: 0.000946\n",
      "Training accuracy: 117.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2644000: 0.002015\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2645000: 0.000861\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2646000: 0.001264\n",
      "Training accuracy: 34.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2647000: 0.000981\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2648000: 0.000738\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2649000: 0.001227\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2650000: 0.001230\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2651000: 0.000900\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2652000: 0.000697\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2653000: 0.001063\n",
      "Training accuracy: 180.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2654000: 0.000778\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2655000: 0.001194\n",
      "Training accuracy: 14.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2656000: 0.000701\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2657000: 0.000540\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2658000: 0.000973\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2659000: 0.000944\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2660000: 0.000904\n",
      "Training accuracy: -58.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2661000: 0.000785\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2662000: 0.001278\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2663000: 0.000877\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2664000: 0.000833\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2665000: 0.000906\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2666000: 0.000669\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2667000: 0.000870\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2668000: 0.000811\n",
      "Training accuracy: -181.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2669000: 0.000704\n",
      "Training accuracy: 53.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2670000: 0.001338\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2671000: 0.000926\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2672000: 0.001152\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2673000: 0.002079\n",
      "Training accuracy: 17.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2674000: 0.001199\n",
      "Training accuracy: 0.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2675000: 0.001263\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2676000: 0.000817\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2677000: 0.000735\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2678000: 0.001390\n",
      "Training accuracy: -54.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2679000: 0.001027\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2680000: 0.000856\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2681000: 0.000700\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2682000: 0.001190\n",
      "Training accuracy: 144.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2683000: 0.001268\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2684000: 0.001298\n",
      "Training accuracy: 10.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2685000: 0.000669\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2686000: 0.000563\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2687000: 0.000735\n",
      "Training accuracy: 118.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2688000: 0.001028\n",
      "Training accuracy: -84.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2689000: 0.000627\n",
      "Training accuracy: -40.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2690000: 0.000858\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2691000: 0.001375\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2692000: 0.000759\n",
      "Training accuracy: 55.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2693000: 0.000824\n",
      "Training accuracy: 49.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2694000: 0.001138\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2695000: 0.000991\n",
      "Training accuracy: 123.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2696000: 0.000898\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2697000: 0.000784\n",
      "Training accuracy: -203.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2698000: 0.000744\n",
      "Training accuracy: 40.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2699000: 0.001146\n",
      "Training accuracy: 40.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2700000: 0.000915\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2701000: 0.001231\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2702000: 0.001824\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2703000: 0.001076\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2704000: 0.001537\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2705000: 0.000908\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2706000: 0.000740\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2707000: 0.001285\n",
      "Training accuracy: -44.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2708000: 0.000683\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2709000: 0.000733\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2710000: 0.000544\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2711000: 0.000871\n",
      "Training accuracy: 153.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2712000: 0.001293\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2713000: 0.001201\n",
      "Training accuracy: 31.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2714000: 0.000646\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2715000: 0.000621\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2716000: 0.001071\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2717000: 0.001088\n",
      "Training accuracy: -97.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2718000: 0.000777\n",
      "Training accuracy: 95.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2719000: 0.000795\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2720000: 0.001325\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2721000: 0.000789\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2722000: 0.001039\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2723000: 0.001072\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2724000: 0.000994\n",
      "Training accuracy: 144.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2725000: 0.001050\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2726000: 0.000777\n",
      "Training accuracy: -136.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2727000: 0.000751\n",
      "Training accuracy: 39.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2728000: 0.001136\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2729000: 0.000921\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2730000: 0.001166\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2731000: 0.001173\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2732000: 0.001062\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2733000: 0.001204\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2734000: 0.001012\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2735000: 0.000747\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2736000: 0.001407\n",
      "Training accuracy: -101.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2737000: 0.000970\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2738000: 0.000728\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2739000: 0.000557\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2740000: 0.000876\n",
      "Training accuracy: 153.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2741000: 0.001332\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2742000: 0.001179\n",
      "Training accuracy: 33.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2743000: 0.000626\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2744000: 0.000634\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2745000: 0.000978\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2746000: 0.001098\n",
      "Training accuracy: -63.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2747000: 0.000788\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2748000: 0.000754\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2749000: 0.001397\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2750000: 0.000674\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2751000: 0.001114\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2752000: 0.001085\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2753000: 0.001150\n",
      "Training accuracy: 115.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2754000: 0.001121\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2755000: 0.000753\n",
      "Training accuracy: -118.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2756000: 0.000983\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2757000: 0.000847\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2758000: 0.000942\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2759000: 0.001106\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2760000: 0.000946\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2761000: 0.001051\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2762000: 0.000862\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2763000: 0.000922\n",
      "Training accuracy: 100.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2764000: 0.001075\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2765000: 0.001051\n",
      "Training accuracy: -95.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2766000: 0.001212\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2767000: 0.000734\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2768000: 0.000550\n",
      "Training accuracy: 100.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2769000: 0.000906\n",
      "Training accuracy: 118.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2770000: 0.001437\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2771000: 0.001181\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2772000: 0.000598\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2773000: 0.000587\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2774000: 0.001009\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2775000: 0.000852\n",
      "Training accuracy: -53.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2776000: 0.001074\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2777000: 0.000674\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2778000: 0.000791\n",
      "Training accuracy: 59.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2779000: 0.000948\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2780000: 0.001159\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2781000: 0.001074\n",
      "Training accuracy: 41.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2782000: 0.001147\n",
      "Training accuracy: 114.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2783000: 0.000940\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2784000: 0.000853\n",
      "Training accuracy: -125.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2785000: 0.001286\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2786000: 0.000771\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2787000: 0.001028\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2788000: 0.001272\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2789000: 0.000863\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2790000: 0.001145\n",
      "Training accuracy: 26.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2791000: 0.000881\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2792000: 0.000823\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2793000: 0.001009\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2794000: 0.000886\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2795000: 0.001297\n",
      "Training accuracy: -64.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2796000: 0.000686\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2797000: 0.000601\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2798000: 0.000865\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2799000: 0.001396\n",
      "Training accuracy: 28.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2800000: 0.000929\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2801000: 0.000538\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2802000: 0.000718\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2803000: 0.001108\n",
      "Training accuracy: -74.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2804000: 0.000798\n",
      "Training accuracy: -75.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2805000: 0.001083\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2806000: 0.000676\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2807000: 0.000799\n",
      "Training accuracy: 109.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2808000: 0.001810\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2809000: 0.001124\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2810000: 0.001548\n",
      "Training accuracy: 36.5%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2811000: 0.001138\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2812000: 0.000919\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2813000: 0.000831\n",
      "Training accuracy: 1.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2814000: 0.001285\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2815000: 0.000824\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2816000: 0.000733\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2817000: 0.001008\n",
      "Training accuracy: 111.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2818000: 0.000739\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2819000: 0.000776\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2820000: 0.000906\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2821000: 0.000695\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2822000: 0.000983\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2823000: 0.000898\n",
      "Training accuracy: 5.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2824000: 0.001200\n",
      "Training accuracy: -55.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2825000: 0.000728\n",
      "Training accuracy: 103.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2826000: 0.000595\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2827000: 0.000851\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2828000: 0.000944\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2829000: 0.000795\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2830000: 0.000634\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2831000: 0.000741\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2832000: 0.001086\n",
      "Training accuracy: -77.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2833000: 0.000778\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2834000: 0.001382\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2835000: 0.000548\n",
      "Training accuracy: 69.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2836000: 0.000896\n",
      "Training accuracy: 116.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2837000: 0.001860\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2838000: 0.001117\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2839000: 0.001319\n",
      "Training accuracy: 18.4%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2840000: 0.000930\n",
      "Training accuracy: 116.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2841000: 0.000883\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2842000: 0.001165\n",
      "Training accuracy: -2.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2843000: 0.001254\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2844000: 0.000747\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2845000: 0.000712\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2846000: 0.001001\n",
      "Training accuracy: 146.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2847000: 0.000811\n",
      "Training accuracy: 62.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2848000: 0.001078\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2849000: 0.000571\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2850000: 0.000645\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2851000: 0.000924\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2852000: 0.000960\n",
      "Training accuracy: -2.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2853000: 0.001191\n",
      "Training accuracy: -58.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2854000: 0.000780\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2855000: 0.001242\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2856000: 0.000849\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2857000: 0.000789\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2858000: 0.000801\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2859000: 0.000641\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2860000: 0.000838\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2861000: 0.000754\n",
      "Training accuracy: -54.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2862000: 0.000743\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2863000: 0.001366\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2864000: 0.000576\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2865000: 0.000933\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2866000: 0.001989\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2867000: 0.000821\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2868000: 0.001248\n",
      "Training accuracy: 32.7%\n",
      "Validation accuracy: 89.0%\n",
      "Loss at step 2869000: 0.000963\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2870000: 0.000721\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2871000: 0.001249\n",
      "Training accuracy: 27.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2872000: 0.001200\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2873000: 0.000876\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2874000: 0.000681\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2875000: 0.001066\n",
      "Training accuracy: 180.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2876000: 0.000733\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2877000: 0.001175\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2878000: 0.000670\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2879000: 0.000520\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2880000: 0.000957\n",
      "Training accuracy: 124.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2881000: 0.000937\n",
      "Training accuracy: 54.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2882000: 0.000893\n",
      "Training accuracy: -58.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2883000: 0.000853\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2884000: 0.001309\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2885000: 0.000859\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2886000: 0.000807\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2887000: 0.000886\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2888000: 0.000660\n",
      "Training accuracy: 127.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2889000: 0.000865\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2890000: 0.000803\n",
      "Training accuracy: -183.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2891000: 0.000682\n",
      "Training accuracy: 53.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2892000: 0.001400\n",
      "Training accuracy: 38.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2893000: 0.000890\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2894000: 0.001121\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2895000: 0.002086\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2896000: 0.001183\n",
      "Training accuracy: 0.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2897000: 0.001228\n",
      "Training accuracy: 28.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2898000: 0.000791\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2899000: 0.000716\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2900000: 0.001381\n",
      "Training accuracy: -55.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2901000: 0.000995\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2902000: 0.000839\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2903000: 0.000666\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2904000: 0.001170\n",
      "Training accuracy: 146.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2905000: 0.001244\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2906000: 0.001275\n",
      "Training accuracy: 10.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2907000: 0.000648\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2908000: 0.000540\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2909000: 0.000713\n",
      "Training accuracy: 119.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2910000: 0.001001\n",
      "Training accuracy: -89.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2911000: 0.000710\n",
      "Training accuracy: -46.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2912000: 0.000846\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2913000: 0.001364\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2914000: 0.000744\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2915000: 0.000798\n",
      "Training accuracy: 49.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2916000: 0.001136\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2917000: 0.000983\n",
      "Training accuracy: 123.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2918000: 0.000889\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2919000: 0.000762\n",
      "Training accuracy: -205.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2920000: 0.000736\n",
      "Training accuracy: 38.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2921000: 0.001140\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2922000: 0.000896\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2923000: 0.001208\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2924000: 0.001815\n",
      "Training accuracy: 27.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2925000: 0.001043\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2926000: 0.001523\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2927000: 0.000982\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2928000: 0.000721\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2929000: 0.001345\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2930000: 0.000682\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2931000: 0.000709\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2932000: 0.000526\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2933000: 0.000858\n",
      "Training accuracy: 155.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2934000: 0.001277\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2935000: 0.001185\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2936000: 0.000631\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2937000: 0.000592\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2938000: 0.001020\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2939000: 0.001101\n",
      "Training accuracy: -90.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2940000: 0.000758\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2941000: 0.000773\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2942000: 0.001324\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2943000: 0.000803\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2944000: 0.001026\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2945000: 0.001040\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2946000: 0.000967\n",
      "Training accuracy: 145.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2947000: 0.001014\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2948000: 0.000789\n",
      "Training accuracy: -133.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2949000: 0.000686\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2950000: 0.001161\n",
      "Training accuracy: 18.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2951000: 0.000910\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2952000: 0.001124\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2953000: 0.001032\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2954000: 0.001026\n",
      "Training accuracy: 32.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2955000: 0.000909\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2956000: 0.000999\n",
      "Training accuracy: 111.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2957000: 0.001034\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2958000: 0.001393\n",
      "Training accuracy: -102.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2959000: 0.000918\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2960000: 0.000704\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2961000: 0.000530\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2962000: 0.000860\n",
      "Training accuracy: 154.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2963000: 0.001412\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2964000: 0.001226\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2965000: 0.000598\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2966000: 0.000616\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2967000: 0.000964\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2968000: 0.000919\n",
      "Training accuracy: -60.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2969000: 0.000768\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2970000: 0.000728\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2971000: 0.001395\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2972000: 0.000639\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2973000: 0.001090\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2974000: 0.001069\n",
      "Training accuracy: 71.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2975000: 0.001133\n",
      "Training accuracy: 116.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2976000: 0.001077\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2977000: 0.000730\n",
      "Training accuracy: -120.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2978000: 0.000956\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2979000: 0.000822\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2980000: 0.000927\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2981000: 0.000986\n",
      "Training accuracy: 105.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2982000: 0.000919\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2983000: 0.001033\n",
      "Training accuracy: 30.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2984000: 0.000846\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2985000: 0.000866\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2986000: 0.001077\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2987000: 0.001036\n",
      "Training accuracy: -95.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2988000: 0.001201\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2989000: 0.000704\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2990000: 0.000534\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2991000: 0.000889\n",
      "Training accuracy: 118.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2992000: 0.001411\n",
      "Training accuracy: 41.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2993000: 0.000871\n",
      "Training accuracy: 53.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2994000: 0.000583\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2995000: 0.000565\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2996000: 0.001045\n",
      "Training accuracy: -23.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2997000: 0.000832\n",
      "Training accuracy: -55.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2998000: 0.001054\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 2999000: 0.000640\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3000000: 0.000773\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3001000: 0.000943\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3002000: 0.001139\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3003000: 0.001382\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3004000: 0.001138\n",
      "Training accuracy: 114.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3005000: 0.000924\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3006000: 0.000836\n",
      "Training accuracy: -128.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3007000: 0.001260\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3008000: 0.000767\n",
      "Training accuracy: 34.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3009000: 0.001030\n",
      "Training accuracy: 68.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3010000: 0.001236\n",
      "Training accuracy: 103.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3011000: 0.000835\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3012000: 0.001134\n",
      "Training accuracy: 26.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3013000: 0.000855\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3014000: 0.000796\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3015000: 0.000990\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3016000: 0.000818\n",
      "Training accuracy: -46.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3017000: 0.001296\n",
      "Training accuracy: -64.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3018000: 0.000676\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3019000: 0.000592\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3020000: 0.000831\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3021000: 0.001388\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3022000: 0.000914\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3023000: 0.000518\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3024000: 0.000695\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3025000: 0.001151\n",
      "Training accuracy: -90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3026000: 0.000771\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3027000: 0.001387\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3028000: 0.000555\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3029000: 0.000719\n",
      "Training accuracy: 127.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3030000: 0.001899\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3031000: 0.001104\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3032000: 0.001547\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3033000: 0.001121\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3034000: 0.000901\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3035000: 0.000808\n",
      "Training accuracy: 1.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3036000: 0.001280\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3037000: 0.000728\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3038000: 0.000718\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3039000: 0.000980\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3040000: 0.000700\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3041000: 0.000756\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3042000: 0.000886\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3043000: 0.000680\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3044000: 0.000960\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3045000: 0.000964\n",
      "Training accuracy: 0.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3046000: 0.001188\n",
      "Training accuracy: -55.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3047000: 0.000720\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3048000: 0.000577\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3049000: 0.000847\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3050000: 0.000918\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3051000: 0.000783\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3052000: 0.000626\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3053000: 0.000735\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3054000: 0.001073\n",
      "Training accuracy: -77.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3055000: 0.000760\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3056000: 0.001364\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3057000: 0.000523\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3058000: 0.000875\n",
      "Training accuracy: 118.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3059000: 0.001846\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3060000: 0.001103\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3061000: 0.001293\n",
      "Training accuracy: 18.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3062000: 0.000958\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3063000: 0.000870\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3064000: 0.001203\n",
      "Training accuracy: -20.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3065000: 0.001231\n",
      "Training accuracy: 100.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3066000: 0.000726\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3067000: 0.000698\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3068000: 0.001001\n",
      "Training accuracy: 145.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3069000: 0.000790\n",
      "Training accuracy: 59.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3070000: 0.001063\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3071000: 0.000554\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3072000: 0.000526\n",
      "Training accuracy: 108.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3073000: 0.000910\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3074000: 0.000875\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3075000: 0.001165\n",
      "Training accuracy: -69.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3076000: 0.000764\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3077000: 0.001232\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3078000: 0.000850\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3079000: 0.000759\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3080000: 0.000787\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3081000: 0.000633\n",
      "Training accuracy: 128.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3082000: 0.000832\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3083000: 0.000738\n",
      "Training accuracy: -53.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3084000: 0.000702\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3085000: 0.001361\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3086000: 0.000562\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3087000: 0.000904\n",
      "Training accuracy: 117.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3088000: 0.001953\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3089000: 0.000799\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3090000: 0.001240\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3091000: 0.000964\n",
      "Training accuracy: 111.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3092000: 0.000708\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3093000: 0.001220\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3094000: 0.001194\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3095000: 0.000809\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3096000: 0.000651\n",
      "Training accuracy: 59.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3097000: 0.001063\n",
      "Training accuracy: 180.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3098000: 0.000827\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3099000: 0.001167\n",
      "Training accuracy: 15.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3100000: 0.000655\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3101000: 0.000486\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3102000: 0.000635\n",
      "Training accuracy: 130.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3103000: 0.000948\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3104000: 0.000889\n",
      "Training accuracy: -59.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3105000: 0.000846\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3106000: 0.001296\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3107000: 0.000854\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3108000: 0.000801\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3109000: 0.000793\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3110000: 0.000979\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3111000: 0.000856\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3112000: 0.000798\n",
      "Training accuracy: -184.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3113000: 0.000667\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3114000: 0.001393\n",
      "Training accuracy: 38.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3115000: 0.000872\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3116000: 0.001100\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3117000: 0.002102\n",
      "Training accuracy: 22.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3118000: 0.001161\n",
      "Training accuracy: 1.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3119000: 0.001536\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3120000: 0.000773\n",
      "Training accuracy: 108.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3121000: 0.000697\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3122000: 0.001369\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3123000: 0.000973\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3124000: 0.000811\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3125000: 0.000663\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3126000: 0.001153\n",
      "Training accuracy: 147.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3127000: 0.001234\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3128000: 0.001263\n",
      "Training accuracy: 11.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3129000: 0.000631\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3130000: 0.000548\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3131000: 0.000682\n",
      "Training accuracy: 121.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3132000: 0.001093\n",
      "Training accuracy: -99.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3133000: 0.000701\n",
      "Training accuracy: -46.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3134000: 0.000837\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3135000: 0.001357\n",
      "Training accuracy: 46.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3136000: 0.000734\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3137000: 0.000789\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3138000: 0.001190\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3139000: 0.000964\n",
      "Training accuracy: 124.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3140000: 0.000876\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3141000: 0.000710\n",
      "Training accuracy: -69.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3142000: 0.000730\n",
      "Training accuracy: 37.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3143000: 0.001118\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3144000: 0.000908\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3145000: 0.001173\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3146000: 0.001793\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3147000: 0.001028\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3148000: 0.001190\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3149000: 0.000959\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3150000: 0.000712\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3151000: 0.001372\n",
      "Training accuracy: -85.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3152000: 0.000660\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3153000: 0.000697\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3154000: 0.000493\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3155000: 0.000859\n",
      "Training accuracy: 155.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3156000: 0.001274\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3157000: 0.001177\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3158000: 0.000634\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3159000: 0.000573\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3160000: 0.001020\n",
      "Training accuracy: 111.8%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3161000: 0.001101\n",
      "Training accuracy: -90.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3162000: 0.000759\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3163000: 0.000751\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3164000: 0.001320\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3165000: 0.000795\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3166000: 0.001004\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3167000: 0.001044\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3168000: 0.000958\n",
      "Training accuracy: 145.7%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3169000: 0.001006\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3170000: 0.000731\n",
      "Training accuracy: -118.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3171000: 0.000679\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3172000: 0.000815\n",
      "Training accuracy: 22.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3173000: 0.000902\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3174000: 0.001129\n",
      "Training accuracy: 68.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3175000: 0.000912\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3176000: 0.001024\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3177000: 0.000903\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3178000: 0.000975\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3179000: 0.001021\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3180000: 0.001380\n",
      "Training accuracy: -102.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3181000: 0.000890\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3182000: 0.000693\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3183000: 0.000518\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3184000: 0.000872\n",
      "Training accuracy: 154.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3185000: 0.001410\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3186000: 0.001218\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3187000: 0.000582\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3188000: 0.000609\n",
      "Training accuracy: 104.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3189000: 0.000956\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3190000: 0.000818\n",
      "Training accuracy: -56.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3191000: 0.000755\n",
      "Training accuracy: 95.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3192000: 0.000701\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3193000: 0.001409\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3194000: 0.000624\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3195000: 0.001097\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3196000: 0.001066\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3197000: 0.001120\n",
      "Training accuracy: 116.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3198000: 0.001055\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3199000: 0.000731\n",
      "Training accuracy: -122.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3200000: 0.000950\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3201000: 0.000728\n",
      "Training accuracy: 30.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3202000: 0.001005\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3203000: 0.000967\n",
      "Training accuracy: 103.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3204000: 0.000903\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3205000: 0.001021\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3206000: 0.000850\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3207000: 0.000808\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3208000: 0.001062\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3209000: 0.000982\n",
      "Training accuracy: -78.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3210000: 0.001195\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3211000: 0.000689\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3212000: 0.000525\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3213000: 0.000870\n",
      "Training accuracy: 119.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3214000: 0.001399\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3215000: 0.000875\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3216000: 0.000572\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3217000: 0.000560\n",
      "Training accuracy: 93.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3218000: 0.001038\n",
      "Training accuracy: -21.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3219000: 0.000816\n",
      "Training accuracy: -58.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3220000: 0.001042\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3221000: 0.000682\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3222000: 0.000760\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3223000: 0.000919\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3224000: 0.001143\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3225000: 0.001383\n",
      "Training accuracy: 35.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3226000: 0.001104\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3227000: 0.000918\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3228000: 0.000823\n",
      "Training accuracy: -129.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3229000: 0.001242\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3230000: 0.000763\n",
      "Training accuracy: 34.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3231000: 0.001023\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3232000: 0.001246\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3233000: 0.000810\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3234000: 0.001142\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3235000: 0.000861\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3236000: 0.000772\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3237000: 0.000992\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3238000: 0.000807\n",
      "Training accuracy: -51.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3239000: 0.001288\n",
      "Training accuracy: -62.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3240000: 0.000664\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3241000: 0.000594\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3242000: 0.000823\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3243000: 0.001301\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3244000: 0.000898\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3245000: 0.000500\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3246000: 0.000685\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3247000: 0.001154\n",
      "Training accuracy: -90.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3248000: 0.000736\n",
      "Training accuracy: -86.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3249000: 0.001374\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3250000: 0.000538\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3251000: 0.000714\n",
      "Training accuracy: 128.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3252000: 0.001877\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3253000: 0.001090\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3254000: 0.001545\n",
      "Training accuracy: 34.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3255000: 0.000782\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3256000: 0.000880\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3257000: 0.001122\n",
      "Training accuracy: -2.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3258000: 0.001273\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3259000: 0.000720\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3260000: 0.000703\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3261000: 0.001004\n",
      "Training accuracy: 147.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3262000: 0.000708\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3263000: 0.000789\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3264000: 0.000551\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3265000: 0.000673\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3266000: 0.000945\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3267000: 0.000970\n",
      "Training accuracy: -0.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3268000: 0.001196\n",
      "Training accuracy: -57.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3269000: 0.000714\n",
      "Training accuracy: 108.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3270000: 0.000562\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3271000: 0.000833\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3272000: 0.000907\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3273000: 0.000778\n",
      "Training accuracy: 69.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3274000: 0.000621\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3275000: 0.000855\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3276000: 0.001083\n",
      "Training accuracy: -76.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3277000: 0.000653\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3278000: 0.001354\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3279000: 0.000506\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3280000: 0.000872\n",
      "Training accuracy: 118.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3281000: 0.001845\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3282000: 0.001095\n",
      "Training accuracy: 71.6%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3283000: 0.001217\n",
      "Training accuracy: 37.0%\n",
      "Validation accuracy: 88.9%\n",
      "Loss at step 3284000: 0.000946\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3285000: 0.000858\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3286000: 0.001195\n",
      "Training accuracy: -22.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3287000: 0.001222\n",
      "Training accuracy: 100.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3288000: 0.000836\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3289000: 0.000683\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3290000: 0.000994\n",
      "Training accuracy: 145.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3291000: 0.000778\n",
      "Training accuracy: 58.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3292000: 0.001047\n",
      "Training accuracy: 32.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3293000: 0.000550\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3294000: 0.000509\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3295000: 0.000908\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3296000: 0.000837\n",
      "Training accuracy: 40.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3297000: 0.001170\n",
      "Training accuracy: -65.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3298000: 0.000738\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3299000: 0.001230\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3300000: 0.000827\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3301000: 0.000750\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3302000: 0.000783\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3303000: 0.000612\n",
      "Training accuracy: 128.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3304000: 0.000834\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3305000: 0.000728\n",
      "Training accuracy: -49.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3306000: 0.000682\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3307000: 0.001327\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3308000: 0.000553\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3309000: 0.000883\n",
      "Training accuracy: 115.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3310000: 0.001998\n",
      "Training accuracy: 27.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3311000: 0.000788\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3312000: 0.001221\n",
      "Training accuracy: 26.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3313000: 0.000953\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3314000: 0.000691\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3315000: 0.001192\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3316000: 0.001180\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3317000: 0.000822\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3318000: 0.000642\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3319000: 0.001049\n",
      "Training accuracy: 173.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3320000: 0.000833\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3321000: 0.001160\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3322000: 0.000633\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3323000: 0.000475\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3324000: 0.000625\n",
      "Training accuracy: 128.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3325000: 0.000945\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3326000: 0.000905\n",
      "Training accuracy: -44.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3327000: 0.000834\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3328000: 0.001300\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3329000: 0.000866\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3330000: 0.000778\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3331000: 0.000787\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3332000: 0.000974\n",
      "Training accuracy: 124.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3333000: 0.000848\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3334000: 0.000779\n",
      "Training accuracy: -184.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3335000: 0.000668\n",
      "Training accuracy: 51.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3336000: 0.001401\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3337000: 0.000873\n",
      "Training accuracy: 58.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3338000: 0.001073\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3339000: 0.002088\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3340000: 0.001141\n",
      "Training accuracy: 4.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3341000: 0.001535\n",
      "Training accuracy: 23.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3342000: 0.000756\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3343000: 0.000698\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3344000: 0.001340\n",
      "Training accuracy: -55.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3345000: 0.000965\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3346000: 0.000793\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3347000: 0.000565\n",
      "Training accuracy: 71.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3348000: 0.001151\n",
      "Training accuracy: 147.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3349000: 0.001222\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3350000: 0.001274\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3351000: 0.000609\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3352000: 0.000542\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3353000: 0.000680\n",
      "Training accuracy: 120.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3354000: 0.001079\n",
      "Training accuracy: -99.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3355000: 0.000705\n",
      "Training accuracy: -48.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3356000: 0.000829\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3357000: 0.001353\n",
      "Training accuracy: 46.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3358000: 0.000726\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3359000: 0.000665\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3360000: 0.001183\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3361000: 0.000961\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3362000: 0.000863\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3363000: 0.000801\n",
      "Training accuracy: -79.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3364000: 0.000723\n",
      "Training accuracy: 37.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3365000: 0.001110\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3366000: 0.000849\n",
      "Training accuracy: 73.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3367000: 0.001174\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3368000: 0.001887\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3369000: 0.001008\n",
      "Training accuracy: 28.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3370000: 0.001183\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3371000: 0.000958\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3372000: 0.000691\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3373000: 0.001400\n",
      "Training accuracy: -109.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3374000: 0.000653\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3375000: 0.000674\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3376000: 0.000502\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3377000: 0.000855\n",
      "Training accuracy: 155.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3378000: 0.001319\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3379000: 0.001207\n",
      "Training accuracy: 8.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3380000: 0.000608\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3381000: 0.000564\n",
      "Training accuracy: 107.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3382000: 0.001002\n",
      "Training accuracy: 112.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3383000: 0.001111\n",
      "Training accuracy: -82.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3384000: 0.000760\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3385000: 0.000741\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3386000: 0.001328\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3387000: 0.000786\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3388000: 0.000995\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3389000: 0.001038\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3390000: 0.000953\n",
      "Training accuracy: 145.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3391000: 0.001004\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3392000: 0.000708\n",
      "Training accuracy: -122.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3393000: 0.000685\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3394000: 0.000801\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3395000: 0.000893\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3396000: 0.001120\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3397000: 0.000909\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3398000: 0.001013\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3399000: 0.000889\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3400000: 0.000965\n",
      "Training accuracy: 112.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3401000: 0.001029\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3402000: 0.001031\n",
      "Training accuracy: -100.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3403000: 0.000875\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3404000: 0.000679\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3405000: 0.000508\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3406000: 0.000856\n",
      "Training accuracy: 121.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3407000: 0.001472\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3408000: 0.001171\n",
      "Training accuracy: 48.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3409000: 0.000584\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3410000: 0.000605\n",
      "Training accuracy: 104.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3411000: 0.000943\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3412000: 0.000804\n",
      "Training accuracy: -56.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3413000: 0.000742\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3414000: 0.000693\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3415000: 0.001409\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3416000: 0.000612\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3417000: 0.001089\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3418000: 0.001057\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3419000: 0.001111\n",
      "Training accuracy: 117.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3420000: 0.000903\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3421000: 0.000704\n",
      "Training accuracy: -126.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3422000: 0.000950\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3423000: 0.000737\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3424000: 0.001057\n",
      "Training accuracy: 52.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3425000: 0.000944\n",
      "Training accuracy: 104.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3426000: 0.000957\n",
      "Training accuracy: 27.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3427000: 0.001141\n",
      "Training accuracy: 23.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3428000: 0.000847\n",
      "Training accuracy: 73.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3429000: 0.000814\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3430000: 0.001064\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3431000: 0.000968\n",
      "Training accuracy: -79.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3432000: 0.001187\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3433000: 0.000575\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3434000: 0.000510\n",
      "Training accuracy: 101.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3435000: 0.000872\n",
      "Training accuracy: 119.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3436000: 0.001396\n",
      "Training accuracy: 43.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3437000: 0.000887\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3438000: 0.000563\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3439000: 0.000575\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3440000: 0.001125\n",
      "Training accuracy: -27.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3441000: 0.000810\n",
      "Training accuracy: -58.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3442000: 0.001023\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3443000: 0.000674\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3444000: 0.000753\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3445000: 0.001246\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3446000: 0.001127\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3447000: 0.001381\n",
      "Training accuracy: 34.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3448000: 0.001113\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3449000: 0.000916\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3450000: 0.000812\n",
      "Training accuracy: -135.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3451000: 0.001236\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3452000: 0.000755\n",
      "Training accuracy: 34.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3453000: 0.001017\n",
      "Training accuracy: 69.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3454000: 0.001247\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3455000: 0.000763\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3456000: 0.001134\n",
      "Training accuracy: 25.8%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3457000: 0.000845\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3458000: 0.000754\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3459000: 0.000993\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3460000: 0.000872\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3461000: 0.001275\n",
      "Training accuracy: -62.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3462000: 0.000638\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3463000: 0.000589\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3464000: 0.000823\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3465000: 0.001299\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3466000: 0.000886\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3467000: 0.000543\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3468000: 0.000685\n",
      "Training accuracy: 91.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3469000: 0.001149\n",
      "Training accuracy: -90.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3470000: 0.000725\n",
      "Training accuracy: -86.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3471000: 0.001344\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3472000: 0.000527\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3473000: 0.000706\n",
      "Training accuracy: 125.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3474000: 0.001842\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3475000: 0.001094\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3476000: 0.001532\n",
      "Training accuracy: 34.7%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3477000: 0.000789\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3478000: 0.000871\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3479000: 0.001119\n",
      "Training accuracy: -5.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3480000: 0.001262\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3481000: 0.000693\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3482000: 0.000695\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3483000: 0.000997\n",
      "Training accuracy: 145.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3484000: 0.000695\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3485000: 0.000787\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3486000: 0.000527\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3487000: 0.000672\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3488000: 0.000929\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3489000: 0.000968\n",
      "Training accuracy: -0.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3490000: 0.001190\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3491000: 0.000709\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3492000: 0.000588\n",
      "Training accuracy: 95.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3493000: 0.000825\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3494000: 0.000902\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3495000: 0.000773\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3496000: 0.000614\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3497000: 0.000844\n",
      "Training accuracy: 61.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3498000: 0.001086\n",
      "Training accuracy: -76.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3499000: 0.000648\n",
      "Training accuracy: 58.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3500000: 0.001409\n",
      "Training accuracy: 49.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3501000: 0.000506\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3502000: 0.000866\n",
      "Training accuracy: 118.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3503000: 0.001992\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3504000: 0.001092\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3505000: 0.001198\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.8%\n",
      "Loss at step 3506000: 0.000967\n",
      "Training accuracy: 115.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3507000: 0.000863\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3508000: 0.001089\n",
      "Training accuracy: -16.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3509000: 0.001215\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3510000: 0.000833\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3511000: 0.000677\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3512000: 0.000981\n",
      "Training accuracy: 145.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3513000: 0.000704\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3514000: 0.001037\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3515000: 0.000636\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3516000: 0.000495\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3517000: 0.000910\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3518000: 0.000790\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3519000: 0.001167\n",
      "Training accuracy: -65.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3520000: 0.000736\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3521000: 0.001206\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3522000: 0.000803\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3523000: 0.000743\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3524000: 0.000724\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3525000: 0.000611\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3526000: 0.000847\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3527000: 0.000728\n",
      "Training accuracy: -50.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3528000: 0.000668\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3529000: 0.001315\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3530000: 0.000558\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3531000: 0.000859\n",
      "Training accuracy: 110.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3532000: 0.001992\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3533000: 0.000806\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3534000: 0.001220\n",
      "Training accuracy: 26.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3535000: 0.000958\n",
      "Training accuracy: 114.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3536000: 0.000687\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3537000: 0.001188\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3538000: 0.001174\n",
      "Training accuracy: 107.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3539000: 0.000815\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3540000: 0.000639\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3541000: 0.001047\n",
      "Training accuracy: 174.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3542000: 0.000831\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3543000: 0.001153\n",
      "Training accuracy: 16.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3544000: 0.000631\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3545000: 0.000464\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3546000: 0.000681\n",
      "Training accuracy: 122.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3547000: 0.000942\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3548000: 0.000911\n",
      "Training accuracy: -45.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3549000: 0.000832\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3550000: 0.001309\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3551000: 0.000854\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3552000: 0.000752\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3553000: 0.000796\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3554000: 0.000959\n",
      "Training accuracy: 124.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3555000: 0.000837\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3556000: 0.000776\n",
      "Training accuracy: -184.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3557000: 0.000653\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3558000: 0.001397\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3559000: 0.000876\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3560000: 0.001167\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3561000: 0.002106\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3562000: 0.001134\n",
      "Training accuracy: 4.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3563000: 0.001544\n",
      "Training accuracy: 22.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3564000: 0.000742\n",
      "Training accuracy: 109.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3565000: 0.000710\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3566000: 0.001335\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3567000: 0.000950\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3568000: 0.000770\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3569000: 0.000508\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3570000: 0.001154\n",
      "Training accuracy: 147.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3571000: 0.001150\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3572000: 0.001136\n",
      "Training accuracy: 23.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3573000: 0.000605\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3574000: 0.000526\n",
      "Training accuracy: 102.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3575000: 0.000677\n",
      "Training accuracy: 118.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3576000: 0.001072\n",
      "Training accuracy: -100.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3577000: 0.000693\n",
      "Training accuracy: -48.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3578000: 0.000815\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3579000: 0.001345\n",
      "Training accuracy: 44.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3580000: 0.000717\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3581000: 0.000650\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3582000: 0.001173\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3583000: 0.000965\n",
      "Training accuracy: 120.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3584000: 0.000842\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3585000: 0.000750\n",
      "Training accuracy: -148.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3586000: 0.000710\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3587000: 0.001104\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3588000: 0.000847\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3589000: 0.001167\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3590000: 0.001558\n",
      "Training accuracy: 25.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3591000: 0.001023\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3592000: 0.001169\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3593000: 0.000934\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3594000: 0.000675\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3595000: 0.001394\n",
      "Training accuracy: -109.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3596000: 0.000646\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3597000: 0.000690\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3598000: 0.000496\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3599000: 0.000846\n",
      "Training accuracy: 156.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3600000: 0.001304\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3601000: 0.001212\n",
      "Training accuracy: 7.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3602000: 0.000606\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3603000: 0.000562\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3604000: 0.000989\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3605000: 0.001044\n",
      "Training accuracy: -64.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3606000: 0.000770\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3607000: 0.000735\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3608000: 0.001338\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3609000: 0.000762\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3610000: 0.000986\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3611000: 0.001034\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3612000: 0.000891\n",
      "Training accuracy: 163.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3613000: 0.001092\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3614000: 0.000698\n",
      "Training accuracy: -124.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3615000: 0.000675\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3616000: 0.000813\n",
      "Training accuracy: 21.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3617000: 0.000891\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3618000: 0.001125\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3619000: 0.000898\n",
      "Training accuracy: 48.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3620000: 0.001002\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3621000: 0.000896\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3622000: 0.000947\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3623000: 0.001030\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3624000: 0.001019\n",
      "Training accuracy: -99.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3625000: 0.000866\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3626000: 0.000687\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3627000: 0.000501\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3628000: 0.000855\n",
      "Training accuracy: 121.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3629000: 0.001478\n",
      "Training accuracy: 31.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3630000: 0.001167\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3631000: 0.000570\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3632000: 0.000589\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3633000: 0.000968\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3634000: 0.000800\n",
      "Training accuracy: -57.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3635000: 0.000737\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3636000: 0.000685\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3637000: 0.001380\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3638000: 0.000923\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3639000: 0.001087\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3640000: 0.001039\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3641000: 0.001122\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3642000: 0.000916\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3643000: 0.000697\n",
      "Training accuracy: -128.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3644000: 0.000953\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3645000: 0.000774\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3646000: 0.001042\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3647000: 0.001278\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3648000: 0.000812\n",
      "Training accuracy: 31.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3649000: 0.001142\n",
      "Training accuracy: 22.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3650000: 0.000837\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3651000: 0.000785\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3652000: 0.001047\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3653000: 0.000960\n",
      "Training accuracy: -78.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3654000: 0.001200\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3655000: 0.000601\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3656000: 0.000505\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3657000: 0.000869\n",
      "Training accuracy: 118.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3658000: 0.001359\n",
      "Training accuracy: 29.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3659000: 0.000886\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3660000: 0.000496\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3661000: 0.000565\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3662000: 0.001128\n",
      "Training accuracy: -28.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3663000: 0.000820\n",
      "Training accuracy: -59.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3664000: 0.001021\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3665000: 0.000672\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3666000: 0.000777\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3667000: 0.001255\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3668000: 0.001136\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3669000: 0.001380\n",
      "Training accuracy: 34.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3670000: 0.001115\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3671000: 0.000900\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3672000: 0.000814\n",
      "Training accuracy: -137.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3673000: 0.001219\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3674000: 0.000745\n",
      "Training accuracy: 35.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3675000: 0.001006\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3676000: 0.001246\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3677000: 0.000775\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3678000: 0.001121\n",
      "Training accuracy: 20.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3679000: 0.000842\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3680000: 0.000736\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3681000: 0.000981\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3682000: 0.000875\n",
      "Training accuracy: -71.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3683000: 0.001281\n",
      "Training accuracy: -67.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3684000: 0.000642\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3685000: 0.000589\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3686000: 0.000814\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3687000: 0.001286\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3688000: 0.000883\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3689000: 0.000549\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3690000: 0.000680\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3691000: 0.001092\n",
      "Training accuracy: -78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3692000: 0.000724\n",
      "Training accuracy: -87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3693000: 0.001348\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3694000: 0.000526\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3695000: 0.000697\n",
      "Training accuracy: 125.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3696000: 0.001829\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3697000: 0.001096\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3698000: 0.001562\n",
      "Training accuracy: 14.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 3699000: 0.000828\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3700000: 0.000865\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3701000: 0.001119\n",
      "Training accuracy: -5.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3702000: 0.001263\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3703000: 0.000704\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3704000: 0.000690\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3705000: 0.000891\n",
      "Training accuracy: 151.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3706000: 0.000689\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3707000: 0.000796\n",
      "Training accuracy: 53.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3708000: 0.000522\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3709000: 0.000686\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3710000: 0.000904\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3711000: 0.000968\n",
      "Training accuracy: -0.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3712000: 0.001190\n",
      "Training accuracy: -58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3713000: 0.000713\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3714000: 0.000595\n",
      "Training accuracy: 92.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3715000: 0.000829\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3716000: 0.000770\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3717000: 0.000786\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3718000: 0.000603\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3719000: 0.000835\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3720000: 0.001078\n",
      "Training accuracy: -77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3721000: 0.000656\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3722000: 0.001423\n",
      "Training accuracy: 48.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3723000: 0.000543\n",
      "Training accuracy: 50.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3724000: 0.000864\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3725000: 0.001991\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3726000: 0.001100\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3727000: 0.001195\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3728000: 0.000951\n",
      "Training accuracy: 115.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3729000: 0.000873\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3730000: 0.001041\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3731000: 0.001205\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3732000: 0.000831\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3733000: 0.000671\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3734000: 0.001017\n",
      "Training accuracy: 185.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3735000: 0.000693\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3736000: 0.001014\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3737000: 0.000639\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3738000: 0.000489\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3739000: 0.000902\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3740000: 0.000891\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3741000: 0.001165\n",
      "Training accuracy: -65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3742000: 0.000721\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3743000: 0.001212\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3744000: 0.000808\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3745000: 0.000775\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3746000: 0.000710\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3747000: 0.000604\n",
      "Training accuracy: 128.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3748000: 0.000842\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3749000: 0.000726\n",
      "Training accuracy: -51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3750000: 0.000662\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3751000: 0.001302\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3752000: 0.000866\n",
      "Training accuracy: 52.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3753000: 0.001008\n",
      "Training accuracy: 105.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3754000: 0.001995\n",
      "Training accuracy: 27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3755000: 0.000854\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3756000: 0.001219\n",
      "Training accuracy: 26.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3757000: 0.001035\n",
      "Training accuracy: 106.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3758000: 0.000591\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3759000: 0.001184\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3760000: 0.001178\n",
      "Training accuracy: 106.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3761000: 0.000799\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3762000: 0.000638\n",
      "Training accuracy: 59.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3763000: 0.001035\n",
      "Training accuracy: 174.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3764000: 0.000911\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3765000: 0.001165\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3766000: 0.000616\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3767000: 0.000455\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3768000: 0.000672\n",
      "Training accuracy: 122.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3769000: 0.000950\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3770000: 0.000915\n",
      "Training accuracy: -45.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3771000: 0.000826\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3772000: 0.001329\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3773000: 0.000849\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3774000: 0.000745\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3775000: 0.001095\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3776000: 0.000960\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3777000: 0.000836\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3778000: 0.000757\n",
      "Training accuracy: -194.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3779000: 0.000648\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3780000: 0.001390\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3781000: 0.000872\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3782000: 0.001149\n",
      "Training accuracy: 94.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3783000: 0.001787\n",
      "Training accuracy: 26.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3784000: 0.001133\n",
      "Training accuracy: 4.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3785000: 0.001538\n",
      "Training accuracy: 22.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3786000: 0.000726\n",
      "Training accuracy: 109.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3787000: 0.000679\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3788000: 0.001331\n",
      "Training accuracy: -51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3789000: 0.000944\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3790000: 0.000672\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3791000: 0.000506\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3792000: 0.000827\n",
      "Training accuracy: 150.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3793000: 0.001147\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3794000: 0.001122\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3795000: 0.000607\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3796000: 0.000525\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3797000: 0.000986\n",
      "Training accuracy: 114.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3798000: 0.001081\n",
      "Training accuracy: -101.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3799000: 0.000792\n",
      "Training accuracy: -53.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3800000: 0.000789\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3801000: 0.001355\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3802000: 0.000710\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3803000: 0.000645\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3804000: 0.001174\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3805000: 0.000933\n",
      "Training accuracy: 145.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3806000: 0.001131\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3807000: 0.000736\n",
      "Training accuracy: -148.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3808000: 0.000707\n",
      "Training accuracy: 36.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3809000: 0.001103\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3810000: 0.000863\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3811000: 0.001140\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3812000: 0.001564\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3813000: 0.001011\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3814000: 0.001169\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3815000: 0.000939\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3816000: 0.000676\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3817000: 0.001392\n",
      "Training accuracy: -104.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3818000: 0.000650\n",
      "Training accuracy: 111.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3819000: 0.000696\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3820000: 0.000497\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3821000: 0.000834\n",
      "Training accuracy: 157.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3822000: 0.001295\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3823000: 0.001191\n",
      "Training accuracy: 8.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3824000: 0.000600\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3825000: 0.000570\n",
      "Training accuracy: 106.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3826000: 0.000995\n",
      "Training accuracy: 112.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3827000: 0.001042\n",
      "Training accuracy: -65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3828000: 0.000761\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3829000: 0.000724\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3830000: 0.001348\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3831000: 0.000761\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3832000: 0.001072\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3833000: 0.001037\n",
      "Training accuracy: 70.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3834000: 0.000881\n",
      "Training accuracy: 163.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3835000: 0.001090\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3836000: 0.000679\n",
      "Training accuracy: -127.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3837000: 0.000982\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3838000: 0.000795\n",
      "Training accuracy: 23.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3839000: 0.000890\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3840000: 0.001114\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3841000: 0.000906\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3842000: 0.001008\n",
      "Training accuracy: 32.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3843000: 0.000846\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3844000: 0.000899\n",
      "Training accuracy: 100.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3845000: 0.001040\n",
      "Training accuracy: 74.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3846000: 0.001018\n",
      "Training accuracy: -101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3847000: 0.000871\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3848000: 0.000674\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3849000: 0.000494\n",
      "Training accuracy: 99.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3850000: 0.000972\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3851000: 0.001473\n",
      "Training accuracy: 31.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3852000: 0.001148\n",
      "Training accuracy: 49.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3853000: 0.000552\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3854000: 0.000570\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3855000: 0.000981\n",
      "Training accuracy: 118.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3856000: 0.000794\n",
      "Training accuracy: -56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3857000: 0.000726\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3858000: 0.000676\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3859000: 0.001369\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3860000: 0.000909\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3861000: 0.001082\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3862000: 0.001060\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3863000: 0.001122\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3864000: 0.000915\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3865000: 0.000708\n",
      "Training accuracy: -129.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3866000: 0.000956\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3867000: 0.000761\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3868000: 0.001015\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3869000: 0.001275\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3870000: 0.000818\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3871000: 0.001126\n",
      "Training accuracy: 25.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3872000: 0.000825\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3873000: 0.000773\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3874000: 0.001026\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3875000: 0.000951\n",
      "Training accuracy: -78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3876000: 0.001201\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3877000: 0.000626\n",
      "Training accuracy: 99.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3878000: 0.000513\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3879000: 0.000833\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3880000: 0.001369\n",
      "Training accuracy: 27.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3881000: 0.000981\n",
      "Training accuracy: 42.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3882000: 0.000486\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3883000: 0.000680\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3884000: 0.001125\n",
      "Training accuracy: -29.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3885000: 0.000726\n",
      "Training accuracy: -55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3886000: 0.001029\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3887000: 0.000654\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3888000: 0.000769\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3889000: 0.001256\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3890000: 0.001085\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3891000: 0.001391\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3892000: 0.001115\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3893000: 0.000899\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3894000: 0.000828\n",
      "Training accuracy: -139.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3895000: 0.001220\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3896000: 0.000740\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3897000: 0.000701\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3898000: 0.001074\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3899000: 0.000766\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3900000: 0.001074\n",
      "Training accuracy: 95.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3901000: 0.000834\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3902000: 0.000659\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3903000: 0.000990\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3904000: 0.000877\n",
      "Training accuracy: -69.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3905000: 0.001273\n",
      "Training accuracy: -67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3906000: 0.000654\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3907000: 0.000592\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3908000: 0.000809\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3909000: 0.001193\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3910000: 0.000863\n",
      "Training accuracy: 62.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3911000: 0.000580\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3912000: 0.000690\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3913000: 0.001081\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3914000: 0.000726\n",
      "Training accuracy: -88.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3915000: 0.001350\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3916000: 0.000507\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3917000: 0.000685\n",
      "Training accuracy: 125.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3918000: 0.001823\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3919000: 0.001104\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3920000: 0.001255\n",
      "Training accuracy: 17.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3921000: 0.000840\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3922000: 0.000862\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3923000: 0.001110\n",
      "Training accuracy: -5.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3924000: 0.001267\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3925000: 0.000703\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3926000: 0.000691\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3927000: 0.000890\n",
      "Training accuracy: 152.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3928000: 0.000698\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3929000: 0.000786\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3930000: 0.000523\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3931000: 0.000691\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3932000: 0.000904\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3933000: 0.000961\n",
      "Training accuracy: -5.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3934000: 0.001178\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3935000: 0.000698\n",
      "Training accuracy: 108.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3936000: 0.000903\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3937000: 0.000824\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3938000: 0.000732\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3939000: 0.000812\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3940000: 0.000597\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3941000: 0.000829\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3942000: 0.000763\n",
      "Training accuracy: -73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3943000: 0.000646\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3944000: 0.001303\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3945000: 0.000527\n",
      "Training accuracy: 53.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3946000: 0.000866\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3947000: 0.001991\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3948000: 0.001120\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3949000: 0.001197\n",
      "Training accuracy: 31.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3950000: 0.000934\n",
      "Training accuracy: 112.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3951000: 0.000675\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3952000: 0.001153\n",
      "Training accuracy: 51.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3953000: 0.001189\n",
      "Training accuracy: 101.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3954000: 0.000831\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3955000: 0.000646\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3956000: 0.001020\n",
      "Training accuracy: 185.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3957000: 0.000681\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3958000: 0.001022\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3959000: 0.000641\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3960000: 0.000467\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3961000: 0.000919\n",
      "Training accuracy: 119.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3962000: 0.000894\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3963000: 0.001152\n",
      "Training accuracy: -63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3964000: 0.000711\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3965000: 0.001258\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3966000: 0.000808\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3967000: 0.000760\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3968000: 0.000717\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3969000: 0.000612\n",
      "Training accuracy: 128.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3970000: 0.000830\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3971000: 0.000715\n",
      "Training accuracy: -50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3972000: 0.000657\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3973000: 0.001296\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3974000: 0.000873\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3975000: 0.000994\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3976000: 0.002007\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3977000: 0.001057\n",
      "Training accuracy: 7.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3978000: 0.001213\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3979000: 0.001032\n",
      "Training accuracy: 106.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3980000: 0.000588\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3981000: 0.001175\n",
      "Training accuracy: 23.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3982000: 0.000966\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3983000: 0.000797\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3984000: 0.000640\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3985000: 0.001034\n",
      "Training accuracy: 174.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3986000: 0.001186\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3987000: 0.001267\n",
      "Training accuracy: 10.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3988000: 0.000632\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3989000: 0.000477\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3990000: 0.000665\n",
      "Training accuracy: 123.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3991000: 0.000944\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3992000: 0.000905\n",
      "Training accuracy: -45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3993000: 0.000817\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3994000: 0.001332\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3995000: 0.000726\n",
      "Training accuracy: 53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3996000: 0.000757\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3997000: 0.001113\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3998000: 0.000959\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 3999000: 0.000837\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4000000: 0.000730\n",
      "Training accuracy: -208.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4001000: 0.000654\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4002000: 0.001408\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4003000: 0.000880\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4004000: 0.001154\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4005000: 0.001790\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4006000: 0.001168\n",
      "Training accuracy: 19.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4007000: 0.001493\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4008000: 0.000724\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4009000: 0.000677\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4010000: 0.001320\n",
      "Training accuracy: -50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4011000: 0.000932\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4012000: 0.000661\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4013000: 0.000482\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4014000: 0.000827\n",
      "Training accuracy: 150.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4015000: 0.001148\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4016000: 0.001128\n",
      "Training accuracy: 21.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4017000: 0.000615\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4018000: 0.000525\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4019000: 0.001028\n",
      "Training accuracy: 95.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4020000: 0.001073\n",
      "Training accuracy: -101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4021000: 0.000800\n",
      "Training accuracy: -54.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4022000: 0.000765\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4023000: 0.001355\n",
      "Training accuracy: 43.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4024000: 0.000724\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4025000: 0.000643\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4026000: 0.001076\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4027000: 0.000938\n",
      "Training accuracy: 145.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4028000: 0.001019\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4029000: 0.000734\n",
      "Training accuracy: -148.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4030000: 0.000685\n",
      "Training accuracy: 37.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4031000: 0.001092\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4032000: 0.000872\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4033000: 0.001136\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4034000: 0.001550\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4035000: 0.001020\n",
      "Training accuracy: 35.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4036000: 0.001153\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4037000: 0.000938\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4038000: 0.000672\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4039000: 0.001377\n",
      "Training accuracy: -104.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4040000: 0.000929\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4041000: 0.000701\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4042000: 0.000494\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4043000: 0.000836\n",
      "Training accuracy: 157.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4044000: 0.001299\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4045000: 0.001183\n",
      "Training accuracy: 8.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4046000: 0.000599\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4047000: 0.000558\n",
      "Training accuracy: 106.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4048000: 0.000985\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4049000: 0.001036\n",
      "Training accuracy: -67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4050000: 0.000759\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4051000: 0.000702\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4052000: 0.001346\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4053000: 0.000772\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4054000: 0.001068\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4055000: 0.001037\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4056000: 0.001135\n",
      "Training accuracy: 120.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4057000: 0.001091\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4058000: 0.000697\n",
      "Training accuracy: -128.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4059000: 0.000981\n",
      "Training accuracy: 59.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4060000: 0.000779\n",
      "Training accuracy: 24.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4061000: 0.000889\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4062000: 0.001102\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4063000: 0.000906\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4064000: 0.000999\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4065000: 0.000847\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4066000: 0.000882\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4067000: 0.001051\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4068000: 0.001017\n",
      "Training accuracy: -101.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4069000: 0.000868\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4070000: 0.000689\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4071000: 0.000482\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4072000: 0.000977\n",
      "Training accuracy: 113.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4073000: 0.001473\n",
      "Training accuracy: 30.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4074000: 0.001162\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4075000: 0.000552\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4076000: 0.000553\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4077000: 0.000982\n",
      "Training accuracy: 119.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4078000: 0.000799\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4079000: 0.001054\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4080000: 0.000678\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4081000: 0.001063\n",
      "Training accuracy: 55.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4082000: 0.000893\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4083000: 0.000974\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4084000: 0.001040\n",
      "Training accuracy: 38.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4085000: 0.001123\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4086000: 0.000914\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4087000: 0.000807\n",
      "Training accuracy: -135.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4088000: 0.000956\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4089000: 0.000760\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4090000: 0.001008\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4091000: 0.001259\n",
      "Training accuracy: 101.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4092000: 0.000818\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4093000: 0.001117\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4094000: 0.000820\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4095000: 0.000773\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4096000: 0.000934\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4097000: 0.000850\n",
      "Training accuracy: -73.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4098000: 0.001255\n",
      "Training accuracy: -65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4099000: 0.000637\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4100000: 0.000567\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4101000: 0.000823\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4102000: 0.001360\n",
      "Training accuracy: 27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4103000: 0.000982\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4104000: 0.000479\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4105000: 0.000675\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4106000: 0.001106\n",
      "Training accuracy: -66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4107000: 0.000712\n",
      "Training accuracy: -54.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4108000: 0.001033\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4109000: 0.000651\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4110000: 0.000719\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4111000: 0.001582\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4112000: 0.001074\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4113000: 0.001385\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4114000: 0.001111\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4115000: 0.000899\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4116000: 0.000826\n",
      "Training accuracy: -139.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4117000: 0.001221\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4118000: 0.000756\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4119000: 0.000702\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4120000: 0.001085\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4121000: 0.000753\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4122000: 0.000803\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4123000: 0.000828\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4124000: 0.000659\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4125000: 0.000997\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4126000: 0.000880\n",
      "Training accuracy: -69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4127000: 0.001174\n",
      "Training accuracy: -59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4128000: 0.000654\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4129000: 0.000573\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4130000: 0.000966\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4131000: 0.000913\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4132000: 0.000757\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4133000: 0.000561\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4134000: 0.000669\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4135000: 0.001073\n",
      "Training accuracy: -77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4136000: 0.000780\n",
      "Training accuracy: -107.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4137000: 0.001344\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4138000: 0.000513\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4139000: 0.000767\n",
      "Training accuracy: 120.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4140000: 0.001821\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4141000: 0.001096\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4142000: 0.001256\n",
      "Training accuracy: 17.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4143000: 0.000849\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4144000: 0.000857\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4145000: 0.001115\n",
      "Training accuracy: -6.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4146000: 0.001266\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4147000: 0.000684\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4148000: 0.000674\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4149000: 0.000986\n",
      "Training accuracy: 146.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4150000: 0.000698\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4151000: 0.000744\n",
      "Training accuracy: 39.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4152000: 0.000523\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4153000: 0.000704\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4154000: 0.000905\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4155000: 0.000954\n",
      "Training accuracy: -4.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4156000: 0.001172\n",
      "Training accuracy: -60.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4157000: 0.000739\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4158000: 0.001230\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4159000: 0.000824\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4160000: 0.000735\n",
      "Training accuracy: 72.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4161000: 0.000798\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4162000: 0.000587\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4163000: 0.000849\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4164000: 0.000726\n",
      "Training accuracy: -53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4165000: 0.000667\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4166000: 0.001308\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4167000: 0.000530\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4168000: 0.000870\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4169000: 0.001991\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4170000: 0.001124\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4171000: 0.001196\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4172000: 0.000919\n",
      "Training accuracy: 113.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4173000: 0.000670\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4174000: 0.001158\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4175000: 0.001187\n",
      "Training accuracy: 102.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4176000: 0.000824\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4177000: 0.000634\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4178000: 0.001023\n",
      "Training accuracy: 184.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4179000: 0.000676\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4180000: 0.001023\n",
      "Training accuracy: 24.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4181000: 0.000639\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4182000: 0.000462\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4183000: 0.000924\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4184000: 0.000899\n",
      "Training accuracy: 52.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4185000: 0.000857\n",
      "Training accuracy: -59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4186000: 0.000710\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4187000: 0.001251\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4188000: 0.000814\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4189000: 0.000759\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4190000: 0.000865\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4191000: 0.000615\n",
      "Training accuracy: 128.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4192000: 0.000840\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4193000: 0.000716\n",
      "Training accuracy: -50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4194000: 0.000654\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4195000: 0.001295\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4196000: 0.000873\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4197000: 0.000984\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4198000: 0.002041\n",
      "Training accuracy: 3.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4199000: 0.001169\n",
      "Training accuracy: 1.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4200000: 0.001216\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4201000: 0.000752\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4202000: 0.000667\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4203000: 0.001211\n",
      "Training accuracy: -50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4204000: 0.000951\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4205000: 0.000808\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4206000: 0.000640\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4207000: 0.001198\n",
      "Training accuracy: 168.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4208000: 0.001197\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4209000: 0.001261\n",
      "Training accuracy: 11.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4210000: 0.000618\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4211000: 0.000477\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4212000: 0.000672\n",
      "Training accuracy: 123.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4213000: 0.000994\n",
      "Training accuracy: -88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4214000: 0.000899\n",
      "Training accuracy: -44.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4215000: 0.000804\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4216000: 0.001352\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4217000: 0.000715\n",
      "Training accuracy: 53.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4218000: 0.000749\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4219000: 0.001096\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4220000: 0.000952\n",
      "Training accuracy: 125.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4221000: 0.000835\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4222000: 0.000731\n",
      "Training accuracy: -210.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4223000: 0.000698\n",
      "Training accuracy: 36.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4224000: 0.001085\n",
      "Training accuracy: 39.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4225000: 0.000865\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4226000: 0.001154\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4227000: 0.001807\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4228000: 0.001162\n",
      "Training accuracy: 24.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4229000: 0.001493\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4230000: 0.000719\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4231000: 0.000676\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4232000: 0.001240\n",
      "Training accuracy: -45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4233000: 0.000944\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4234000: 0.000665\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4235000: 0.000489\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4236000: 0.000839\n",
      "Training accuracy: 154.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4237000: 0.001143\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4238000: 0.001146\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4239000: 0.000608\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4240000: 0.000537\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4241000: 0.001023\n",
      "Training accuracy: 93.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4242000: 0.001061\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4243000: 0.000752\n",
      "Training accuracy: 95.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4244000: 0.000746\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4245000: 0.001304\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4246000: 0.000719\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4247000: 0.000970\n",
      "Training accuracy: 51.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4248000: 0.001057\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4249000: 0.000944\n",
      "Training accuracy: 145.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4250000: 0.001022\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4251000: 0.000724\n",
      "Training accuracy: -147.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4252000: 0.000694\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4253000: 0.001083\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4254000: 0.000868\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4255000: 0.001130\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4256000: 0.001220\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4257000: 0.001017\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4258000: 0.001149\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4259000: 0.000947\n",
      "Training accuracy: 112.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4260000: 0.000686\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4261000: 0.001375\n",
      "Training accuracy: -103.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4262000: 0.000921\n",
      "Training accuracy: 109.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4263000: 0.000669\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4264000: 0.000484\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4265000: 0.000832\n",
      "Training accuracy: 157.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4266000: 0.001297\n",
      "Training accuracy: 59.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4267000: 0.001150\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4268000: 0.000600\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4269000: 0.000561\n",
      "Training accuracy: 106.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4270000: 0.000984\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4271000: 0.001049\n",
      "Training accuracy: -67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4272000: 0.000753\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4273000: 0.000705\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4274000: 0.001366\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4275000: 0.000621\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4276000: 0.001078\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4277000: 0.001025\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4278000: 0.001146\n",
      "Training accuracy: 119.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4279000: 0.001085\n",
      "Training accuracy: 60.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4280000: 0.000698\n",
      "Training accuracy: -129.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4281000: 0.000925\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4282000: 0.000783\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4283000: 0.000882\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4284000: 0.001051\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4285000: 0.000898\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4286000: 0.001000\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4287000: 0.000830\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4288000: 0.000870\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4289000: 0.001049\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4290000: 0.001017\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4291000: 0.001188\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4292000: 0.000688\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4293000: 0.000489\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4294000: 0.000879\n",
      "Training accuracy: 119.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4295000: 0.001466\n",
      "Training accuracy: 31.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4296000: 0.001166\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4297000: 0.000556\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4298000: 0.000535\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4299000: 0.000989\n",
      "Training accuracy: 119.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4300000: 0.000801\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4301000: 0.001047\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4302000: 0.000634\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4303000: 0.000735\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4304000: 0.000893\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4305000: 0.001116\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4306000: 0.001043\n",
      "Training accuracy: 37.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4307000: 0.001126\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4308000: 0.000896\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4309000: 0.000796\n",
      "Training accuracy: -135.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4310000: 0.001265\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4311000: 0.000739\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4312000: 0.001001\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4313000: 0.001246\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4314000: 0.000805\n",
      "Training accuracy: 31.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4315000: 0.001112\n",
      "Training accuracy: 27.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4316000: 0.000817\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4317000: 0.000777\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4318000: 0.000936\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4319000: 0.000843\n",
      "Training accuracy: -71.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4320000: 0.001262\n",
      "Training accuracy: -66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4321000: 0.000647\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4322000: 0.000566\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4323000: 0.000820\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4324000: 0.001367\n",
      "Training accuracy: 26.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4325000: 0.000986\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4326000: 0.000478\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4327000: 0.000670\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4328000: 0.001100\n",
      "Training accuracy: -66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4329000: 0.000703\n",
      "Training accuracy: -54.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4330000: 0.001042\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4331000: 0.000641\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4332000: 0.000735\n",
      "Training accuracy: 109.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4333000: 0.001674\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4334000: 0.001074\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4335000: 0.001226\n",
      "Training accuracy: 38.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4336000: 0.001114\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4337000: 0.000885\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4338000: 0.000824\n",
      "Training accuracy: -139.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4339000: 0.001229\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4340000: 0.000757\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4341000: 0.000692\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4342000: 0.001084\n",
      "Training accuracy: 105.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4343000: 0.000711\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4344000: 0.000690\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4345000: 0.000839\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4346000: 0.000644\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4347000: 0.000943\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4348000: 0.000829\n",
      "Training accuracy: 4.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4349000: 0.001181\n",
      "Training accuracy: -57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4350000: 0.000660\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4351000: 0.000571\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4352000: 0.000805\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4353000: 0.000903\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4354000: 0.000756\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4355000: 0.000582\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4356000: 0.000665\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4357000: 0.001055\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4358000: 0.000719\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4359000: 0.001349\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4360000: 0.000509\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4361000: 0.000749\n",
      "Training accuracy: 120.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4362000: 0.001832\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4363000: 0.001090\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4364000: 0.001276\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4365000: 0.000856\n",
      "Training accuracy: 91.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4366000: 0.000873\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4367000: 0.001135\n",
      "Training accuracy: -7.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4368000: 0.001219\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4369000: 0.000688\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4370000: 0.000673\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4371000: 0.000994\n",
      "Training accuracy: 146.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4372000: 0.000741\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4373000: 0.000737\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4374000: 0.000521\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4375000: 0.000709\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4376000: 0.000904\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4377000: 0.000930\n",
      "Training accuracy: -3.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4378000: 0.001157\n",
      "Training accuracy: -59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4379000: 0.000743\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4380000: 0.001220\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4381000: 0.000810\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4382000: 0.000732\n",
      "Training accuracy: 72.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4383000: 0.000766\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4384000: 0.000585\n",
      "Training accuracy: 91.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4385000: 0.000833\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4386000: 0.000724\n",
      "Training accuracy: -53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4387000: 0.000673\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4388000: 0.001327\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4389000: 0.000545\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4390000: 0.000872\n",
      "Training accuracy: 116.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4391000: 0.001997\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4392000: 0.000799\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4393000: 0.001203\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4394000: 0.000927\n",
      "Training accuracy: 112.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4395000: 0.000667\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4396000: 0.001168\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4397000: 0.001176\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4398000: 0.000832\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4399000: 0.000640\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4400000: 0.001033\n",
      "Training accuracy: 183.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4401000: 0.000694\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4402000: 0.001132\n",
      "Training accuracy: 15.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4403000: 0.000641\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4404000: 0.000462\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4405000: 0.000928\n",
      "Training accuracy: 127.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4406000: 0.000904\n",
      "Training accuracy: 53.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4407000: 0.000858\n",
      "Training accuracy: -59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4408000: 0.000722\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4409000: 0.001257\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4410000: 0.000816\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4411000: 0.000747\n",
      "Training accuracy: 54.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4412000: 0.000854\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4413000: 0.000611\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4414000: 0.000837\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4415000: 0.000756\n",
      "Training accuracy: -187.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4416000: 0.000637\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4417000: 0.001294\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4418000: 0.000884\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4419000: 0.001076\n",
      "Training accuracy: 95.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4420000: 0.002065\n",
      "Training accuracy: 16.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4421000: 0.001147\n",
      "Training accuracy: 2.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4422000: 0.001214\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4423000: 0.000743\n",
      "Training accuracy: 109.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4424000: 0.000667\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4425000: 0.001339\n",
      "Training accuracy: -56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4426000: 0.000960\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4427000: 0.000801\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4428000: 0.000648\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4429000: 0.001165\n",
      "Training accuracy: 146.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4430000: 0.001211\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4431000: 0.001259\n",
      "Training accuracy: 11.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4432000: 0.000613\n",
      "Training accuracy: 83.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4433000: 0.000485\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4434000: 0.000675\n",
      "Training accuracy: 122.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4435000: 0.000990\n",
      "Training accuracy: -88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4436000: 0.000585\n",
      "Training accuracy: -41.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4437000: 0.000805\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4438000: 0.001357\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4439000: 0.000699\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4440000: 0.000749\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4441000: 0.001097\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4442000: 0.000952\n",
      "Training accuracy: 123.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4443000: 0.000862\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4444000: 0.000728\n",
      "Training accuracy: -210.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4445000: 0.000693\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4446000: 0.001091\n",
      "Training accuracy: 39.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4447000: 0.000872\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4448000: 0.001174\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4449000: 0.001805\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4450000: 0.001018\n",
      "Training accuracy: 28.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4451000: 0.001488\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4452000: 0.000834\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4453000: 0.000678\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4454000: 0.001240\n",
      "Training accuracy: -45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4455000: 0.000611\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4456000: 0.000679\n",
      "Training accuracy: 82.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4457000: 0.000491\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4458000: 0.000842\n",
      "Training accuracy: 156.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4459000: 0.001248\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4460000: 0.001159\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4461000: 0.000600\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4462000: 0.000547\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4463000: 0.001030\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4464000: 0.001060\n",
      "Training accuracy: -102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4465000: 0.000744\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4466000: 0.000736\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4467000: 0.001312\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4468000: 0.000736\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4469000: 0.000981\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4470000: 0.001036\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4471000: 0.000949\n",
      "Training accuracy: 145.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4472000: 0.001023\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4473000: 0.000736\n",
      "Training accuracy: -148.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4474000: 0.000703\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4475000: 0.001077\n",
      "Training accuracy: 42.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4476000: 0.000867\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4477000: 0.001113\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4478000: 0.001132\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4479000: 0.001013\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4480000: 0.001158\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4481000: 0.000947\n",
      "Training accuracy: 112.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4482000: 0.000690\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4483000: 0.001366\n",
      "Training accuracy: -103.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4484000: 0.000918\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4485000: 0.000678\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4486000: 0.000502\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4487000: 0.000835\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4488000: 0.001297\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4489000: 0.001151\n",
      "Training accuracy: 30.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4490000: 0.000579\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4491000: 0.000568\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4492000: 0.000943\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4493000: 0.001055\n",
      "Training accuracy: -68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4494000: 0.000747\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4495000: 0.000700\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4496000: 0.001384\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4497000: 0.000619\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4498000: 0.001069\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4499000: 0.001044\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4500000: 0.001124\n",
      "Training accuracy: 116.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4501000: 0.001098\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4502000: 0.000702\n",
      "Training accuracy: -129.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4503000: 0.000940\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4504000: 0.000775\n",
      "Training accuracy: 22.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4505000: 0.000895\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4506000: 0.001058\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4507000: 0.000897\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4508000: 0.001001\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4509000: 0.000807\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4510000: 0.000869\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4511000: 0.001039\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4512000: 0.001003\n",
      "Training accuracy: -96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4513000: 0.001174\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4514000: 0.000682\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4515000: 0.000503\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4516000: 0.000865\n",
      "Training accuracy: 120.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4517000: 0.001404\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4518000: 0.001158\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4519000: 0.000548\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4520000: 0.000529\n",
      "Training accuracy: 94.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4521000: 0.000979\n",
      "Training accuracy: 115.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4522000: 0.000799\n",
      "Training accuracy: -57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4523000: 0.001044\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4524000: 0.000621\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4525000: 0.000738\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4526000: 0.000893\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4527000: 0.001124\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4528000: 0.001039\n",
      "Training accuracy: 38.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4529000: 0.001127\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4530000: 0.000897\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4531000: 0.000799\n",
      "Training accuracy: -136.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4532000: 0.001253\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4533000: 0.000713\n",
      "Training accuracy: 37.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4534000: 0.000988\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4535000: 0.001230\n",
      "Training accuracy: 102.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4536000: 0.000797\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4537000: 0.001103\n",
      "Training accuracy: 27.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4538000: 0.000822\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4539000: 0.000766\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4540000: 0.000978\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4541000: 0.000835\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4542000: 0.001263\n",
      "Training accuracy: -66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4543000: 0.000637\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4544000: 0.000563\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4545000: 0.000821\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4546000: 0.001359\n",
      "Training accuracy: 26.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4547000: 0.000892\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4548000: 0.000485\n",
      "Training accuracy: 62.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4549000: 0.000671\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4550000: 0.001078\n",
      "Training accuracy: -72.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4551000: 0.000742\n",
      "Training accuracy: -79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4552000: 0.001051\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4553000: 0.000635\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4554000: 0.000745\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4555000: 0.001783\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4556000: 0.001092\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4557000: 0.001516\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4558000: 0.001115\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4559000: 0.000878\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4560000: 0.000777\n",
      "Training accuracy: -2.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4561000: 0.001261\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4562000: 0.000762\n",
      "Training accuracy: 50.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4563000: 0.000690\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4564000: 0.000979\n",
      "Training accuracy: 110.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4565000: 0.000674\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4566000: 0.000710\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4567000: 0.000847\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4568000: 0.000645\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4569000: 0.000949\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4570000: 0.000864\n",
      "Training accuracy: 5.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4571000: 0.001174\n",
      "Training accuracy: -56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4572000: 0.000677\n",
      "Training accuracy: 104.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4573000: 0.000558\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4574000: 0.000805\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4575000: 0.000897\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4576000: 0.000749\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4577000: 0.000586\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4578000: 0.000705\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4579000: 0.001061\n",
      "Training accuracy: -77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4580000: 0.000730\n",
      "Training accuracy: 45.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4581000: 0.001349\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4582000: 0.000503\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4583000: 0.000855\n",
      "Training accuracy: 115.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4584000: 0.001845\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4585000: 0.001088\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4586000: 0.001277\n",
      "Training accuracy: 15.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4587000: 0.000886\n",
      "Training accuracy: 115.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4588000: 0.000849\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4589000: 0.001125\n",
      "Training accuracy: -6.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4590000: 0.001218\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4591000: 0.000694\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4592000: 0.000669\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4593000: 0.000973\n",
      "Training accuracy: 147.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4594000: 0.000750\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4595000: 0.001031\n",
      "Training accuracy: 32.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4596000: 0.000523\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4597000: 0.000593\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4598000: 0.000894\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4599000: 0.000932\n",
      "Training accuracy: -3.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4600000: 0.001174\n",
      "Training accuracy: -60.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4601000: 0.000739\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4602000: 0.001234\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4603000: 0.000803\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4604000: 0.000738\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4605000: 0.000758\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4606000: 0.000594\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4607000: 0.000823\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4608000: 0.000720\n",
      "Training accuracy: -53.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4609000: 0.000690\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4610000: 0.001326\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4611000: 0.000545\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4612000: 0.000882\n",
      "Training accuracy: 116.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4613000: 0.001984\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4614000: 0.000782\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4615000: 0.001210\n",
      "Training accuracy: 30.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4616000: 0.000931\n",
      "Training accuracy: 112.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4617000: 0.000673\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4618000: 0.001202\n",
      "Training accuracy: 26.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4619000: 0.001167\n",
      "Training accuracy: 103.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4620000: 0.000834\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4621000: 0.000647\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4622000: 0.001049\n",
      "Training accuracy: 183.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4623000: 0.000678\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4624000: 0.001131\n",
      "Training accuracy: 16.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4625000: 0.000636\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4626000: 0.000464\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4627000: 0.000930\n",
      "Training accuracy: 127.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4628000: 0.000915\n",
      "Training accuracy: 53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4629000: 0.000864\n",
      "Training accuracy: -60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4630000: 0.000815\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4631000: 0.001305\n",
      "Training accuracy: 43.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4632000: 0.000822\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4633000: 0.000748\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4634000: 0.000853\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4635000: 0.000626\n",
      "Training accuracy: 127.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4636000: 0.000846\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4637000: 0.000769\n",
      "Training accuracy: -188.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4638000: 0.000640\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4639000: 0.001366\n",
      "Training accuracy: 37.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4640000: 0.000869\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4641000: 0.001069\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4642000: 0.002078\n",
      "Training accuracy: 16.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4643000: 0.001149\n",
      "Training accuracy: 2.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4644000: 0.001201\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4645000: 0.000743\n",
      "Training accuracy: 108.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4646000: 0.000669\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4647000: 0.001343\n",
      "Training accuracy: -56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4648000: 0.000950\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4649000: 0.000807\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4650000: 0.000634\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4651000: 0.001157\n",
      "Training accuracy: 148.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4652000: 0.001205\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4653000: 0.001250\n",
      "Training accuracy: 11.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4654000: 0.000613\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4655000: 0.000487\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4656000: 0.000673\n",
      "Training accuracy: 122.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4657000: 0.000979\n",
      "Training accuracy: -92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4658000: 0.000693\n",
      "Training accuracy: -47.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4659000: 0.000815\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4660000: 0.001356\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4661000: 0.000707\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4662000: 0.000748\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4663000: 0.001109\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4664000: 0.000961\n",
      "Training accuracy: 123.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4665000: 0.000868\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4666000: 0.000726\n",
      "Training accuracy: -210.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4667000: 0.000702\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4668000: 0.001102\n",
      "Training accuracy: 42.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4669000: 0.000870\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4670000: 0.001170\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4671000: 0.001807\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4672000: 0.001004\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4673000: 0.001493\n",
      "Training accuracy: 44.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4674000: 0.000932\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4675000: 0.000680\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4676000: 0.001309\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4677000: 0.000637\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4678000: 0.000674\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4679000: 0.000494\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4680000: 0.000845\n",
      "Training accuracy: 157.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4681000: 0.001250\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4682000: 0.001158\n",
      "Training accuracy: 31.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4683000: 0.000602\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4684000: 0.000543\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4685000: 0.000995\n",
      "Training accuracy: 114.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4686000: 0.001086\n",
      "Training accuracy: -93.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4687000: 0.000740\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4688000: 0.000737\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4689000: 0.001319\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4690000: 0.000769\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4691000: 0.000988\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4692000: 0.001019\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4693000: 0.000938\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4694000: 0.001000\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4695000: 0.000760\n",
      "Training accuracy: -143.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4696000: 0.000656\n",
      "Training accuracy: 58.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4697000: 0.001115\n",
      "Training accuracy: 18.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4698000: 0.000874\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4699000: 0.001093\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4700000: 0.001005\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4701000: 0.000996\n",
      "Training accuracy: 34.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4702000: 0.000875\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4703000: 0.000955\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4704000: 0.001015\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4705000: 0.001365\n",
      "Training accuracy: -103.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4706000: 0.000883\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4707000: 0.000672\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4708000: 0.000492\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4709000: 0.000835\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4710000: 0.001389\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4711000: 0.001216\n",
      "Training accuracy: 24.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4712000: 0.000568\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4713000: 0.000574\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4714000: 0.000944\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4715000: 0.000888\n",
      "Training accuracy: -62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4716000: 0.000742\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4717000: 0.000699\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4718000: 0.001391\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4719000: 0.000602\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4720000: 0.001063\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4721000: 0.001042\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4722000: 0.001119\n",
      "Training accuracy: 116.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4723000: 0.001064\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4724000: 0.000695\n",
      "Training accuracy: -129.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4725000: 0.000932\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4726000: 0.000773\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4727000: 0.000898\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4728000: 0.000950\n",
      "Training accuracy: 104.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4729000: 0.000883\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4730000: 0.000999\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4731000: 0.000811\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4732000: 0.000831\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4733000: 0.001053\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4734000: 0.001004\n",
      "Training accuracy: -96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4735000: 0.001176\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4736000: 0.000671\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4737000: 0.000505\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4738000: 0.000865\n",
      "Training accuracy: 119.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4739000: 0.001391\n",
      "Training accuracy: 41.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4740000: 0.000849\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4741000: 0.000549\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4742000: 0.000527\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4743000: 0.001027\n",
      "Training accuracy: -20.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4744000: 0.000796\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4745000: 0.001031\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4746000: 0.000608\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4747000: 0.000734\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4748000: 0.000906\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4749000: 0.001118\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4750000: 0.001359\n",
      "Training accuracy: 34.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4751000: 0.001129\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4752000: 0.000895\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4753000: 0.000800\n",
      "Training accuracy: -136.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4754000: 0.001242\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4755000: 0.000727\n",
      "Training accuracy: 36.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4756000: 0.001005\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4757000: 0.001213\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4758000: 0.000788\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4759000: 0.001105\n",
      "Training accuracy: 27.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4760000: 0.000815\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4761000: 0.000759\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4762000: 0.000969\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4763000: 0.000788\n",
      "Training accuracy: -47.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4764000: 0.001273\n",
      "Training accuracy: -66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4765000: 0.000644\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4766000: 0.000568\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4767000: 0.000803\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4768000: 0.001364\n",
      "Training accuracy: 24.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4769000: 0.000891\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4770000: 0.000482\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4771000: 0.000666\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4772000: 0.001133\n",
      "Training accuracy: -89.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4773000: 0.000736\n",
      "Training accuracy: -79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4774000: 0.001374\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4775000: 0.000529\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4776000: 0.000682\n",
      "Training accuracy: 126.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4777000: 0.001889\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4778000: 0.001086\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4779000: 0.001525\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4780000: 0.001108\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4781000: 0.000875\n",
      "Training accuracy: 89.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4782000: 0.000771\n",
      "Training accuracy: -2.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4783000: 0.001265\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4784000: 0.000692\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4785000: 0.000691\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4786000: 0.000964\n",
      "Training accuracy: 110.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4787000: 0.000656\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4788000: 0.000709\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4789000: 0.000847\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4790000: 0.000647\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4791000: 0.000938\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4792000: 0.000949\n",
      "Training accuracy: -0.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4793000: 0.001171\n",
      "Training accuracy: -56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4794000: 0.000685\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4795000: 0.000553\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4796000: 0.000817\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4797000: 0.000890\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4798000: 0.000752\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4799000: 0.000595\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4800000: 0.000712\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4801000: 0.001059\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4802000: 0.000729\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4803000: 0.001340\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4804000: 0.000496\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4805000: 0.000846\n",
      "Training accuracy: 117.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4806000: 0.001841\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4807000: 0.001085\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4808000: 0.001265\n",
      "Training accuracy: 15.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4809000: 0.000928\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4810000: 0.000847\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4811000: 0.001182\n",
      "Training accuracy: -25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4812000: 0.001210\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4813000: 0.000691\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4814000: 0.000671\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4815000: 0.000983\n",
      "Training accuracy: 146.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4816000: 0.000749\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4817000: 0.001029\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4818000: 0.000525\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4819000: 0.000492\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4820000: 0.000891\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4821000: 0.000864\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4822000: 0.001154\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4823000: 0.000737\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4824000: 0.001231\n",
      "Training accuracy: 91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4825000: 0.000817\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4826000: 0.000723\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4827000: 0.000758\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4828000: 0.000604\n",
      "Training accuracy: 128.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4829000: 0.000825\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4830000: 0.000717\n",
      "Training accuracy: -53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4831000: 0.000667\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4832000: 0.001334\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4833000: 0.000543\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4834000: 0.000871\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4835000: 0.001952\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4836000: 0.000773\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4837000: 0.001215\n",
      "Training accuracy: 30.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4838000: 0.000941\n",
      "Training accuracy: 111.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4839000: 0.000675\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4840000: 0.001189\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4841000: 0.001173\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4842000: 0.000784\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4843000: 0.000632\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4844000: 0.001053\n",
      "Training accuracy: 183.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4845000: 0.000794\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4846000: 0.001137\n",
      "Training accuracy: 16.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4847000: 0.000631\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4848000: 0.000449\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4849000: 0.000603\n",
      "Training accuracy: 132.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4850000: 0.000935\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4851000: 0.000870\n",
      "Training accuracy: -60.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4852000: 0.000823\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4853000: 0.001298\n",
      "Training accuracy: 44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4854000: 0.000830\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4855000: 0.000761\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4856000: 0.000768\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4857000: 0.000964\n",
      "Training accuracy: 124.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4858000: 0.000846\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4859000: 0.000776\n",
      "Training accuracy: -188.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4860000: 0.000640\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4861000: 0.001372\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4862000: 0.000860\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4863000: 0.001065\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4864000: 0.002102\n",
      "Training accuracy: 22.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4865000: 0.001138\n",
      "Training accuracy: 3.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4866000: 0.001520\n",
      "Training accuracy: 22.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4867000: 0.000740\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4868000: 0.000669\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4869000: 0.001344\n",
      "Training accuracy: -56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4870000: 0.000942\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4871000: 0.000792\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4872000: 0.000648\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4873000: 0.001151\n",
      "Training accuracy: 148.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4874000: 0.001208\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4875000: 0.001247\n",
      "Training accuracy: 11.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4876000: 0.000610\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4877000: 0.000512\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4878000: 0.000656\n",
      "Training accuracy: 123.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4879000: 0.001083\n",
      "Training accuracy: -101.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4880000: 0.000692\n",
      "Training accuracy: -47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4881000: 0.000817\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4882000: 0.001354\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4883000: 0.000710\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4884000: 0.000757\n",
      "Training accuracy: 49.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4885000: 0.001175\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4886000: 0.000953\n",
      "Training accuracy: 123.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4887000: 0.000865\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4888000: 0.000688\n",
      "Training accuracy: -74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4889000: 0.000709\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4890000: 0.001093\n",
      "Training accuracy: 43.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4891000: 0.000892\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4892000: 0.001150\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4893000: 0.001792\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4894000: 0.001001\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4895000: 0.001170\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4896000: 0.000926\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4897000: 0.000684\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4898000: 0.001350\n",
      "Training accuracy: -86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4899000: 0.000630\n",
      "Training accuracy: 112.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4900000: 0.000675\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4901000: 0.000474\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4902000: 0.000852\n",
      "Training accuracy: 157.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4903000: 0.001258\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4904000: 0.001159\n",
      "Training accuracy: 31.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4905000: 0.000617\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4906000: 0.000540\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4907000: 0.001002\n",
      "Training accuracy: 113.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4908000: 0.001090\n",
      "Training accuracy: -92.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4909000: 0.000747\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4910000: 0.000730\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4911000: 0.001318\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4912000: 0.000773\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4913000: 0.000981\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4914000: 0.001030\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4915000: 0.000941\n",
      "Training accuracy: 146.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4916000: 0.001000\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4917000: 0.000709\n",
      "Training accuracy: -126.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4918000: 0.000658\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4919000: 0.000778\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4920000: 0.000879\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4921000: 0.001111\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4922000: 0.000890\n",
      "Training accuracy: 49.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4923000: 0.001007\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4924000: 0.000881\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4925000: 0.000945\n",
      "Training accuracy: 112.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4926000: 0.001009\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4927000: 0.001362\n",
      "Training accuracy: -103.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4928000: 0.000869\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4929000: 0.000672\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4930000: 0.000494\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4931000: 0.000855\n",
      "Training accuracy: 156.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4932000: 0.001397\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4933000: 0.001213\n",
      "Training accuracy: 24.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4934000: 0.000562\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4935000: 0.000580\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4936000: 0.000943\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4937000: 0.000794\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4938000: 0.000739\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4939000: 0.000683\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4940000: 0.001407\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4941000: 0.000600\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4942000: 0.001079\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4943000: 0.001048\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4944000: 0.001114\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4945000: 0.001047\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4946000: 0.000709\n",
      "Training accuracy: -130.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4947000: 0.000934\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4948000: 0.000691\n",
      "Training accuracy: 30.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4949000: 0.000991\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4950000: 0.000944\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4951000: 0.000877\n",
      "Training accuracy: 45.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4952000: 0.000999\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4953000: 0.000825\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4954000: 0.000786\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4955000: 0.001046\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4956000: 0.000957\n",
      "Training accuracy: -78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4957000: 0.001179\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4958000: 0.000669\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4959000: 0.000508\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4960000: 0.000855\n",
      "Training accuracy: 120.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4961000: 0.001384\n",
      "Training accuracy: 41.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4962000: 0.000860\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4963000: 0.000549\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4964000: 0.000535\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4965000: 0.001027\n",
      "Training accuracy: -20.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4966000: 0.000792\n",
      "Training accuracy: -59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4967000: 0.001029\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4968000: 0.000664\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4969000: 0.000737\n",
      "Training accuracy: 50.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4970000: 0.000896\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4971000: 0.001128\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4972000: 0.001368\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4973000: 0.001098\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4974000: 0.000899\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4975000: 0.000799\n",
      "Training accuracy: -136.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4976000: 0.001232\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4977000: 0.000732\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4978000: 0.001010\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4979000: 0.001228\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4980000: 0.000778\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4981000: 0.001120\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4982000: 0.000831\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4983000: 0.000748\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4984000: 0.000978\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4985000: 0.000790\n",
      "Training accuracy: -51.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4986000: 0.001275\n",
      "Training accuracy: -63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4987000: 0.000643\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4988000: 0.000578\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4989000: 0.000807\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4990000: 0.001280\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4991000: 0.000883\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4992000: 0.000477\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4993000: 0.000667\n",
      "Training accuracy: 92.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4994000: 0.001144\n",
      "Training accuracy: -91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4995000: 0.000714\n",
      "Training accuracy: -87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4996000: 0.001368\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4997000: 0.000521\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4998000: 0.000690\n",
      "Training accuracy: 128.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 4999000: 0.001872\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5000000: 0.001079\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5001000: 0.001532\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5002000: 0.000769\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5003000: 0.000863\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5004000: 0.001102\n",
      "Training accuracy: -5.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5005000: 0.001266\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5006000: 0.000694\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5007000: 0.000685\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5008000: 0.000995\n",
      "Training accuracy: 147.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5009000: 0.000673\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5010000: 0.000755\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5011000: 0.000525\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5012000: 0.000650\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5013000: 0.000929\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5014000: 0.000961\n",
      "Training accuracy: -0.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5015000: 0.001187\n",
      "Training accuracy: -57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5016000: 0.000693\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5017000: 0.000545\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5018000: 0.000810\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5019000: 0.000886\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5020000: 0.000758\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5021000: 0.000598\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5022000: 0.000846\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5023000: 0.001076\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5024000: 0.000631\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5025000: 0.001340\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5026000: 0.000489\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5027000: 0.000855\n",
      "Training accuracy: 117.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5028000: 0.001841\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5029000: 0.001081\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5030000: 0.001199\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5031000: 0.000928\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5032000: 0.000844\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5033000: 0.001184\n",
      "Training accuracy: -25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5034000: 0.001210\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5035000: 0.000812\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5036000: 0.000665\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5037000: 0.000983\n",
      "Training accuracy: 145.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5038000: 0.000750\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5039000: 0.001023\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5040000: 0.000532\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5041000: 0.000485\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5042000: 0.000895\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5043000: 0.000828\n",
      "Training accuracy: 39.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5044000: 0.001168\n",
      "Training accuracy: -65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5045000: 0.000723\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5046000: 0.001232\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5047000: 0.000809\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5048000: 0.000724\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5049000: 0.000766\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5050000: 0.000590\n",
      "Training accuracy: 128.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5051000: 0.000829\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5052000: 0.000717\n",
      "Training accuracy: -51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5053000: 0.000660\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5054000: 0.001312\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5055000: 0.000543\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5056000: 0.000861\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5057000: 0.001999\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5058000: 0.000768\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5059000: 0.001212\n",
      "Training accuracy: 25.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5060000: 0.000943\n",
      "Training accuracy: 111.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5061000: 0.000671\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5062000: 0.001177\n",
      "Training accuracy: 25.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5063000: 0.001167\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5064000: 0.000804\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5065000: 0.000631\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5066000: 0.001045\n",
      "Training accuracy: 174.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5067000: 0.000810\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5068000: 0.001135\n",
      "Training accuracy: 16.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5069000: 0.000621\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5070000: 0.000449\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5071000: 0.000604\n",
      "Training accuracy: 130.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5072000: 0.000939\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5073000: 0.000895\n",
      "Training accuracy: -43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5074000: 0.000818\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5075000: 0.001305\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5076000: 0.000856\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5077000: 0.000753\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5078000: 0.000774\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5079000: 0.000963\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5080000: 0.000842\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5081000: 0.000768\n",
      "Training accuracy: -187.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5082000: 0.000649\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5083000: 0.001389\n",
      "Training accuracy: 37.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5084000: 0.000865\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5085000: 0.001049\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5086000: 0.002088\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5087000: 0.001124\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5088000: 0.001527\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5089000: 0.000734\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5090000: 0.000681\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5091000: 0.001326\n",
      "Training accuracy: -55.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5092000: 0.000946\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5093000: 0.000778\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5094000: 0.000553\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5095000: 0.001153\n",
      "Training accuracy: 148.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5096000: 0.001206\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5097000: 0.001259\n",
      "Training accuracy: 16.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5098000: 0.000598\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5099000: 0.000518\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5100000: 0.000665\n",
      "Training accuracy: 122.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5101000: 0.001075\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5102000: 0.000696\n",
      "Training accuracy: -47.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5103000: 0.000816\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5104000: 0.001354\n",
      "Training accuracy: 45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5105000: 0.000711\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5106000: 0.000643\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5107000: 0.001176\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5108000: 0.000953\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5109000: 0.000856\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5110000: 0.000793\n",
      "Training accuracy: -82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5111000: 0.000709\n",
      "Training accuracy: 35.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5112000: 0.001094\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5113000: 0.000837\n",
      "Training accuracy: 73.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5114000: 0.001156\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5115000: 0.001886\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5116000: 0.000992\n",
      "Training accuracy: 29.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5117000: 0.001173\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5118000: 0.000938\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5119000: 0.000675\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5120000: 0.001388\n",
      "Training accuracy: -109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5121000: 0.000633\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5122000: 0.000660\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5123000: 0.000490\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5124000: 0.000852\n",
      "Training accuracy: 157.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5125000: 0.001311\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5126000: 0.001195\n",
      "Training accuracy: 7.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5127000: 0.000599\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5128000: 0.000543\n",
      "Training accuracy: 107.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5129000: 0.000995\n",
      "Training accuracy: 113.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5130000: 0.001103\n",
      "Training accuracy: -84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5131000: 0.000751\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5132000: 0.000728\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5133000: 0.001331\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5134000: 0.000770\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5135000: 0.000982\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5136000: 0.001030\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5137000: 0.000941\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5138000: 0.001002\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5139000: 0.000696\n",
      "Training accuracy: -126.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5140000: 0.000670\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5141000: 0.000776\n",
      "Training accuracy: 22.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5142000: 0.000879\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5143000: 0.001110\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5144000: 0.000895\n",
      "Training accuracy: 49.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5145000: 0.001003\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5146000: 0.000874\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5147000: 0.000948\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5148000: 0.001024\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5149000: 0.001017\n",
      "Training accuracy: -99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5150000: 0.000865\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5151000: 0.000667\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5152000: 0.000495\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5153000: 0.000843\n",
      "Training accuracy: 122.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5154000: 0.001465\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5155000: 0.001166\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5156000: 0.000572\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5157000: 0.000586\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5158000: 0.000937\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5159000: 0.000789\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5160000: 0.000730\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5161000: 0.000682\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5162000: 0.001410\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5163000: 0.000597\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5164000: 0.001080\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5165000: 0.001044\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5166000: 0.001111\n",
      "Training accuracy: 116.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5167000: 0.000898\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5168000: 0.000693\n",
      "Training accuracy: -129.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5169000: 0.000939\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5170000: 0.000714\n",
      "Training accuracy: 37.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5171000: 0.001050\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5172000: 0.000928\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5173000: 0.000941\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5174000: 0.001124\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5175000: 0.000829\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5176000: 0.000800\n",
      "Training accuracy: 102.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5177000: 0.001056\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5178000: 0.000952\n",
      "Training accuracy: -78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5179000: 0.001182\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5180000: 0.000562\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5181000: 0.000499\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5182000: 0.000865\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5183000: 0.001387\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5184000: 0.000876\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5185000: 0.000547\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5186000: 0.000560\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5187000: 0.001122\n",
      "Training accuracy: -27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5188000: 0.000793\n",
      "Training accuracy: -59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5189000: 0.001013\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5190000: 0.000663\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5191000: 0.000737\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5192000: 0.001234\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5193000: 0.001119\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5194000: 0.001372\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5195000: 0.001112\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5196000: 0.000908\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5197000: 0.000800\n",
      "Training accuracy: -138.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5198000: 0.001232\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5199000: 0.000736\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5200000: 0.001005\n",
      "Training accuracy: 68.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5201000: 0.001235\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5202000: 0.000742\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5203000: 0.001120\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5204000: 0.000824\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5205000: 0.000738\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5206000: 0.000984\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5207000: 0.000862\n",
      "Training accuracy: -70.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5208000: 0.001271\n",
      "Training accuracy: -62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5209000: 0.000625\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5210000: 0.000579\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5211000: 0.000813\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5212000: 0.001286\n",
      "Training accuracy: 44.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5213000: 0.000877\n",
      "Training accuracy: 61.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5214000: 0.000528\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5215000: 0.000676\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5216000: 0.001146\n",
      "Training accuracy: -91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5217000: 0.000711\n",
      "Training accuracy: -87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5218000: 0.001340\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5219000: 0.000517\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5220000: 0.000688\n",
      "Training accuracy: 125.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5221000: 0.001836\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5222000: 0.001086\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5223000: 0.001525\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5224000: 0.000781\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5225000: 0.000863\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5226000: 0.001108\n",
      "Training accuracy: -6.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5227000: 0.001257\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5228000: 0.000679\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5229000: 0.000683\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5230000: 0.000994\n",
      "Training accuracy: 146.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5231000: 0.000674\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5232000: 0.000764\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5233000: 0.000515\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5234000: 0.000659\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5235000: 0.000920\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5236000: 0.000961\n",
      "Training accuracy: -0.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5237000: 0.001186\n",
      "Training accuracy: -57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5238000: 0.000696\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5239000: 0.000579\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5240000: 0.000812\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5241000: 0.000888\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5242000: 0.000762\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5243000: 0.000598\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5244000: 0.000840\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5245000: 0.001083\n",
      "Training accuracy: -77.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5246000: 0.000633\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5247000: 0.001401\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5248000: 0.000496\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5249000: 0.000856\n",
      "Training accuracy: 117.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5250000: 0.001991\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5251000: 0.001081\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5252000: 0.001189\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5253000: 0.000956\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5254000: 0.000857\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5255000: 0.001080\n",
      "Training accuracy: -17.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5256000: 0.001207\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5257000: 0.000818\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5258000: 0.000665\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5259000: 0.000978\n",
      "Training accuracy: 146.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5260000: 0.000685\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5261000: 0.001020\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5262000: 0.000627\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5263000: 0.000481\n",
      "Training accuracy: 108.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5264000: 0.000902\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5265000: 0.000786\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5266000: 0.001165\n",
      "Training accuracy: -64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5267000: 0.000726\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5268000: 0.001210\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5269000: 0.000793\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5270000: 0.000726\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5271000: 0.000713\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5272000: 0.000597\n",
      "Training accuracy: 128.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5273000: 0.000844\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5274000: 0.000724\n",
      "Training accuracy: -51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5275000: 0.000652\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5276000: 0.001305\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5277000: 0.000553\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5278000: 0.000844\n",
      "Training accuracy: 110.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5279000: 0.001993\n",
      "Training accuracy: 27.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5280000: 0.000790\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5281000: 0.001214\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5282000: 0.000950\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5283000: 0.000676\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5284000: 0.001178\n",
      "Training accuracy: 25.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5285000: 0.001164\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5286000: 0.000804\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5287000: 0.000632\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5288000: 0.001044\n",
      "Training accuracy: 174.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5289000: 0.000814\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5290000: 0.001136\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5291000: 0.000623\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5292000: 0.000447\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5293000: 0.000666\n",
      "Training accuracy: 123.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5294000: 0.000938\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5295000: 0.000903\n",
      "Training accuracy: -44.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5296000: 0.000822\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5297000: 0.001314\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5298000: 0.000849\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5299000: 0.000735\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5300000: 0.000787\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5301000: 0.000952\n",
      "Training accuracy: 124.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5302000: 0.000834\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5303000: 0.000770\n",
      "Training accuracy: -186.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5304000: 0.000641\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5305000: 0.001387\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5306000: 0.000872\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5307000: 0.001154\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5308000: 0.002105\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5309000: 0.001123\n",
      "Training accuracy: 5.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5310000: 0.001539\n",
      "Training accuracy: 21.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5311000: 0.000726\n",
      "Training accuracy: 109.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5312000: 0.000699\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5313000: 0.001325\n",
      "Training accuracy: -55.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5314000: 0.000938\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5315000: 0.000761\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5316000: 0.000498\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5317000: 0.001157\n",
      "Training accuracy: 148.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5318000: 0.001137\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5319000: 0.001129\n",
      "Training accuracy: 22.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5320000: 0.000600\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5321000: 0.000511\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5322000: 0.000666\n",
      "Training accuracy: 119.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5323000: 0.001070\n",
      "Training accuracy: -100.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5324000: 0.000688\n",
      "Training accuracy: -47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5325000: 0.000807\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5326000: 0.001348\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5327000: 0.000708\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5328000: 0.000637\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5329000: 0.001171\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5330000: 0.000959\n",
      "Training accuracy: 120.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5331000: 0.000837\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5332000: 0.000744\n",
      "Training accuracy: -150.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5333000: 0.000704\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5334000: 0.001092\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5335000: 0.000839\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5336000: 0.001155\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5337000: 0.001555\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5338000: 0.001013\n",
      "Training accuracy: 36.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5339000: 0.001163\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5340000: 0.000922\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5341000: 0.000667\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5342000: 0.001386\n",
      "Training accuracy: -109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5343000: 0.000633\n",
      "Training accuracy: 112.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5344000: 0.000680\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5345000: 0.000488\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5346000: 0.000845\n",
      "Training accuracy: 157.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5347000: 0.001298\n",
      "Training accuracy: 59.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5348000: 0.001204\n",
      "Training accuracy: 7.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5349000: 0.000600\n",
      "Training accuracy: 73.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5350000: 0.000549\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5351000: 0.000985\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5352000: 0.001039\n",
      "Training accuracy: -65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5353000: 0.000763\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5354000: 0.000726\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5355000: 0.001342\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5356000: 0.000752\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5357000: 0.000979\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5358000: 0.001030\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5359000: 0.000883\n",
      "Training accuracy: 163.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5360000: 0.001093\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5361000: 0.000688\n",
      "Training accuracy: -125.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5362000: 0.000666\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5363000: 0.000793\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5364000: 0.000883\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5365000: 0.001118\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5366000: 0.000890\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5367000: 0.000996\n",
      "Training accuracy: 33.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5368000: 0.000886\n",
      "Training accuracy: 49.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5369000: 0.000937\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5370000: 0.001027\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5371000: 0.001010\n",
      "Training accuracy: -99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5372000: 0.000861\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5373000: 0.000679\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5374000: 0.000491\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5375000: 0.000848\n",
      "Training accuracy: 121.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5376000: 0.001473\n",
      "Training accuracy: 30.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5377000: 0.001164\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5378000: 0.000560\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5379000: 0.000576\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5380000: 0.000965\n",
      "Training accuracy: 103.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5381000: 0.000791\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5382000: 0.000729\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5383000: 0.000679\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5384000: 0.001381\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5385000: 0.000913\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5386000: 0.001082\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5387000: 0.001030\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5388000: 0.001123\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5389000: 0.000913\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5390000: 0.000689\n",
      "Training accuracy: -129.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5391000: 0.000946\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5392000: 0.000759\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5393000: 0.001037\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5394000: 0.001267\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5395000: 0.000800\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5396000: 0.001131\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5397000: 0.000826\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5398000: 0.000776\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5399000: 0.001043\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5400000: 0.000950\n",
      "Training accuracy: -77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5401000: 0.001197\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5402000: 0.000594\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5403000: 0.000498\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5404000: 0.000863\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5405000: 0.001352\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5406000: 0.000879\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5407000: 0.000484\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5408000: 0.000556\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5409000: 0.001127\n",
      "Training accuracy: -27.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5410000: 0.000807\n",
      "Training accuracy: -59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5411000: 0.001013\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5412000: 0.000667\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5413000: 0.000768\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5414000: 0.001244\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5415000: 0.001130\n",
      "Training accuracy: 43.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5416000: 0.001373\n",
      "Training accuracy: 33.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5417000: 0.001114\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5418000: 0.000896\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5419000: 0.000804\n",
      "Training accuracy: -138.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5420000: 0.001217\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5421000: 0.000732\n",
      "Training accuracy: 35.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5422000: 0.000998\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5423000: 0.001240\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5424000: 0.000760\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5425000: 0.001113\n",
      "Training accuracy: 20.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5426000: 0.000829\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5427000: 0.000727\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5428000: 0.000977\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5429000: 0.000870\n",
      "Training accuracy: -71.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5430000: 0.001281\n",
      "Training accuracy: -67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5431000: 0.000635\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5432000: 0.000583\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5433000: 0.000805\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5434000: 0.001276\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5435000: 0.000877\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5436000: 0.000538\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5437000: 0.000674\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5438000: 0.001092\n",
      "Training accuracy: -78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5439000: 0.000713\n",
      "Training accuracy: -87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5440000: 0.001344\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5441000: 0.000517\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5442000: 0.000686\n",
      "Training accuracy: 125.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5443000: 0.001825\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5444000: 0.001090\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5445000: 0.001559\n",
      "Training accuracy: 13.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5446000: 0.000822\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5447000: 0.000861\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5448000: 0.001110\n",
      "Training accuracy: -5.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5449000: 0.001258\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5450000: 0.000692\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5451000: 0.000681\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5452000: 0.000887\n",
      "Training accuracy: 151.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5453000: 0.000675\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5454000: 0.000782\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5455000: 0.000516\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5456000: 0.000678\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5457000: 0.000900\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5458000: 0.000965\n",
      "Training accuracy: -0.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5459000: 0.001189\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5460000: 0.000703\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5461000: 0.000590\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5462000: 0.000819\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5463000: 0.000760\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5464000: 0.000779\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5465000: 0.000592\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5466000: 0.000834\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5467000: 0.001078\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5468000: 0.000645\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5469000: 0.001416\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5470000: 0.000537\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5471000: 0.000858\n",
      "Training accuracy: 117.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5472000: 0.001989\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5473000: 0.001094\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5474000: 0.001187\n",
      "Training accuracy: 31.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5475000: 0.000947\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5476000: 0.000871\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5477000: 0.001036\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5478000: 0.001198\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5479000: 0.000821\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5480000: 0.000663\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5481000: 0.001015\n",
      "Training accuracy: 186.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5482000: 0.000680\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5483000: 0.001004\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5484000: 0.000635\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5485000: 0.000480\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5486000: 0.000898\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5487000: 0.000890\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5488000: 0.001166\n",
      "Training accuracy: -64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5489000: 0.000716\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5490000: 0.001215\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5491000: 0.000800\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5492000: 0.000763\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5493000: 0.000703\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5494000: 0.000597\n",
      "Training accuracy: 128.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5495000: 0.000840\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5496000: 0.000722\n",
      "Training accuracy: -51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5497000: 0.000651\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5498000: 0.001295\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5499000: 0.000867\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5500000: 0.000996\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5501000: 0.001995\n",
      "Training accuracy: 26.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5502000: 0.000843\n",
      "Training accuracy: 5.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5503000: 0.001212\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5504000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5505000: 0.000584\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5506000: 0.001178\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5507000: 0.001172\n",
      "Training accuracy: 107.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5508000: 0.000793\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5509000: 0.000632\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5510000: 0.001034\n",
      "Training accuracy: 175.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5511000: 0.000901\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5512000: 0.001155\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5513000: 0.000613\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5514000: 0.000445\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5515000: 0.000665\n",
      "Training accuracy: 123.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5516000: 0.000948\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5517000: 0.000911\n",
      "Training accuracy: -44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5518000: 0.000820\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5519000: 0.001331\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5520000: 0.000844\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5521000: 0.000735\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5522000: 0.001091\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5523000: 0.000956\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5524000: 0.000835\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5525000: 0.000752\n",
      "Training accuracy: -195.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5526000: 0.000639\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5527000: 0.001382\n",
      "Training accuracy: 37.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5528000: 0.000871\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5529000: 0.001142\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5530000: 0.001786\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5531000: 0.001127\n",
      "Training accuracy: 5.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5532000: 0.001532\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5533000: 0.000718\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5534000: 0.000673\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5535000: 0.001324\n",
      "Training accuracy: -50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5536000: 0.000936\n",
      "Training accuracy: 92.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5537000: 0.000666\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5538000: 0.000500\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5539000: 0.000827\n",
      "Training accuracy: 151.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5540000: 0.001138\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5541000: 0.001119\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5542000: 0.000604\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5543000: 0.000516\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5544000: 0.000984\n",
      "Training accuracy: 115.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5545000: 0.001078\n",
      "Training accuracy: -101.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5546000: 0.000791\n",
      "Training accuracy: -53.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5547000: 0.000783\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5548000: 0.001357\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5549000: 0.000704\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5550000: 0.000636\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5551000: 0.001172\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5552000: 0.000928\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5553000: 0.001131\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5554000: 0.000730\n",
      "Training accuracy: -149.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5555000: 0.000702\n",
      "Training accuracy: 35.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5556000: 0.001093\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5557000: 0.000858\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5558000: 0.001132\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5559000: 0.001564\n",
      "Training accuracy: 24.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5560000: 0.001006\n",
      "Training accuracy: 36.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5561000: 0.001163\n",
      "Training accuracy: 47.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5562000: 0.000934\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5563000: 0.000672\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5564000: 0.001384\n",
      "Training accuracy: -104.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5565000: 0.000641\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5566000: 0.000690\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5567000: 0.000491\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5568000: 0.000832\n",
      "Training accuracy: 158.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5569000: 0.001290\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5570000: 0.001189\n",
      "Training accuracy: 8.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5571000: 0.000595\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5572000: 0.000562\n",
      "Training accuracy: 106.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5573000: 0.000995\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5574000: 0.001036\n",
      "Training accuracy: -65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5575000: 0.000756\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5576000: 0.000718\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5577000: 0.001350\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5578000: 0.000755\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5579000: 0.001069\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5580000: 0.001034\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5581000: 0.000876\n",
      "Training accuracy: 163.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5582000: 0.001092\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5583000: 0.000673\n",
      "Training accuracy: -128.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5584000: 0.000980\n",
      "Training accuracy: 54.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5585000: 0.000782\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5586000: 0.000885\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5587000: 0.001109\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5588000: 0.000901\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5589000: 0.001003\n",
      "Training accuracy: 32.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5590000: 0.000837\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5591000: 0.000896\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5592000: 0.001039\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5593000: 0.001010\n",
      "Training accuracy: -100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5594000: 0.000868\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5595000: 0.000670\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5596000: 0.000488\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5597000: 0.000966\n",
      "Training accuracy: 115.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5598000: 0.001469\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5599000: 0.001147\n",
      "Training accuracy: 49.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5600000: 0.000545\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5601000: 0.000562\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5602000: 0.000982\n",
      "Training accuracy: 118.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5603000: 0.000784\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5604000: 0.000722\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5605000: 0.000672\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5606000: 0.001370\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5607000: 0.000902\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5608000: 0.001078\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5609000: 0.001056\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5610000: 0.001123\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5611000: 0.000912\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5612000: 0.000702\n",
      "Training accuracy: -129.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5613000: 0.000952\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5614000: 0.000751\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5615000: 0.001010\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5616000: 0.001268\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5617000: 0.000810\n",
      "Training accuracy: 31.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5618000: 0.001121\n",
      "Training accuracy: 25.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5619000: 0.000815\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5620000: 0.000768\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5621000: 0.001025\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5622000: 0.000943\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5623000: 0.001201\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5624000: 0.000623\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5625000: 0.000507\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5626000: 0.000826\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5627000: 0.001363\n",
      "Training accuracy: 26.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5628000: 0.000977\n",
      "Training accuracy: 42.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5629000: 0.000478\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5630000: 0.000674\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5631000: 0.001127\n",
      "Training accuracy: -28.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5632000: 0.000715\n",
      "Training accuracy: -55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5633000: 0.001026\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5634000: 0.000650\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5635000: 0.000763\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5636000: 0.001249\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5637000: 0.001082\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5638000: 0.001387\n",
      "Training accuracy: 32.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5639000: 0.001115\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5640000: 0.000896\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5641000: 0.000821\n",
      "Training accuracy: -139.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5642000: 0.001221\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5643000: 0.000731\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5644000: 0.000693\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5645000: 0.001073\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5646000: 0.000757\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5647000: 0.001070\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5648000: 0.000825\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5649000: 0.000653\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5650000: 0.000987\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5651000: 0.000872\n",
      "Training accuracy: -69.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5652000: 0.001273\n",
      "Training accuracy: -66.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5653000: 0.000649\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5654000: 0.000587\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5655000: 0.000801\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5656000: 0.001185\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5657000: 0.000860\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5658000: 0.000573\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5659000: 0.000687\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5660000: 0.001081\n",
      "Training accuracy: -78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5661000: 0.000718\n",
      "Training accuracy: -88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5662000: 0.001349\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5663000: 0.000503\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5664000: 0.000678\n",
      "Training accuracy: 125.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5665000: 0.001820\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5666000: 0.001101\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5667000: 0.001250\n",
      "Training accuracy: 16.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5668000: 0.000837\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5669000: 0.000859\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5670000: 0.001105\n",
      "Training accuracy: -5.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5671000: 0.001267\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5672000: 0.000697\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5673000: 0.000684\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5674000: 0.000888\n",
      "Training accuracy: 152.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5675000: 0.000689\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5676000: 0.000778\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5677000: 0.000518\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5678000: 0.000686\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5679000: 0.000901\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5680000: 0.000959\n",
      "Training accuracy: -5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5681000: 0.001179\n",
      "Training accuracy: -56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5682000: 0.000693\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5683000: 0.000900\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5684000: 0.000816\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5685000: 0.000725\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5686000: 0.000807\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5687000: 0.000590\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5688000: 0.000828\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5689000: 0.000759\n",
      "Training accuracy: -73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5690000: 0.000638\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5691000: 0.001298\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5692000: 0.000524\n",
      "Training accuracy: 53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5693000: 0.000861\n",
      "Training accuracy: 117.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5694000: 0.001990\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5695000: 0.001116\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5696000: 0.001192\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5697000: 0.000933\n",
      "Training accuracy: 112.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5698000: 0.000671\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5699000: 0.001149\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5700000: 0.001185\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5701000: 0.000825\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5702000: 0.000641\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5703000: 0.001020\n",
      "Training accuracy: 186.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5704000: 0.000672\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5705000: 0.001017\n",
      "Training accuracy: 24.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5706000: 0.000639\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5707000: 0.000461\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5708000: 0.000916\n",
      "Training accuracy: 120.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5709000: 0.000894\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5710000: 0.001152\n",
      "Training accuracy: -63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5711000: 0.000708\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5712000: 0.001260\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5713000: 0.000803\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5714000: 0.000752\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5715000: 0.000711\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5716000: 0.000607\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5717000: 0.000829\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5718000: 0.000712\n",
      "Training accuracy: -50.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5719000: 0.000650\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5720000: 0.001292\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5721000: 0.000873\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5722000: 0.000986\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5723000: 0.002007\n",
      "Training accuracy: 26.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5724000: 0.001051\n",
      "Training accuracy: 7.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5725000: 0.001208\n",
      "Training accuracy: 25.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5726000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5727000: 0.000583\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5728000: 0.001169\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5729000: 0.000960\n",
      "Training accuracy: 102.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5730000: 0.000793\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5731000: 0.000636\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5732000: 0.001035\n",
      "Training accuracy: 174.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5733000: 0.001180\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5734000: 0.001264\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5735000: 0.000629\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5736000: 0.000469\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5737000: 0.000660\n",
      "Training accuracy: 123.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5738000: 0.000942\n",
      "Training accuracy: 63.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5739000: 0.000903\n",
      "Training accuracy: -44.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5740000: 0.000813\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5741000: 0.001335\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5742000: 0.000721\n",
      "Training accuracy: 53.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5743000: 0.000750\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5744000: 0.001110\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5745000: 0.000957\n",
      "Training accuracy: 124.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5746000: 0.000837\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5747000: 0.000726\n",
      "Training accuracy: -209.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5748000: 0.000648\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5749000: 0.001403\n",
      "Training accuracy: 36.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5750000: 0.000879\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5751000: 0.001149\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5752000: 0.001789\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5753000: 0.001164\n",
      "Training accuracy: 19.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5754000: 0.001487\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5755000: 0.000719\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5756000: 0.000673\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5757000: 0.001315\n",
      "Training accuracy: -50.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5758000: 0.000927\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5759000: 0.000657\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5760000: 0.000478\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5761000: 0.000827\n",
      "Training accuracy: 151.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5762000: 0.001143\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5763000: 0.001127\n",
      "Training accuracy: 21.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5764000: 0.000614\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5765000: 0.000519\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5766000: 0.001028\n",
      "Training accuracy: 95.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5767000: 0.001071\n",
      "Training accuracy: -101.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5768000: 0.000800\n",
      "Training accuracy: -53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5769000: 0.000760\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5770000: 0.001358\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5771000: 0.000720\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5772000: 0.000637\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5773000: 0.001075\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5774000: 0.000934\n",
      "Training accuracy: 146.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5775000: 0.001018\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5776000: 0.000730\n",
      "Training accuracy: -149.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5777000: 0.000683\n",
      "Training accuracy: 36.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5778000: 0.001085\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5779000: 0.000868\n",
      "Training accuracy: 80.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5780000: 0.001130\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5781000: 0.001550\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5782000: 0.001017\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5783000: 0.001149\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5784000: 0.000935\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5785000: 0.000669\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5786000: 0.001371\n",
      "Training accuracy: -103.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5787000: 0.000927\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5788000: 0.000697\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5789000: 0.000490\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5790000: 0.000834\n",
      "Training accuracy: 158.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5791000: 0.001295\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5792000: 0.001184\n",
      "Training accuracy: 8.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5793000: 0.000595\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5794000: 0.000553\n",
      "Training accuracy: 106.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5795000: 0.000986\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5796000: 0.001031\n",
      "Training accuracy: -67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5797000: 0.000757\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5798000: 0.000699\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5799000: 0.001348\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5800000: 0.000767\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5801000: 0.001065\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5802000: 0.001036\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5803000: 0.001134\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5804000: 0.001091\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5805000: 0.000693\n",
      "Training accuracy: -129.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5806000: 0.000980\n",
      "Training accuracy: 59.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5807000: 0.000770\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5808000: 0.000886\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5809000: 0.001099\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5810000: 0.000902\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5811000: 0.000996\n",
      "Training accuracy: 32.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5812000: 0.000840\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5813000: 0.000880\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5814000: 0.001051\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5815000: 0.001011\n",
      "Training accuracy: -100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5816000: 0.000867\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5817000: 0.000686\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5818000: 0.000478\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5819000: 0.000973\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5820000: 0.001470\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5821000: 0.001163\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5822000: 0.000548\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5823000: 0.000548\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5824000: 0.000983\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5825000: 0.000792\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5826000: 0.001053\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5827000: 0.000676\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5828000: 0.001063\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5829000: 0.000888\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5830000: 0.000970\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5831000: 0.001037\n",
      "Training accuracy: 37.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5832000: 0.001124\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5833000: 0.000911\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5834000: 0.000804\n",
      "Training accuracy: -136.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5835000: 0.000954\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5836000: 0.000753\n",
      "Training accuracy: 49.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5837000: 0.001005\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5838000: 0.001255\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5839000: 0.000812\n",
      "Training accuracy: 31.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5840000: 0.001114\n",
      "Training accuracy: 25.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5841000: 0.000813\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5842000: 0.000769\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5843000: 0.000933\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5844000: 0.000845\n",
      "Training accuracy: -72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5845000: 0.001254\n",
      "Training accuracy: -64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5846000: 0.000634\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5847000: 0.000564\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5848000: 0.000819\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5849000: 0.001356\n",
      "Training accuracy: 27.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5850000: 0.000980\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5851000: 0.000473\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5852000: 0.000671\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5853000: 0.001106\n",
      "Training accuracy: -66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5854000: 0.000703\n",
      "Training accuracy: -55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5855000: 0.001031\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5856000: 0.000648\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5857000: 0.000716\n",
      "Training accuracy: 110.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5858000: 0.001578\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5859000: 0.001072\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5860000: 0.001382\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5861000: 0.001111\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5862000: 0.000897\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5863000: 0.000822\n",
      "Training accuracy: -139.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5864000: 0.001222\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5865000: 0.000750\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5866000: 0.000697\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5867000: 0.001084\n",
      "Training accuracy: 105.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5868000: 0.000746\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5869000: 0.000798\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5870000: 0.000821\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5871000: 0.000654\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5872000: 0.000994\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5873000: 0.000877\n",
      "Training accuracy: -69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5874000: 0.001175\n",
      "Training accuracy: -59.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5875000: 0.000651\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5876000: 0.000570\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5877000: 0.000962\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5878000: 0.000907\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5879000: 0.000754\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5880000: 0.000557\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5881000: 0.000667\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5882000: 0.001074\n",
      "Training accuracy: -77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5883000: 0.000775\n",
      "Training accuracy: -107.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5884000: 0.001344\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5885000: 0.000511\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5886000: 0.000763\n",
      "Training accuracy: 120.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5887000: 0.001819\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5888000: 0.001094\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5889000: 0.001253\n",
      "Training accuracy: 16.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5890000: 0.000847\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5891000: 0.000854\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5892000: 0.001112\n",
      "Training accuracy: -6.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5893000: 0.001265\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5894000: 0.000679\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5895000: 0.000670\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5896000: 0.000986\n",
      "Training accuracy: 146.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5897000: 0.000691\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5898000: 0.000738\n",
      "Training accuracy: 39.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5899000: 0.000519\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5900000: 0.000700\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5901000: 0.000903\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5902000: 0.000953\n",
      "Training accuracy: -4.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5903000: 0.001172\n",
      "Training accuracy: -60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5904000: 0.000737\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5905000: 0.001231\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5906000: 0.000819\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5907000: 0.000730\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5908000: 0.000794\n",
      "Training accuracy: 71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5909000: 0.000583\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5910000: 0.000849\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5911000: 0.000724\n",
      "Training accuracy: -53.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5912000: 0.000662\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5913000: 0.001304\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5914000: 0.000529\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5915000: 0.000867\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5916000: 0.001991\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5917000: 0.001121\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5918000: 0.001191\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5919000: 0.000918\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5920000: 0.000667\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5921000: 0.001155\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5922000: 0.001184\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5923000: 0.000820\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5924000: 0.000631\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5925000: 0.001024\n",
      "Training accuracy: 185.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5926000: 0.000669\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5927000: 0.001020\n",
      "Training accuracy: 24.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5928000: 0.000638\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5929000: 0.000458\n",
      "Training accuracy: 96.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5930000: 0.000922\n",
      "Training accuracy: 120.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5931000: 0.000898\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5932000: 0.000855\n",
      "Training accuracy: -59.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5933000: 0.000709\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5934000: 0.001253\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5935000: 0.000810\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5936000: 0.000753\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5937000: 0.000862\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5938000: 0.000612\n",
      "Training accuracy: 128.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5939000: 0.000840\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5940000: 0.000714\n",
      "Training accuracy: -50.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5941000: 0.000650\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5942000: 0.001292\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5943000: 0.000873\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5944000: 0.000979\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5945000: 0.002041\n",
      "Training accuracy: 3.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5946000: 0.001166\n",
      "Training accuracy: 1.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5947000: 0.001212\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5948000: 0.000750\n",
      "Training accuracy: 108.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5949000: 0.000665\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5950000: 0.001206\n",
      "Training accuracy: -50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5951000: 0.000946\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5952000: 0.000806\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5953000: 0.000637\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5954000: 0.001200\n",
      "Training accuracy: 169.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5955000: 0.001192\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5956000: 0.001260\n",
      "Training accuracy: 10.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5957000: 0.000617\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5958000: 0.000472\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5959000: 0.000670\n",
      "Training accuracy: 123.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5960000: 0.000994\n",
      "Training accuracy: -88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5961000: 0.000898\n",
      "Training accuracy: -44.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5962000: 0.000803\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5963000: 0.001354\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5964000: 0.000711\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5965000: 0.000744\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5966000: 0.001094\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5967000: 0.000950\n",
      "Training accuracy: 125.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5968000: 0.000835\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5969000: 0.000728\n",
      "Training accuracy: -210.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5970000: 0.000696\n",
      "Training accuracy: 35.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5971000: 0.001080\n",
      "Training accuracy: 39.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5972000: 0.000865\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5973000: 0.001150\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5974000: 0.001806\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5975000: 0.001160\n",
      "Training accuracy: 24.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5976000: 0.001489\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5977000: 0.000716\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5978000: 0.000673\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5979000: 0.001236\n",
      "Training accuracy: -45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5980000: 0.000940\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5981000: 0.000663\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5982000: 0.000487\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5983000: 0.000840\n",
      "Training accuracy: 154.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5984000: 0.001138\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5985000: 0.001146\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5986000: 0.000607\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5987000: 0.000532\n",
      "Training accuracy: 101.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5988000: 0.001023\n",
      "Training accuracy: 93.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5989000: 0.001060\n",
      "Training accuracy: -101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5990000: 0.000753\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5991000: 0.000743\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5992000: 0.001307\n",
      "Training accuracy: 60.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5993000: 0.000716\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5994000: 0.000965\n",
      "Training accuracy: 51.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5995000: 0.001055\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5996000: 0.000940\n",
      "Training accuracy: 145.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5997000: 0.001022\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5998000: 0.000721\n",
      "Training accuracy: -147.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 5999000: 0.000693\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6000000: 0.001078\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6001000: 0.000866\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6002000: 0.001126\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6003000: 0.001219\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6004000: 0.001015\n",
      "Training accuracy: 35.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6005000: 0.001145\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6006000: 0.000944\n",
      "Training accuracy: 112.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6007000: 0.000685\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6008000: 0.001370\n",
      "Training accuracy: -103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6009000: 0.000919\n",
      "Training accuracy: 109.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6010000: 0.000666\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6011000: 0.000481\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6012000: 0.000830\n",
      "Training accuracy: 158.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6013000: 0.001293\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6014000: 0.001151\n",
      "Training accuracy: 30.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6015000: 0.000597\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6016000: 0.000557\n",
      "Training accuracy: 106.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6017000: 0.000985\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6018000: 0.001046\n",
      "Training accuracy: -68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6019000: 0.000752\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6020000: 0.000704\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6021000: 0.001368\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6022000: 0.000618\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6023000: 0.001075\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6024000: 0.001024\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6025000: 0.001144\n",
      "Training accuracy: 119.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6026000: 0.001086\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6027000: 0.000696\n",
      "Training accuracy: -129.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6028000: 0.000925\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6029000: 0.000776\n",
      "Training accuracy: 24.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6030000: 0.000880\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6031000: 0.001047\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6032000: 0.000896\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6033000: 0.000997\n",
      "Training accuracy: 32.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6034000: 0.000826\n",
      "Training accuracy: 73.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6035000: 0.000869\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6036000: 0.001049\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6037000: 0.001012\n",
      "Training accuracy: -100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6038000: 0.001188\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6039000: 0.000687\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6040000: 0.000486\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6041000: 0.000877\n",
      "Training accuracy: 119.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6042000: 0.001464\n",
      "Training accuracy: 31.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6043000: 0.001167\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6044000: 0.000553\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6045000: 0.000532\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6046000: 0.000990\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6047000: 0.000795\n",
      "Training accuracy: -57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6048000: 0.001047\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6049000: 0.000632\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6050000: 0.000733\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6051000: 0.000890\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6052000: 0.001114\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6053000: 0.001041\n",
      "Training accuracy: 37.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6054000: 0.001125\n",
      "Training accuracy: 114.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6055000: 0.000894\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6056000: 0.000793\n",
      "Training accuracy: -135.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6057000: 0.001264\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6058000: 0.000734\n",
      "Training accuracy: 45.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6059000: 0.000999\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6060000: 0.001242\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6061000: 0.000801\n",
      "Training accuracy: 31.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6062000: 0.001110\n",
      "Training accuracy: 27.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6063000: 0.000813\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6064000: 0.000775\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6065000: 0.000935\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6066000: 0.000839\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6067000: 0.001262\n",
      "Training accuracy: -65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6068000: 0.000645\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6069000: 0.000564\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6070000: 0.000817\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6071000: 0.001363\n",
      "Training accuracy: 26.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6072000: 0.000985\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6073000: 0.000474\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6074000: 0.000667\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6075000: 0.001101\n",
      "Training accuracy: -66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6076000: 0.000698\n",
      "Training accuracy: -54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6077000: 0.001041\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6078000: 0.000639\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6079000: 0.000733\n",
      "Training accuracy: 109.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6080000: 0.001671\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6081000: 0.001073\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6082000: 0.001224\n",
      "Training accuracy: 38.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6083000: 0.001113\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6084000: 0.000883\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6085000: 0.000821\n",
      "Training accuracy: -139.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6086000: 0.001229\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6087000: 0.000753\n",
      "Training accuracy: 50.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6088000: 0.000689\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6089000: 0.001083\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6090000: 0.000707\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6091000: 0.000685\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6092000: 0.000835\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6093000: 0.000641\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6094000: 0.000941\n",
      "Training accuracy: 88.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6095000: 0.000827\n",
      "Training accuracy: 4.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6096000: 0.001181\n",
      "Training accuracy: -56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6097000: 0.000658\n",
      "Training accuracy: 105.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6098000: 0.000570\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6099000: 0.000801\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6100000: 0.000899\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6101000: 0.000754\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6102000: 0.000579\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6103000: 0.000664\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6104000: 0.001056\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6105000: 0.000716\n",
      "Training accuracy: 45.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6106000: 0.001348\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6107000: 0.000507\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6108000: 0.000747\n",
      "Training accuracy: 120.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6109000: 0.001830\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6110000: 0.001090\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6111000: 0.001273\n",
      "Training accuracy: 15.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6112000: 0.000853\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6113000: 0.000871\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6114000: 0.001133\n",
      "Training accuracy: -7.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6115000: 0.001218\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6116000: 0.000685\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6117000: 0.000670\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6118000: 0.000994\n",
      "Training accuracy: 146.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6119000: 0.000736\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6120000: 0.000733\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6121000: 0.000519\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6122000: 0.000705\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6123000: 0.000904\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6124000: 0.000930\n",
      "Training accuracy: -3.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6125000: 0.001157\n",
      "Training accuracy: -59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6126000: 0.000742\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6127000: 0.001222\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6128000: 0.000807\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6129000: 0.000729\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6130000: 0.000764\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6131000: 0.000581\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6132000: 0.000834\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6133000: 0.000723\n",
      "Training accuracy: -53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6134000: 0.000669\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6135000: 0.001323\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6136000: 0.000544\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6137000: 0.000870\n",
      "Training accuracy: 117.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6138000: 0.001996\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6139000: 0.000797\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6140000: 0.001200\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6141000: 0.000926\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6142000: 0.000665\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6143000: 0.001166\n",
      "Training accuracy: 49.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6144000: 0.001174\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6145000: 0.000829\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6146000: 0.000638\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6147000: 0.001034\n",
      "Training accuracy: 184.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6148000: 0.000688\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6149000: 0.001130\n",
      "Training accuracy: 15.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6150000: 0.000641\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6151000: 0.000458\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6152000: 0.000927\n",
      "Training accuracy: 127.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6153000: 0.000905\n",
      "Training accuracy: 53.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6154000: 0.000857\n",
      "Training accuracy: -59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6155000: 0.000721\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6156000: 0.001258\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6157000: 0.000814\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6158000: 0.000742\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6159000: 0.000852\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6160000: 0.000608\n",
      "Training accuracy: 128.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6161000: 0.000837\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6162000: 0.000755\n",
      "Training accuracy: -187.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6163000: 0.000633\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6164000: 0.001292\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6165000: 0.000884\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6166000: 0.001072\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6167000: 0.002065\n",
      "Training accuracy: 16.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6168000: 0.001146\n",
      "Training accuracy: 2.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6169000: 0.001212\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6170000: 0.000741\n",
      "Training accuracy: 109.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6171000: 0.000666\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6172000: 0.001335\n",
      "Training accuracy: -56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6173000: 0.000956\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6174000: 0.000799\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6175000: 0.000645\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6176000: 0.001166\n",
      "Training accuracy: 147.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6177000: 0.001207\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6178000: 0.001260\n",
      "Training accuracy: 10.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6179000: 0.000612\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6180000: 0.000481\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6181000: 0.000673\n",
      "Training accuracy: 122.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6182000: 0.000990\n",
      "Training accuracy: -88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6183000: 0.000584\n",
      "Training accuracy: -41.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6184000: 0.000803\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6185000: 0.001359\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6186000: 0.000697\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6187000: 0.000745\n",
      "Training accuracy: 50.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6188000: 0.001096\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6189000: 0.000951\n",
      "Training accuracy: 123.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6190000: 0.000862\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6191000: 0.000726\n",
      "Training accuracy: -210.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6192000: 0.000692\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6193000: 0.001087\n",
      "Training accuracy: 39.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6194000: 0.000872\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6195000: 0.001171\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6196000: 0.001804\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6197000: 0.001016\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6198000: 0.001486\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6199000: 0.000832\n",
      "Training accuracy: 103.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6200000: 0.000677\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6201000: 0.001237\n",
      "Training accuracy: -45.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6202000: 0.000607\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6203000: 0.000677\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6204000: 0.000489\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6205000: 0.000843\n",
      "Training accuracy: 156.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6206000: 0.001245\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6207000: 0.001159\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6208000: 0.000600\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6209000: 0.000543\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6210000: 0.001031\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6211000: 0.001059\n",
      "Training accuracy: -102.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6212000: 0.000745\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6213000: 0.000734\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6214000: 0.001314\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6215000: 0.000734\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6216000: 0.000977\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6217000: 0.001035\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6218000: 0.000946\n",
      "Training accuracy: 145.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6219000: 0.001024\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6220000: 0.000735\n",
      "Training accuracy: -148.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6221000: 0.000702\n",
      "Training accuracy: 35.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6222000: 0.001073\n",
      "Training accuracy: 42.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6223000: 0.000866\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6224000: 0.001110\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6225000: 0.001131\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6226000: 0.001012\n",
      "Training accuracy: 33.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6227000: 0.001156\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6228000: 0.000945\n",
      "Training accuracy: 112.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6229000: 0.000689\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6230000: 0.001362\n",
      "Training accuracy: -103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6231000: 0.000917\n",
      "Training accuracy: 110.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6232000: 0.000676\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6233000: 0.000500\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6234000: 0.000833\n",
      "Training accuracy: 157.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6235000: 0.001294\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6236000: 0.001152\n",
      "Training accuracy: 30.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6237000: 0.000578\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6238000: 0.000565\n",
      "Training accuracy: 105.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6239000: 0.000944\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6240000: 0.001053\n",
      "Training accuracy: -68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6241000: 0.000746\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6242000: 0.000699\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6243000: 0.001386\n",
      "Training accuracy: 71.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6244000: 0.000616\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6245000: 0.001066\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6246000: 0.001043\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6247000: 0.001122\n",
      "Training accuracy: 116.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6248000: 0.001099\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6249000: 0.000701\n",
      "Training accuracy: -130.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6250000: 0.000940\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6251000: 0.000770\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6252000: 0.000894\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6253000: 0.001056\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6254000: 0.000896\n",
      "Training accuracy: 46.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6255000: 0.000998\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6256000: 0.000805\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6257000: 0.000868\n",
      "Training accuracy: 101.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6258000: 0.001040\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6259000: 0.000999\n",
      "Training accuracy: -96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6260000: 0.001174\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6261000: 0.000680\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6262000: 0.000501\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6263000: 0.000864\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6264000: 0.001402\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6265000: 0.001158\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6266000: 0.000546\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6267000: 0.000527\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6268000: 0.000980\n",
      "Training accuracy: 116.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6269000: 0.000795\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6270000: 0.001044\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6271000: 0.000619\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6272000: 0.000736\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6273000: 0.000891\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6274000: 0.001122\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6275000: 0.001038\n",
      "Training accuracy: 37.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6276000: 0.001126\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6277000: 0.000895\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6278000: 0.000796\n",
      "Training accuracy: -136.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6279000: 0.001253\n",
      "Training accuracy: 73.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6280000: 0.000710\n",
      "Training accuracy: 37.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6281000: 0.000986\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6282000: 0.001227\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6283000: 0.000794\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6284000: 0.001102\n",
      "Training accuracy: 28.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6285000: 0.000819\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6286000: 0.000764\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6287000: 0.000977\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6288000: 0.000832\n",
      "Training accuracy: -70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6289000: 0.001263\n",
      "Training accuracy: -65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6290000: 0.000636\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6291000: 0.000562\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6292000: 0.000818\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6293000: 0.001357\n",
      "Training accuracy: 26.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6294000: 0.000892\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6295000: 0.000482\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6296000: 0.000669\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6297000: 0.001078\n",
      "Training accuracy: -72.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6298000: 0.000739\n",
      "Training accuracy: -79.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6299000: 0.001051\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6300000: 0.000634\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6301000: 0.000744\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6302000: 0.001782\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6303000: 0.001092\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6304000: 0.001514\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6305000: 0.001114\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6306000: 0.000876\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6307000: 0.000775\n",
      "Training accuracy: -2.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6308000: 0.001262\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6309000: 0.000758\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6310000: 0.000688\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6311000: 0.000979\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6312000: 0.000671\n",
      "Training accuracy: 63.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6313000: 0.000706\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6314000: 0.000845\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6315000: 0.000642\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6316000: 0.000948\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6317000: 0.000863\n",
      "Training accuracy: 5.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6318000: 0.001174\n",
      "Training accuracy: -56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6319000: 0.000675\n",
      "Training accuracy: 104.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6320000: 0.000557\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6321000: 0.000803\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6322000: 0.000895\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6323000: 0.000747\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6324000: 0.000584\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6325000: 0.000704\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6326000: 0.001062\n",
      "Training accuracy: -77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6327000: 0.000727\n",
      "Training accuracy: 45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6328000: 0.001348\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6329000: 0.000502\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6330000: 0.000854\n",
      "Training accuracy: 115.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6331000: 0.001844\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6332000: 0.001088\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6333000: 0.001275\n",
      "Training accuracy: 15.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6334000: 0.000884\n",
      "Training accuracy: 115.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6335000: 0.000847\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6336000: 0.001123\n",
      "Training accuracy: -6.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6337000: 0.001217\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6338000: 0.000692\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6339000: 0.000667\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6340000: 0.000972\n",
      "Training accuracy: 147.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6341000: 0.000747\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6342000: 0.001030\n",
      "Training accuracy: 32.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6343000: 0.000523\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6344000: 0.000591\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6345000: 0.000894\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6346000: 0.000932\n",
      "Training accuracy: -3.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6347000: 0.001175\n",
      "Training accuracy: -60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6348000: 0.000738\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6349000: 0.001236\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6350000: 0.000801\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6351000: 0.000736\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6352000: 0.000757\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6353000: 0.000591\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6354000: 0.000823\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6355000: 0.000719\n",
      "Training accuracy: -53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6356000: 0.000687\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6357000: 0.001323\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6358000: 0.000545\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6359000: 0.000881\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6360000: 0.001984\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6361000: 0.000781\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6362000: 0.001208\n",
      "Training accuracy: 30.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6363000: 0.000930\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6364000: 0.000672\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6365000: 0.001199\n",
      "Training accuracy: 27.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6366000: 0.001166\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6367000: 0.000832\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6368000: 0.000646\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6369000: 0.001050\n",
      "Training accuracy: 183.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6370000: 0.000674\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6371000: 0.001130\n",
      "Training accuracy: 16.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6372000: 0.000636\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6373000: 0.000461\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6374000: 0.000930\n",
      "Training accuracy: 127.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6375000: 0.000915\n",
      "Training accuracy: 53.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6376000: 0.000863\n",
      "Training accuracy: -59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6377000: 0.000815\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6378000: 0.001307\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6379000: 0.000821\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6380000: 0.000744\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6381000: 0.000851\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6382000: 0.000624\n",
      "Training accuracy: 127.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6383000: 0.000846\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6384000: 0.000768\n",
      "Training accuracy: -188.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6385000: 0.000638\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6386000: 0.001364\n",
      "Training accuracy: 37.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6387000: 0.000869\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6388000: 0.001067\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6389000: 0.002078\n",
      "Training accuracy: 16.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6390000: 0.001149\n",
      "Training accuracy: 2.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6391000: 0.001199\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6392000: 0.000742\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6393000: 0.000667\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6394000: 0.001340\n",
      "Training accuracy: -56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6395000: 0.000948\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6396000: 0.000806\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6397000: 0.000632\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6398000: 0.001157\n",
      "Training accuracy: 148.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6399000: 0.001201\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6400000: 0.001251\n",
      "Training accuracy: 11.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6401000: 0.000613\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6402000: 0.000485\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6403000: 0.000672\n",
      "Training accuracy: 122.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6404000: 0.000979\n",
      "Training accuracy: -92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6405000: 0.000693\n",
      "Training accuracy: -47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6406000: 0.000814\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6407000: 0.001358\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6408000: 0.000705\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6409000: 0.000745\n",
      "Training accuracy: 50.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6410000: 0.001108\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6411000: 0.000959\n",
      "Training accuracy: 123.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6412000: 0.000869\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6413000: 0.000725\n",
      "Training accuracy: -210.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6414000: 0.000701\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6415000: 0.001099\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6416000: 0.000870\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6417000: 0.001168\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6418000: 0.001806\n",
      "Training accuracy: 27.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6419000: 0.001003\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6420000: 0.001491\n",
      "Training accuracy: 44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6421000: 0.000931\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6422000: 0.000679\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6423000: 0.001306\n",
      "Training accuracy: -70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6424000: 0.000634\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6425000: 0.000673\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6426000: 0.000493\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6427000: 0.000845\n",
      "Training accuracy: 157.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6428000: 0.001247\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6429000: 0.001159\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6430000: 0.000602\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6431000: 0.000541\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6432000: 0.000995\n",
      "Training accuracy: 114.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6433000: 0.001085\n",
      "Training accuracy: -94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6434000: 0.000741\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6435000: 0.000736\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6436000: 0.001320\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6437000: 0.000768\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6438000: 0.000985\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6439000: 0.001018\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6440000: 0.000935\n",
      "Training accuracy: 146.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6441000: 0.001001\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6442000: 0.000759\n",
      "Training accuracy: -143.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6443000: 0.000656\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6444000: 0.001111\n",
      "Training accuracy: 18.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6445000: 0.000873\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6446000: 0.001091\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6447000: 0.001005\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6448000: 0.000995\n",
      "Training accuracy: 34.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6449000: 0.000873\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6450000: 0.000954\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6451000: 0.001016\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6452000: 0.001362\n",
      "Training accuracy: -103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6453000: 0.000882\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6454000: 0.000672\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6455000: 0.000490\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6456000: 0.000833\n",
      "Training accuracy: 157.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6457000: 0.001385\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6458000: 0.001218\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6459000: 0.000567\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6460000: 0.000572\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6461000: 0.000945\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6462000: 0.000886\n",
      "Training accuracy: -63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6463000: 0.000742\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6464000: 0.000699\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6465000: 0.001392\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6466000: 0.000600\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6467000: 0.001061\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6468000: 0.001042\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6469000: 0.001117\n",
      "Training accuracy: 116.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6470000: 0.001064\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6471000: 0.000694\n",
      "Training accuracy: -129.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6472000: 0.000932\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6473000: 0.000769\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6474000: 0.000897\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6475000: 0.000948\n",
      "Training accuracy: 104.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6476000: 0.000882\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6477000: 0.000997\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6478000: 0.000809\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6479000: 0.000830\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6480000: 0.001053\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6481000: 0.001000\n",
      "Training accuracy: -96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6482000: 0.001176\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6483000: 0.000671\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6484000: 0.000504\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6485000: 0.000864\n",
      "Training accuracy: 120.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6486000: 0.001389\n",
      "Training accuracy: 41.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6487000: 0.000849\n",
      "Training accuracy: 51.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6488000: 0.000547\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6489000: 0.000525\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6490000: 0.001027\n",
      "Training accuracy: -20.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6491000: 0.000793\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6492000: 0.001031\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6493000: 0.000607\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6494000: 0.000733\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6495000: 0.000904\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6496000: 0.001116\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6497000: 0.001358\n",
      "Training accuracy: 34.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6498000: 0.001127\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6499000: 0.000893\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6500000: 0.000798\n",
      "Training accuracy: -136.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6501000: 0.001243\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6502000: 0.000724\n",
      "Training accuracy: 36.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6503000: 0.001003\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6504000: 0.001211\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6505000: 0.000786\n",
      "Training accuracy: 32.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6506000: 0.001104\n",
      "Training accuracy: 27.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6507000: 0.000812\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6508000: 0.000757\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6509000: 0.000969\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6510000: 0.000786\n",
      "Training accuracy: -47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6511000: 0.001273\n",
      "Training accuracy: -66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6512000: 0.000643\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6513000: 0.000568\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6514000: 0.000801\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6515000: 0.001362\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6516000: 0.000890\n",
      "Training accuracy: 60.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6517000: 0.000480\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6518000: 0.000665\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6519000: 0.001134\n",
      "Training accuracy: -89.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6520000: 0.000734\n",
      "Training accuracy: -79.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6521000: 0.001374\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6522000: 0.000529\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6523000: 0.000681\n",
      "Training accuracy: 127.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6524000: 0.001887\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6525000: 0.001085\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6526000: 0.001524\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6527000: 0.001107\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6528000: 0.000873\n",
      "Training accuracy: 89.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6529000: 0.000769\n",
      "Training accuracy: -2.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6530000: 0.001266\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6531000: 0.000690\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6532000: 0.000690\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6533000: 0.000964\n",
      "Training accuracy: 110.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6534000: 0.000654\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6535000: 0.000707\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6536000: 0.000845\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6537000: 0.000645\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6538000: 0.000938\n",
      "Training accuracy: 90.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6539000: 0.000949\n",
      "Training accuracy: -0.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6540000: 0.001172\n",
      "Training accuracy: -56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6541000: 0.000684\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6542000: 0.000553\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6543000: 0.000815\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6544000: 0.000888\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6545000: 0.000750\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6546000: 0.000593\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6547000: 0.000711\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6548000: 0.001060\n",
      "Training accuracy: -76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6549000: 0.000727\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6550000: 0.001338\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6551000: 0.000495\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6552000: 0.000846\n",
      "Training accuracy: 118.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6553000: 0.001840\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6554000: 0.001084\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6555000: 0.001264\n",
      "Training accuracy: 15.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6556000: 0.000926\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6557000: 0.000845\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6558000: 0.001181\n",
      "Training accuracy: -24.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6559000: 0.001209\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6560000: 0.000690\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6561000: 0.000670\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6562000: 0.000983\n",
      "Training accuracy: 146.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6563000: 0.000747\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6564000: 0.001028\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6565000: 0.000524\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6566000: 0.000490\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6567000: 0.000891\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6568000: 0.000865\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6569000: 0.001155\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6570000: 0.000737\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6571000: 0.001233\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6572000: 0.000816\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6573000: 0.000720\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6574000: 0.000757\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6575000: 0.000603\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6576000: 0.000826\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6577000: 0.000717\n",
      "Training accuracy: -53.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6578000: 0.000665\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6579000: 0.001332\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6580000: 0.000543\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6581000: 0.000871\n",
      "Training accuracy: 117.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6582000: 0.001952\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6583000: 0.000772\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6584000: 0.001213\n",
      "Training accuracy: 29.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6585000: 0.000941\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6586000: 0.000674\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6587000: 0.001187\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6588000: 0.001171\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6589000: 0.000783\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6590000: 0.000631\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6591000: 0.001055\n",
      "Training accuracy: 183.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6592000: 0.000791\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6593000: 0.001137\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6594000: 0.000631\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6595000: 0.000447\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6596000: 0.000602\n",
      "Training accuracy: 132.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6597000: 0.000936\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6598000: 0.000869\n",
      "Training accuracy: -60.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6599000: 0.000822\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6600000: 0.001299\n",
      "Training accuracy: 44.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6601000: 0.000829\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6602000: 0.000758\n",
      "Training accuracy: 54.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6603000: 0.000766\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6604000: 0.000962\n",
      "Training accuracy: 124.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6605000: 0.000847\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6606000: 0.000776\n",
      "Training accuracy: -188.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6607000: 0.000639\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6608000: 0.001370\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6609000: 0.000860\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6610000: 0.001063\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6611000: 0.002102\n",
      "Training accuracy: 22.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6612000: 0.001137\n",
      "Training accuracy: 3.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6613000: 0.001518\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6614000: 0.000739\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6615000: 0.000668\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6616000: 0.001342\n",
      "Training accuracy: -56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6617000: 0.000940\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6618000: 0.000792\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6619000: 0.000647\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6620000: 0.001152\n",
      "Training accuracy: 149.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6621000: 0.001205\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6622000: 0.001248\n",
      "Training accuracy: 11.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6623000: 0.000610\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6624000: 0.000510\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6625000: 0.000655\n",
      "Training accuracy: 123.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6626000: 0.001083\n",
      "Training accuracy: -101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6627000: 0.000693\n",
      "Training accuracy: -47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6628000: 0.000816\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6629000: 0.001356\n",
      "Training accuracy: 45.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6630000: 0.000709\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6631000: 0.000754\n",
      "Training accuracy: 49.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6632000: 0.001175\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6633000: 0.000951\n",
      "Training accuracy: 124.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6634000: 0.000865\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6635000: 0.000687\n",
      "Training accuracy: -74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6636000: 0.000708\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6637000: 0.001090\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6638000: 0.000891\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6639000: 0.001148\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6640000: 0.001791\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6641000: 0.001000\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6642000: 0.001169\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6643000: 0.000926\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6644000: 0.000684\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6645000: 0.001347\n",
      "Training accuracy: -86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6646000: 0.000628\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6647000: 0.000674\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6648000: 0.000473\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6649000: 0.000852\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6650000: 0.001255\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6651000: 0.001160\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6652000: 0.000617\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6653000: 0.000538\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6654000: 0.001003\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6655000: 0.001089\n",
      "Training accuracy: -92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6656000: 0.000748\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6657000: 0.000729\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6658000: 0.001319\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6659000: 0.000772\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6660000: 0.000979\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6661000: 0.001030\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6662000: 0.000938\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6663000: 0.001000\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6664000: 0.000709\n",
      "Training accuracy: -126.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6665000: 0.000657\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6666000: 0.000775\n",
      "Training accuracy: 22.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6667000: 0.000878\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6668000: 0.001109\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6669000: 0.000890\n",
      "Training accuracy: 49.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6670000: 0.001007\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6671000: 0.000879\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6672000: 0.000944\n",
      "Training accuracy: 112.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6673000: 0.001010\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6674000: 0.001358\n",
      "Training accuracy: -103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6675000: 0.000868\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6676000: 0.000672\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6677000: 0.000493\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6678000: 0.000853\n",
      "Training accuracy: 156.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6679000: 0.001394\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6680000: 0.001215\n",
      "Training accuracy: 24.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6681000: 0.000562\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6682000: 0.000578\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6683000: 0.000944\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6684000: 0.000791\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6685000: 0.000739\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6686000: 0.000683\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6687000: 0.001408\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6688000: 0.000599\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6689000: 0.001077\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6690000: 0.001047\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6691000: 0.001112\n",
      "Training accuracy: 116.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6692000: 0.001047\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6693000: 0.000708\n",
      "Training accuracy: -130.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6694000: 0.000935\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6695000: 0.000688\n",
      "Training accuracy: 30.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6696000: 0.000990\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6697000: 0.000942\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6698000: 0.000876\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6699000: 0.000998\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6700000: 0.000824\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6701000: 0.000786\n",
      "Training accuracy: 102.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6702000: 0.001048\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6703000: 0.000954\n",
      "Training accuracy: -78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6704000: 0.001179\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6705000: 0.000668\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6706000: 0.000507\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6707000: 0.000855\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6708000: 0.001382\n",
      "Training accuracy: 41.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6709000: 0.000860\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6710000: 0.000548\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6711000: 0.000534\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6712000: 0.001028\n",
      "Training accuracy: -20.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6713000: 0.000789\n",
      "Training accuracy: -59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6714000: 0.001029\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6715000: 0.000664\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6716000: 0.000736\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6717000: 0.000895\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6718000: 0.001126\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6719000: 0.001367\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6720000: 0.001095\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6721000: 0.000898\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6722000: 0.000797\n",
      "Training accuracy: -136.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6723000: 0.001232\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6724000: 0.000729\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6725000: 0.001010\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6726000: 0.001226\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6727000: 0.000776\n",
      "Training accuracy: 33.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6728000: 0.001120\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6729000: 0.000829\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6730000: 0.000746\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6731000: 0.000978\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6732000: 0.000789\n",
      "Training accuracy: -51.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6733000: 0.001276\n",
      "Training accuracy: -63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6734000: 0.000643\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6735000: 0.000578\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6736000: 0.000805\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6737000: 0.001278\n",
      "Training accuracy: 44.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6738000: 0.000882\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6739000: 0.000475\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6740000: 0.000666\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6741000: 0.001145\n",
      "Training accuracy: -91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6742000: 0.000713\n",
      "Training accuracy: -88.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6743000: 0.001369\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6744000: 0.000520\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6745000: 0.000690\n",
      "Training accuracy: 128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6746000: 0.001871\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6747000: 0.001078\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6748000: 0.001531\n",
      "Training accuracy: 32.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6749000: 0.000768\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6750000: 0.000862\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6751000: 0.001101\n",
      "Training accuracy: -5.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6752000: 0.001266\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6753000: 0.000693\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6754000: 0.000684\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6755000: 0.000995\n",
      "Training accuracy: 147.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6756000: 0.000672\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6757000: 0.000754\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6758000: 0.000524\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6759000: 0.000649\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6760000: 0.000929\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6761000: 0.000961\n",
      "Training accuracy: -0.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6762000: 0.001188\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6763000: 0.000692\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6764000: 0.000545\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6765000: 0.000809\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6766000: 0.000885\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6767000: 0.000757\n",
      "Training accuracy: 68.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6768000: 0.000597\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6769000: 0.000846\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6770000: 0.001076\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6771000: 0.000630\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6772000: 0.001338\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6773000: 0.000489\n",
      "Training accuracy: 73.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6774000: 0.000855\n",
      "Training accuracy: 117.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6775000: 0.001840\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6776000: 0.001080\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6777000: 0.001197\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6778000: 0.000926\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6779000: 0.000843\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6780000: 0.001183\n",
      "Training accuracy: -25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6781000: 0.001209\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6782000: 0.000811\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6783000: 0.000664\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6784000: 0.000983\n",
      "Training accuracy: 145.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6785000: 0.000748\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6786000: 0.001023\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6787000: 0.000532\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6788000: 0.000484\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6789000: 0.000895\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6790000: 0.000829\n",
      "Training accuracy: 39.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6791000: 0.001169\n",
      "Training accuracy: -64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6792000: 0.000722\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6793000: 0.001233\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6794000: 0.000808\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6795000: 0.000722\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6796000: 0.000765\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6797000: 0.000589\n",
      "Training accuracy: 128.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6798000: 0.000830\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6799000: 0.000717\n",
      "Training accuracy: -51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6800000: 0.000658\n",
      "Training accuracy: 58.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6801000: 0.001311\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6802000: 0.000543\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6803000: 0.000861\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6804000: 0.001999\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6805000: 0.000767\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6806000: 0.001210\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6807000: 0.000943\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6808000: 0.000670\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6809000: 0.001175\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6810000: 0.001165\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6811000: 0.000803\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6812000: 0.000630\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6813000: 0.001046\n",
      "Training accuracy: 175.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6814000: 0.000808\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6815000: 0.001134\n",
      "Training accuracy: 16.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6816000: 0.000621\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6817000: 0.000447\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6818000: 0.000603\n",
      "Training accuracy: 130.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6819000: 0.000939\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6820000: 0.000895\n",
      "Training accuracy: -43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6821000: 0.000818\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6822000: 0.001306\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6823000: 0.000856\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6824000: 0.000750\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6825000: 0.000773\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6826000: 0.000962\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6827000: 0.000842\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6828000: 0.000768\n",
      "Training accuracy: -187.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6829000: 0.000648\n",
      "Training accuracy: 50.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6830000: 0.001387\n",
      "Training accuracy: 37.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6831000: 0.000865\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6832000: 0.001048\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6833000: 0.002088\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6834000: 0.001124\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6835000: 0.001525\n",
      "Training accuracy: 21.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6836000: 0.000734\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6837000: 0.000680\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6838000: 0.001324\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6839000: 0.000945\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6840000: 0.000778\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6841000: 0.000552\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6842000: 0.001153\n",
      "Training accuracy: 148.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6843000: 0.001203\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6844000: 0.001260\n",
      "Training accuracy: 15.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6845000: 0.000599\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6846000: 0.000516\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6847000: 0.000664\n",
      "Training accuracy: 122.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6848000: 0.001075\n",
      "Training accuracy: -101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6849000: 0.000697\n",
      "Training accuracy: -47.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6850000: 0.000816\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6851000: 0.001356\n",
      "Training accuracy: 45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6852000: 0.000710\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6853000: 0.000641\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6854000: 0.001176\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6855000: 0.000952\n",
      "Training accuracy: 123.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6856000: 0.000856\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6857000: 0.000792\n",
      "Training accuracy: -82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6858000: 0.000708\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6859000: 0.001092\n",
      "Training accuracy: 42.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6860000: 0.000836\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6861000: 0.001155\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6862000: 0.001886\n",
      "Training accuracy: 22.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6863000: 0.000992\n",
      "Training accuracy: 29.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6864000: 0.001173\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6865000: 0.000938\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6866000: 0.000674\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6867000: 0.001385\n",
      "Training accuracy: -108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6868000: 0.000632\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6869000: 0.000660\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6870000: 0.000489\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6871000: 0.000853\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6872000: 0.001308\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6873000: 0.001197\n",
      "Training accuracy: 7.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6874000: 0.000600\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6875000: 0.000542\n",
      "Training accuracy: 107.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6876000: 0.000995\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6877000: 0.001102\n",
      "Training accuracy: -84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6878000: 0.000751\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6879000: 0.000727\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6880000: 0.001333\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6881000: 0.000769\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6882000: 0.000979\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6883000: 0.001030\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6884000: 0.000939\n",
      "Training accuracy: 146.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6885000: 0.001003\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6886000: 0.000695\n",
      "Training accuracy: -126.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6887000: 0.000671\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6888000: 0.000774\n",
      "Training accuracy: 22.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6889000: 0.000878\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6890000: 0.001110\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6891000: 0.000895\n",
      "Training accuracy: 49.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6892000: 0.001002\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6893000: 0.000873\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6894000: 0.000948\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6895000: 0.001025\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6896000: 0.001013\n",
      "Training accuracy: -99.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6897000: 0.000865\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6898000: 0.000667\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6899000: 0.000493\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6900000: 0.000842\n",
      "Training accuracy: 122.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6901000: 0.001462\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6902000: 0.001167\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6903000: 0.000571\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6904000: 0.000585\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6905000: 0.000938\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6906000: 0.000787\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6907000: 0.000730\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6908000: 0.000682\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6909000: 0.001411\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6910000: 0.000596\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6911000: 0.001078\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6912000: 0.001043\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6913000: 0.001109\n",
      "Training accuracy: 117.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6914000: 0.000898\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6915000: 0.000692\n",
      "Training accuracy: -129.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6916000: 0.000940\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6917000: 0.000712\n",
      "Training accuracy: 37.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6918000: 0.001049\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6919000: 0.000926\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6920000: 0.000940\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6921000: 0.001123\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6922000: 0.000828\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6923000: 0.000800\n",
      "Training accuracy: 102.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6924000: 0.001057\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6925000: 0.000949\n",
      "Training accuracy: -78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6926000: 0.001182\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6927000: 0.000562\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6928000: 0.000499\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6929000: 0.000865\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6930000: 0.001385\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6931000: 0.000876\n",
      "Training accuracy: 50.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6932000: 0.000546\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6933000: 0.000559\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6934000: 0.001122\n",
      "Training accuracy: -27.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6935000: 0.000791\n",
      "Training accuracy: -59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6936000: 0.001014\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6937000: 0.000663\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6938000: 0.000737\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6939000: 0.001232\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6940000: 0.001118\n",
      "Training accuracy: 45.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6941000: 0.001370\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6942000: 0.001110\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6943000: 0.000907\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6944000: 0.000798\n",
      "Training accuracy: -138.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6945000: 0.001232\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6946000: 0.000734\n",
      "Training accuracy: 35.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6947000: 0.001004\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6948000: 0.001234\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6949000: 0.000741\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6950000: 0.001120\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6951000: 0.000823\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6952000: 0.000737\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6953000: 0.000984\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6954000: 0.000861\n",
      "Training accuracy: -70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6955000: 0.001271\n",
      "Training accuracy: -62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6956000: 0.000625\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6957000: 0.000579\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6958000: 0.000812\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6959000: 0.001283\n",
      "Training accuracy: 44.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6960000: 0.000877\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6961000: 0.000527\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6962000: 0.000675\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6963000: 0.001147\n",
      "Training accuracy: -91.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6964000: 0.000710\n",
      "Training accuracy: -88.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6965000: 0.001341\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6966000: 0.000516\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6967000: 0.000688\n",
      "Training accuracy: 125.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6968000: 0.001834\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6969000: 0.001086\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6970000: 0.001524\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6971000: 0.000780\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6972000: 0.000862\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6973000: 0.001107\n",
      "Training accuracy: -6.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6974000: 0.001258\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6975000: 0.000677\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6976000: 0.000683\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6977000: 0.000995\n",
      "Training accuracy: 146.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6978000: 0.000673\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6979000: 0.000764\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6980000: 0.000515\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6981000: 0.000657\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6982000: 0.000920\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6983000: 0.000961\n",
      "Training accuracy: -0.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6984000: 0.001187\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6985000: 0.000696\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6986000: 0.000580\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6987000: 0.000811\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6988000: 0.000887\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6989000: 0.000761\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6990000: 0.000597\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6991000: 0.000840\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6992000: 0.001084\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6993000: 0.000632\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6994000: 0.001401\n",
      "Training accuracy: 49.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6995000: 0.000496\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6996000: 0.000856\n",
      "Training accuracy: 117.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6997000: 0.001991\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6998000: 0.001081\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 6999000: 0.001187\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7000000: 0.000955\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7001000: 0.000856\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7002000: 0.001079\n",
      "Training accuracy: -17.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7003000: 0.001206\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7004000: 0.000818\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7005000: 0.000665\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7006000: 0.000978\n",
      "Training accuracy: 146.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7007000: 0.000684\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7008000: 0.001020\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7009000: 0.000627\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7010000: 0.000479\n",
      "Training accuracy: 108.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7011000: 0.000902\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7012000: 0.000787\n",
      "Training accuracy: 62.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7013000: 0.001166\n",
      "Training accuracy: -64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7014000: 0.000727\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7015000: 0.001211\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7016000: 0.000792\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7017000: 0.000724\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7018000: 0.000712\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7019000: 0.000596\n",
      "Training accuracy: 128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7020000: 0.000844\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7021000: 0.000724\n",
      "Training accuracy: -51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7022000: 0.000651\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7023000: 0.001303\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7024000: 0.000553\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7025000: 0.000844\n",
      "Training accuracy: 110.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7026000: 0.001992\n",
      "Training accuracy: 27.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7027000: 0.000790\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7028000: 0.001212\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7029000: 0.000950\n",
      "Training accuracy: 113.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7030000: 0.000675\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7031000: 0.001176\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7032000: 0.001163\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7033000: 0.000803\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7034000: 0.000631\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7035000: 0.001045\n",
      "Training accuracy: 175.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7036000: 0.000812\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7037000: 0.001137\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7038000: 0.000623\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7039000: 0.000445\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7040000: 0.000665\n",
      "Training accuracy: 123.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7041000: 0.000938\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7042000: 0.000903\n",
      "Training accuracy: -44.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7043000: 0.000822\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7044000: 0.001316\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7045000: 0.000849\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7046000: 0.000733\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7047000: 0.000786\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7048000: 0.000951\n",
      "Training accuracy: 124.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7049000: 0.000835\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7050000: 0.000770\n",
      "Training accuracy: -186.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7051000: 0.000640\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7052000: 0.001386\n",
      "Training accuracy: 37.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7053000: 0.000872\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7054000: 0.001153\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7055000: 0.002105\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7056000: 0.001123\n",
      "Training accuracy: 5.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7057000: 0.001538\n",
      "Training accuracy: 21.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7058000: 0.000726\n",
      "Training accuracy: 109.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7059000: 0.000698\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7060000: 0.001323\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7061000: 0.000936\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7062000: 0.000761\n",
      "Training accuracy: 79.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7063000: 0.000498\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7064000: 0.001157\n",
      "Training accuracy: 148.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7065000: 0.001134\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7066000: 0.001131\n",
      "Training accuracy: 22.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7067000: 0.000600\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7068000: 0.000509\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7069000: 0.000665\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7070000: 0.001070\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7071000: 0.000689\n",
      "Training accuracy: -47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7072000: 0.000807\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7073000: 0.001349\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7074000: 0.000707\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7075000: 0.000635\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7076000: 0.001171\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7077000: 0.000958\n",
      "Training accuracy: 120.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7078000: 0.000838\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7079000: 0.000744\n",
      "Training accuracy: -149.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7080000: 0.000704\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7081000: 0.001090\n",
      "Training accuracy: 43.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7082000: 0.000839\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7083000: 0.001154\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7084000: 0.001555\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7085000: 0.001013\n",
      "Training accuracy: 36.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7086000: 0.001162\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7087000: 0.000921\n",
      "Training accuracy: 97.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7088000: 0.000666\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7089000: 0.001383\n",
      "Training accuracy: -108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7090000: 0.000631\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7091000: 0.000680\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7092000: 0.000488\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7093000: 0.000845\n",
      "Training accuracy: 157.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7094000: 0.001295\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7095000: 0.001206\n",
      "Training accuracy: 7.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7096000: 0.000600\n",
      "Training accuracy: 73.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7097000: 0.000548\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7098000: 0.000986\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7099000: 0.001038\n",
      "Training accuracy: -65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7100000: 0.000763\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7101000: 0.000726\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7102000: 0.001344\n",
      "Training accuracy: 66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7103000: 0.000752\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7104000: 0.000977\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7105000: 0.001030\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7106000: 0.000881\n",
      "Training accuracy: 163.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7107000: 0.001093\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7108000: 0.000688\n",
      "Training accuracy: -125.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7109000: 0.000666\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7110000: 0.000791\n",
      "Training accuracy: 22.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7111000: 0.000883\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7112000: 0.001118\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7113000: 0.000890\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7114000: 0.000996\n",
      "Training accuracy: 33.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7115000: 0.000885\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7116000: 0.000936\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7117000: 0.001028\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7118000: 0.001007\n",
      "Training accuracy: -98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7119000: 0.000860\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7120000: 0.000679\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7121000: 0.000490\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7122000: 0.000847\n",
      "Training accuracy: 122.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7123000: 0.001470\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7124000: 0.001165\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7125000: 0.000560\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7126000: 0.000576\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7127000: 0.000965\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7128000: 0.000789\n",
      "Training accuracy: -57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7129000: 0.000729\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7130000: 0.000679\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7131000: 0.001382\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7132000: 0.000912\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7133000: 0.001080\n",
      "Training accuracy: 45.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7134000: 0.001030\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7135000: 0.001120\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7136000: 0.000912\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7137000: 0.000689\n",
      "Training accuracy: -129.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7138000: 0.000947\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7139000: 0.000757\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7140000: 0.001037\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7141000: 0.001266\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7142000: 0.000799\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7143000: 0.001130\n",
      "Training accuracy: 22.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7144000: 0.000825\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7145000: 0.000775\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7146000: 0.001044\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7147000: 0.000947\n",
      "Training accuracy: -77.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7148000: 0.001197\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7149000: 0.000594\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7150000: 0.000497\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7151000: 0.000863\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7152000: 0.001350\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7153000: 0.000879\n",
      "Training accuracy: 49.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7154000: 0.000483\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7155000: 0.000555\n",
      "Training accuracy: 100.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7156000: 0.001127\n",
      "Training accuracy: -27.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7157000: 0.000805\n",
      "Training accuracy: -60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7158000: 0.001014\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7159000: 0.000666\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7160000: 0.000768\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7161000: 0.001243\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7162000: 0.001129\n",
      "Training accuracy: 43.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7163000: 0.001371\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7164000: 0.001113\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7165000: 0.000894\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7166000: 0.000802\n",
      "Training accuracy: -138.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7167000: 0.001218\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7168000: 0.000731\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7169000: 0.000997\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7170000: 0.001239\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7171000: 0.000759\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7172000: 0.001113\n",
      "Training accuracy: 20.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7173000: 0.000828\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7174000: 0.000726\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7175000: 0.000977\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7176000: 0.000869\n",
      "Training accuracy: -71.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7177000: 0.001282\n",
      "Training accuracy: -67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7178000: 0.000635\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7179000: 0.000584\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7180000: 0.000805\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7181000: 0.001274\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7182000: 0.000877\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7183000: 0.000538\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7184000: 0.000673\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7185000: 0.001093\n",
      "Training accuracy: -78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7186000: 0.000712\n",
      "Training accuracy: -88.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7187000: 0.001345\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7188000: 0.000518\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7189000: 0.000686\n",
      "Training accuracy: 125.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7190000: 0.001824\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7191000: 0.001090\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7192000: 0.001558\n",
      "Training accuracy: 13.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7193000: 0.000821\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7194000: 0.000859\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7195000: 0.001109\n",
      "Training accuracy: -5.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7196000: 0.001259\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7197000: 0.000692\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7198000: 0.000681\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7199000: 0.000888\n",
      "Training accuracy: 151.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7200000: 0.000674\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7201000: 0.000782\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7202000: 0.000515\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7203000: 0.000677\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7204000: 0.000900\n",
      "Training accuracy: 82.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7205000: 0.000964\n",
      "Training accuracy: -0.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7206000: 0.001189\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7207000: 0.000703\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7208000: 0.000590\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7209000: 0.000818\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7210000: 0.000759\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7211000: 0.000778\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7212000: 0.000591\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7213000: 0.000834\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7214000: 0.001078\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7215000: 0.000644\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7216000: 0.001415\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7217000: 0.000537\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7218000: 0.000858\n",
      "Training accuracy: 117.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7219000: 0.001989\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7220000: 0.001094\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7221000: 0.001186\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7222000: 0.000946\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7223000: 0.000870\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7224000: 0.001035\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7225000: 0.001198\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7226000: 0.000820\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7227000: 0.000662\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7228000: 0.001016\n",
      "Training accuracy: 186.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7229000: 0.000679\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7230000: 0.001003\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7231000: 0.000635\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7232000: 0.000479\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7233000: 0.000897\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7234000: 0.000891\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7235000: 0.001166\n",
      "Training accuracy: -64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7236000: 0.000716\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7237000: 0.001217\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7238000: 0.000800\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7239000: 0.000761\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7240000: 0.000702\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7241000: 0.000596\n",
      "Training accuracy: 128.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7242000: 0.000841\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7243000: 0.000723\n",
      "Training accuracy: -51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7244000: 0.000651\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7245000: 0.001293\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7246000: 0.000867\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7247000: 0.000996\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7248000: 0.001995\n",
      "Training accuracy: 26.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7249000: 0.000843\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7250000: 0.001213\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7251000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7252000: 0.000583\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7253000: 0.001176\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7254000: 0.001171\n",
      "Training accuracy: 107.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7255000: 0.000792\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7256000: 0.000632\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7257000: 0.001034\n",
      "Training accuracy: 175.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7258000: 0.000899\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7259000: 0.001155\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7260000: 0.000613\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7261000: 0.000443\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7262000: 0.000664\n",
      "Training accuracy: 123.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7263000: 0.000949\n",
      "Training accuracy: 62.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7264000: 0.000911\n",
      "Training accuracy: -44.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7265000: 0.000820\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7266000: 0.001333\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7267000: 0.000844\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7268000: 0.000733\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7269000: 0.001092\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7270000: 0.000956\n",
      "Training accuracy: 124.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7271000: 0.000836\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7272000: 0.000753\n",
      "Training accuracy: -195.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7273000: 0.000638\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7274000: 0.001381\n",
      "Training accuracy: 37.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7275000: 0.000871\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7276000: 0.001141\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7277000: 0.001786\n",
      "Training accuracy: 26.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7278000: 0.001127\n",
      "Training accuracy: 4.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7279000: 0.001533\n",
      "Training accuracy: 21.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7280000: 0.000717\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7281000: 0.000674\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7282000: 0.001322\n",
      "Training accuracy: -51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7283000: 0.000935\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7284000: 0.000666\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7285000: 0.000499\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7286000: 0.000828\n",
      "Training accuracy: 151.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7287000: 0.001137\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7288000: 0.001120\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7289000: 0.000605\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7290000: 0.000515\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7291000: 0.000983\n",
      "Training accuracy: 115.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7292000: 0.001079\n",
      "Training accuracy: -101.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7293000: 0.000792\n",
      "Training accuracy: -53.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7294000: 0.000782\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7295000: 0.001358\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7296000: 0.000704\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7297000: 0.000634\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7298000: 0.001173\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7299000: 0.000927\n",
      "Training accuracy: 146.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7300000: 0.001132\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7301000: 0.000730\n",
      "Training accuracy: -149.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7302000: 0.000703\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7303000: 0.001091\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7304000: 0.000858\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7305000: 0.001131\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7306000: 0.001564\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7307000: 0.001006\n",
      "Training accuracy: 36.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7308000: 0.001164\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7309000: 0.000933\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7310000: 0.000673\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7311000: 0.001382\n",
      "Training accuracy: -104.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7312000: 0.000641\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7313000: 0.000690\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7314000: 0.000491\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7315000: 0.000832\n",
      "Training accuracy: 158.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7316000: 0.001290\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7317000: 0.001190\n",
      "Training accuracy: 7.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7318000: 0.000595\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7319000: 0.000561\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7320000: 0.000996\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7321000: 0.001035\n",
      "Training accuracy: -65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7322000: 0.000757\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7323000: 0.000717\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7324000: 0.001352\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7325000: 0.000755\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7326000: 0.001068\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7327000: 0.001034\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7328000: 0.000874\n",
      "Training accuracy: 164.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7329000: 0.001093\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7330000: 0.000673\n",
      "Training accuracy: -128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7331000: 0.000981\n",
      "Training accuracy: 54.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7332000: 0.000780\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7333000: 0.000885\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7334000: 0.001109\n",
      "Training accuracy: 66.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7335000: 0.000901\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7336000: 0.001003\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7337000: 0.000837\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7338000: 0.000895\n",
      "Training accuracy: 100.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7339000: 0.001041\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7340000: 0.001008\n",
      "Training accuracy: -100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7341000: 0.000868\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7342000: 0.000670\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7343000: 0.000487\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7344000: 0.000967\n",
      "Training accuracy: 115.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7345000: 0.001469\n",
      "Training accuracy: 30.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7346000: 0.001148\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7347000: 0.000545\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7348000: 0.000561\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7349000: 0.000983\n",
      "Training accuracy: 118.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7350000: 0.000783\n",
      "Training accuracy: -57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7351000: 0.000722\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7352000: 0.000673\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7353000: 0.001371\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7354000: 0.000902\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7355000: 0.001077\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7356000: 0.001056\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7357000: 0.001121\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7358000: 0.000912\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7359000: 0.000701\n",
      "Training accuracy: -130.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7360000: 0.000953\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7361000: 0.000749\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7362000: 0.001011\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7363000: 0.001267\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7364000: 0.000810\n",
      "Training accuracy: 31.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7365000: 0.001120\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7366000: 0.000815\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7367000: 0.000767\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7368000: 0.001026\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7369000: 0.000940\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7370000: 0.001200\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7371000: 0.000623\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7372000: 0.000507\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7373000: 0.000826\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7374000: 0.001364\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7375000: 0.000978\n",
      "Training accuracy: 42.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7376000: 0.000477\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7377000: 0.000674\n",
      "Training accuracy: 94.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7378000: 0.001128\n",
      "Training accuracy: -28.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7379000: 0.000714\n",
      "Training accuracy: -55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7380000: 0.001026\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7381000: 0.000649\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7382000: 0.000763\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7383000: 0.001249\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7384000: 0.001082\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7385000: 0.001388\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7386000: 0.001114\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7387000: 0.000896\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7388000: 0.000820\n",
      "Training accuracy: -139.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7389000: 0.001222\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7390000: 0.000730\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7391000: 0.000693\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7392000: 0.001072\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7393000: 0.000756\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7394000: 0.001069\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7395000: 0.000824\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7396000: 0.000651\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7397000: 0.000987\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7398000: 0.000873\n",
      "Training accuracy: -69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7399000: 0.001274\n",
      "Training accuracy: -66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7400000: 0.000649\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7401000: 0.000588\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7402000: 0.000801\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7403000: 0.001185\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7404000: 0.000860\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7405000: 0.000572\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7406000: 0.000687\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7407000: 0.001082\n",
      "Training accuracy: -78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7408000: 0.000717\n",
      "Training accuracy: -88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7409000: 0.001348\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7410000: 0.000503\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7411000: 0.000679\n",
      "Training accuracy: 125.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7412000: 0.001820\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7413000: 0.001101\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7414000: 0.001251\n",
      "Training accuracy: 16.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7415000: 0.000836\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7416000: 0.000858\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7417000: 0.001104\n",
      "Training accuracy: -5.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7418000: 0.001267\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7419000: 0.000695\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7420000: 0.000684\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7421000: 0.000887\n",
      "Training accuracy: 152.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7422000: 0.000687\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7423000: 0.000776\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7424000: 0.000518\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7425000: 0.000684\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7426000: 0.000901\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7427000: 0.000959\n",
      "Training accuracy: -4.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7428000: 0.001178\n",
      "Training accuracy: -56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7429000: 0.000692\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7430000: 0.000901\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7431000: 0.000816\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7432000: 0.000724\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7433000: 0.000807\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7434000: 0.000589\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7435000: 0.000829\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7436000: 0.000760\n",
      "Training accuracy: -73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7437000: 0.000638\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7438000: 0.001297\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7439000: 0.000524\n",
      "Training accuracy: 53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7440000: 0.000862\n",
      "Training accuracy: 117.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7441000: 0.001990\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7442000: 0.001116\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7443000: 0.001192\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7444000: 0.000932\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7445000: 0.000670\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7446000: 0.001148\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7447000: 0.001185\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7448000: 0.000824\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7449000: 0.000641\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7450000: 0.001021\n",
      "Training accuracy: 186.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7451000: 0.000670\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7452000: 0.001016\n",
      "Training accuracy: 24.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7453000: 0.000639\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7454000: 0.000460\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7455000: 0.000915\n",
      "Training accuracy: 120.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7456000: 0.000895\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7457000: 0.001152\n",
      "Training accuracy: -63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7458000: 0.000708\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7459000: 0.001261\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7460000: 0.000803\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7461000: 0.000751\n",
      "Training accuracy: 54.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7462000: 0.000711\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7463000: 0.000607\n",
      "Training accuracy: 128.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7464000: 0.000830\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7465000: 0.000713\n",
      "Training accuracy: -50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7466000: 0.000650\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7467000: 0.001291\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7468000: 0.000873\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7469000: 0.000986\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7470000: 0.002007\n",
      "Training accuracy: 26.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7471000: 0.001050\n",
      "Training accuracy: 7.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7472000: 0.001209\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7473000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7474000: 0.000583\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7475000: 0.001167\n",
      "Training accuracy: 23.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7476000: 0.000958\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7477000: 0.000792\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7478000: 0.000635\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7479000: 0.001035\n",
      "Training accuracy: 174.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7480000: 0.001179\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7481000: 0.001264\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7482000: 0.000630\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7483000: 0.000467\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7484000: 0.000659\n",
      "Training accuracy: 123.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7485000: 0.000943\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7486000: 0.000902\n",
      "Training accuracy: -44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7487000: 0.000814\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7488000: 0.001336\n",
      "Training accuracy: 46.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7489000: 0.000721\n",
      "Training accuracy: 53.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7490000: 0.000748\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7491000: 0.001110\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7492000: 0.000956\n",
      "Training accuracy: 124.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7493000: 0.000837\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7494000: 0.000726\n",
      "Training accuracy: -209.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7495000: 0.000648\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7496000: 0.001402\n",
      "Training accuracy: 36.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7497000: 0.000879\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7498000: 0.001149\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7499000: 0.001789\n",
      "Training accuracy: 26.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7500000: 0.001164\n",
      "Training accuracy: 19.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7501000: 0.001488\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7502000: 0.000719\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7503000: 0.000673\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7504000: 0.001313\n",
      "Training accuracy: -50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7505000: 0.000926\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7506000: 0.000657\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7507000: 0.000478\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7508000: 0.000828\n",
      "Training accuracy: 151.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7509000: 0.001142\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7510000: 0.001128\n",
      "Training accuracy: 20.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7511000: 0.000614\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7512000: 0.000517\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7513000: 0.001027\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7514000: 0.001072\n",
      "Training accuracy: -101.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7515000: 0.000801\n",
      "Training accuracy: -53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7516000: 0.000759\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7517000: 0.001359\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7518000: 0.000720\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7519000: 0.000636\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7520000: 0.001075\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7521000: 0.000933\n",
      "Training accuracy: 146.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7522000: 0.001019\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7523000: 0.000730\n",
      "Training accuracy: -149.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7524000: 0.000683\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7525000: 0.001083\n",
      "Training accuracy: 43.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7526000: 0.000868\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7527000: 0.001130\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7528000: 0.001550\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7529000: 0.001017\n",
      "Training accuracy: 35.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7530000: 0.001149\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7531000: 0.000934\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7532000: 0.000670\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7533000: 0.001369\n",
      "Training accuracy: -103.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7534000: 0.000926\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7535000: 0.000697\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7536000: 0.000489\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7537000: 0.000835\n",
      "Training accuracy: 158.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7538000: 0.001295\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7539000: 0.001184\n",
      "Training accuracy: 8.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7540000: 0.000595\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7541000: 0.000552\n",
      "Training accuracy: 106.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7542000: 0.000986\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7543000: 0.001031\n",
      "Training accuracy: -67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7544000: 0.000757\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7545000: 0.000699\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7546000: 0.001349\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7547000: 0.000767\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7548000: 0.001064\n",
      "Training accuracy: 46.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7549000: 0.001036\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7550000: 0.001133\n",
      "Training accuracy: 120.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7551000: 0.001092\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7552000: 0.000692\n",
      "Training accuracy: -129.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7553000: 0.000981\n",
      "Training accuracy: 58.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7554000: 0.000768\n",
      "Training accuracy: 24.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7555000: 0.000886\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7556000: 0.001098\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7557000: 0.000903\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7558000: 0.000995\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7559000: 0.000840\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7560000: 0.000879\n",
      "Training accuracy: 100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7561000: 0.001052\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7562000: 0.001009\n",
      "Training accuracy: -100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7563000: 0.000866\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7564000: 0.000686\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7565000: 0.000478\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7566000: 0.000974\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7567000: 0.001471\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7568000: 0.001164\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7569000: 0.000548\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7570000: 0.000547\n",
      "Training accuracy: 93.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7571000: 0.000983\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7572000: 0.000791\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7573000: 0.001053\n",
      "Training accuracy: 93.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7574000: 0.000676\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7575000: 0.001063\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7576000: 0.000888\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7577000: 0.000969\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7578000: 0.001037\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7579000: 0.001122\n",
      "Training accuracy: 115.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7580000: 0.000911\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7581000: 0.000802\n",
      "Training accuracy: -136.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7582000: 0.000954\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7583000: 0.000751\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7584000: 0.001005\n",
      "Training accuracy: 71.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7585000: 0.001254\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7586000: 0.000812\n",
      "Training accuracy: 31.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7587000: 0.001113\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7588000: 0.000813\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7589000: 0.000769\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7590000: 0.000934\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7591000: 0.000844\n",
      "Training accuracy: -72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7592000: 0.001254\n",
      "Training accuracy: -65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7593000: 0.000634\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7594000: 0.000565\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7595000: 0.000819\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7596000: 0.001356\n",
      "Training accuracy: 27.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7597000: 0.000981\n",
      "Training accuracy: 52.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7598000: 0.000472\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7599000: 0.000671\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7600000: 0.001107\n",
      "Training accuracy: -66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7601000: 0.000702\n",
      "Training accuracy: -55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7602000: 0.001031\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7603000: 0.000648\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7604000: 0.000717\n",
      "Training accuracy: 110.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7605000: 0.001578\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7606000: 0.001072\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7607000: 0.001382\n",
      "Training accuracy: 32.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7608000: 0.001110\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7609000: 0.000896\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7610000: 0.000820\n",
      "Training accuracy: -139.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7611000: 0.001223\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7612000: 0.000748\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7613000: 0.000697\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7614000: 0.001084\n",
      "Training accuracy: 105.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7615000: 0.000746\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7616000: 0.000797\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7617000: 0.000821\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7618000: 0.000653\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7619000: 0.000994\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7620000: 0.000877\n",
      "Training accuracy: -69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7621000: 0.001174\n",
      "Training accuracy: -59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7622000: 0.000651\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7623000: 0.000571\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7624000: 0.000962\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7625000: 0.000907\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7626000: 0.000754\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7627000: 0.000556\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7628000: 0.000667\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7629000: 0.001074\n",
      "Training accuracy: -77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7630000: 0.000775\n",
      "Training accuracy: -107.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7631000: 0.001343\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7632000: 0.000510\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7633000: 0.000764\n",
      "Training accuracy: 120.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7634000: 0.001819\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7635000: 0.001094\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7636000: 0.001253\n",
      "Training accuracy: 16.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7637000: 0.000846\n",
      "Training accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7638000: 0.000853\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7639000: 0.001111\n",
      "Training accuracy: -6.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7640000: 0.001265\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7641000: 0.000677\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7642000: 0.000670\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7643000: 0.000986\n",
      "Training accuracy: 146.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7644000: 0.000690\n",
      "Training accuracy: 69.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7645000: 0.000737\n",
      "Training accuracy: 39.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7646000: 0.000519\n",
      "Training accuracy: 87.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7647000: 0.000699\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7648000: 0.000903\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7649000: 0.000953\n",
      "Training accuracy: -4.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7650000: 0.001172\n",
      "Training accuracy: -60.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7651000: 0.000736\n",
      "Training accuracy: 85.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7652000: 0.001233\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7653000: 0.000819\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7654000: 0.000729\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7655000: 0.000794\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7656000: 0.000582\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7657000: 0.000850\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7658000: 0.000724\n",
      "Training accuracy: -53.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7659000: 0.000661\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7660000: 0.001303\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7661000: 0.000529\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7662000: 0.000868\n",
      "Training accuracy: 117.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7663000: 0.001990\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7664000: 0.001121\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7665000: 0.001191\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7666000: 0.000917\n",
      "Training accuracy: 113.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7667000: 0.000666\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7668000: 0.001154\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7669000: 0.001183\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7670000: 0.000820\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7671000: 0.000631\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7672000: 0.001024\n",
      "Training accuracy: 185.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7673000: 0.000668\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7674000: 0.001019\n",
      "Training accuracy: 24.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7675000: 0.000639\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7676000: 0.000456\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7677000: 0.000922\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7678000: 0.000899\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7679000: 0.000855\n",
      "Training accuracy: -59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7680000: 0.000710\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7681000: 0.001253\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7682000: 0.000811\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7683000: 0.000751\n",
      "Training accuracy: 54.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7684000: 0.000862\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7685000: 0.000611\n",
      "Training accuracy: 128.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7686000: 0.000841\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7687000: 0.000714\n",
      "Training accuracy: -50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7688000: 0.000649\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7689000: 0.001292\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7690000: 0.000873\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7691000: 0.000980\n",
      "Training accuracy: 107.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7692000: 0.002041\n",
      "Training accuracy: 3.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7693000: 0.001166\n",
      "Training accuracy: 1.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7694000: 0.001213\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7695000: 0.000750\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7696000: 0.000665\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7697000: 0.001205\n",
      "Training accuracy: -50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7698000: 0.000945\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7699000: 0.000806\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7700000: 0.000637\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7701000: 0.001200\n",
      "Training accuracy: 169.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7702000: 0.001191\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7703000: 0.001260\n",
      "Training accuracy: 10.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7704000: 0.000617\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7705000: 0.000470\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7706000: 0.000669\n",
      "Training accuracy: 123.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7707000: 0.000994\n",
      "Training accuracy: -88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7708000: 0.000898\n",
      "Training accuracy: -44.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7709000: 0.000803\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7710000: 0.001355\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7711000: 0.000711\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7712000: 0.000742\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7713000: 0.001095\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7714000: 0.000949\n",
      "Training accuracy: 125.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7715000: 0.000836\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7716000: 0.000728\n",
      "Training accuracy: -210.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7717000: 0.000696\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7718000: 0.001079\n",
      "Training accuracy: 39.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7719000: 0.000865\n",
      "Training accuracy: 58.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7720000: 0.001150\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7721000: 0.001806\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7722000: 0.001161\n",
      "Training accuracy: 24.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7723000: 0.001489\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7724000: 0.000716\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7725000: 0.000674\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7726000: 0.001235\n",
      "Training accuracy: -45.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7727000: 0.000939\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7728000: 0.000663\n",
      "Training accuracy: 83.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7729000: 0.000487\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7730000: 0.000840\n",
      "Training accuracy: 154.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7731000: 0.001137\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7732000: 0.001146\n",
      "Training accuracy: 33.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7733000: 0.000607\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7734000: 0.000531\n",
      "Training accuracy: 101.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7735000: 0.001023\n",
      "Training accuracy: 93.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7736000: 0.001060\n",
      "Training accuracy: -101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7737000: 0.000754\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7738000: 0.000743\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7739000: 0.001308\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7740000: 0.000717\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7741000: 0.000964\n",
      "Training accuracy: 52.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7742000: 0.001055\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7743000: 0.000939\n",
      "Training accuracy: 145.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7744000: 0.001023\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7745000: 0.000721\n",
      "Training accuracy: -147.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7746000: 0.000694\n",
      "Training accuracy: 35.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7747000: 0.001076\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7748000: 0.000866\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7749000: 0.001126\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7750000: 0.001220\n",
      "Training accuracy: 30.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7751000: 0.001015\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7752000: 0.001146\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7753000: 0.000944\n",
      "Training accuracy: 112.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7754000: 0.000685\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7755000: 0.001368\n",
      "Training accuracy: -103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7756000: 0.000919\n",
      "Training accuracy: 109.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7757000: 0.000666\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7758000: 0.000481\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7759000: 0.000831\n",
      "Training accuracy: 158.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7760000: 0.001293\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7761000: 0.001152\n",
      "Training accuracy: 30.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7762000: 0.000597\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7763000: 0.000556\n",
      "Training accuracy: 106.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7764000: 0.000985\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7765000: 0.001046\n",
      "Training accuracy: -68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7766000: 0.000752\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7767000: 0.000704\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7768000: 0.001370\n",
      "Training accuracy: 72.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7769000: 0.000617\n",
      "Training accuracy: 89.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7770000: 0.001074\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7771000: 0.001024\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7772000: 0.001143\n",
      "Training accuracy: 119.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7773000: 0.001086\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7774000: 0.000695\n",
      "Training accuracy: -129.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7775000: 0.000926\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7776000: 0.000775\n",
      "Training accuracy: 24.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7777000: 0.000880\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7778000: 0.001047\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7779000: 0.000896\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7780000: 0.000997\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7781000: 0.000826\n",
      "Training accuracy: 73.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7782000: 0.000868\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7783000: 0.001050\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7784000: 0.001010\n",
      "Training accuracy: -100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7785000: 0.001188\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7786000: 0.000687\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7787000: 0.000486\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7788000: 0.000878\n",
      "Training accuracy: 119.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7789000: 0.001464\n",
      "Training accuracy: 31.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7790000: 0.001168\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7791000: 0.000553\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7792000: 0.000531\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7793000: 0.000991\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7794000: 0.000794\n",
      "Training accuracy: -57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7795000: 0.001047\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7796000: 0.000632\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7797000: 0.000733\n",
      "Training accuracy: 60.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7798000: 0.000889\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7799000: 0.001114\n",
      "Training accuracy: 48.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7800000: 0.001041\n",
      "Training accuracy: 37.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7801000: 0.001124\n",
      "Training accuracy: 115.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7802000: 0.000893\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7803000: 0.000792\n",
      "Training accuracy: -135.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7804000: 0.001265\n",
      "Training accuracy: 75.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7805000: 0.000732\n",
      "Training accuracy: 45.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7806000: 0.000999\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7807000: 0.001241\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7808000: 0.000801\n",
      "Training accuracy: 32.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7809000: 0.001110\n",
      "Training accuracy: 27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7810000: 0.000813\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7811000: 0.000775\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7812000: 0.000936\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7813000: 0.000837\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7814000: 0.001263\n",
      "Training accuracy: -66.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7815000: 0.000645\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7816000: 0.000565\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7817000: 0.000817\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7818000: 0.001363\n",
      "Training accuracy: 26.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7819000: 0.000986\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7820000: 0.000473\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7821000: 0.000667\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7822000: 0.001101\n",
      "Training accuracy: -65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7823000: 0.000697\n",
      "Training accuracy: -54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7824000: 0.001042\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7825000: 0.000639\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7826000: 0.000733\n",
      "Training accuracy: 109.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7827000: 0.001670\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7828000: 0.001073\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7829000: 0.001223\n",
      "Training accuracy: 38.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7830000: 0.001112\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7831000: 0.000882\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7832000: 0.000819\n",
      "Training accuracy: -139.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7833000: 0.001229\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7834000: 0.000751\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7835000: 0.000689\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7836000: 0.001083\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7837000: 0.000707\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7838000: 0.000684\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7839000: 0.000835\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7840000: 0.000640\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7841000: 0.000941\n",
      "Training accuracy: 88.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7842000: 0.000826\n",
      "Training accuracy: 4.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7843000: 0.001181\n",
      "Training accuracy: -56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7844000: 0.000658\n",
      "Training accuracy: 105.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7845000: 0.000571\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7846000: 0.000801\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7847000: 0.000899\n",
      "Training accuracy: 55.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7848000: 0.000754\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7849000: 0.000578\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7850000: 0.000664\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7851000: 0.001056\n",
      "Training accuracy: -76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7852000: 0.000716\n",
      "Training accuracy: 45.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7853000: 0.001348\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7854000: 0.000507\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7855000: 0.000747\n",
      "Training accuracy: 120.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7856000: 0.001830\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7857000: 0.001090\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7858000: 0.001273\n",
      "Training accuracy: 15.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7859000: 0.000852\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7860000: 0.000870\n",
      "Training accuracy: 90.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7861000: 0.001132\n",
      "Training accuracy: -7.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7862000: 0.001218\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7863000: 0.000684\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7864000: 0.000670\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7865000: 0.000994\n",
      "Training accuracy: 146.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7866000: 0.000735\n",
      "Training accuracy: 54.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7867000: 0.000732\n",
      "Training accuracy: 35.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7868000: 0.000519\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7869000: 0.000704\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7870000: 0.000904\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7871000: 0.000930\n",
      "Training accuracy: -3.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7872000: 0.001158\n",
      "Training accuracy: -59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7873000: 0.000742\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7874000: 0.001224\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7875000: 0.000807\n",
      "Training accuracy: 70.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7876000: 0.000728\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7877000: 0.000764\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7878000: 0.000581\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7879000: 0.000835\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7880000: 0.000723\n",
      "Training accuracy: -53.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7881000: 0.000668\n",
      "Training accuracy: 58.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7882000: 0.001323\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7883000: 0.000544\n",
      "Training accuracy: 57.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7884000: 0.000870\n",
      "Training accuracy: 117.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7885000: 0.001996\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7886000: 0.000797\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7887000: 0.001200\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7888000: 0.000926\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7889000: 0.000665\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7890000: 0.001165\n",
      "Training accuracy: 49.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7891000: 0.001173\n",
      "Training accuracy: 103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7892000: 0.000829\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7893000: 0.000638\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7894000: 0.001035\n",
      "Training accuracy: 184.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7895000: 0.000687\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7896000: 0.001129\n",
      "Training accuracy: 15.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7897000: 0.000641\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7898000: 0.000457\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7899000: 0.000927\n",
      "Training accuracy: 128.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7900000: 0.000905\n",
      "Training accuracy: 53.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7901000: 0.000857\n",
      "Training accuracy: -59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7902000: 0.000722\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7903000: 0.001259\n",
      "Training accuracy: 61.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7904000: 0.000815\n",
      "Training accuracy: 67.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7905000: 0.000741\n",
      "Training accuracy: 55.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7906000: 0.000853\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7907000: 0.000607\n",
      "Training accuracy: 128.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7908000: 0.000838\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7909000: 0.000756\n",
      "Training accuracy: -187.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7910000: 0.000633\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7911000: 0.001291\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7912000: 0.000884\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7913000: 0.001072\n",
      "Training accuracy: 95.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7914000: 0.002065\n",
      "Training accuracy: 16.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7915000: 0.001146\n",
      "Training accuracy: 2.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7916000: 0.001212\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7917000: 0.000741\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7918000: 0.000666\n",
      "Training accuracy: 80.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7919000: 0.001334\n",
      "Training accuracy: -56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7920000: 0.000955\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7921000: 0.000800\n",
      "Training accuracy: 86.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7922000: 0.000645\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7923000: 0.001166\n",
      "Training accuracy: 147.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7924000: 0.001206\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7925000: 0.001260\n",
      "Training accuracy: 10.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7926000: 0.000612\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7927000: 0.000479\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7928000: 0.000673\n",
      "Training accuracy: 122.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7929000: 0.000990\n",
      "Training accuracy: -88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7930000: 0.000585\n",
      "Training accuracy: -41.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7931000: 0.000804\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7932000: 0.001360\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7933000: 0.000697\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7934000: 0.000744\n",
      "Training accuracy: 50.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7935000: 0.001096\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7936000: 0.000950\n",
      "Training accuracy: 123.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7937000: 0.000863\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7938000: 0.000726\n",
      "Training accuracy: -210.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7939000: 0.000692\n",
      "Training accuracy: 36.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7940000: 0.001086\n",
      "Training accuracy: 39.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7941000: 0.000872\n",
      "Training accuracy: 57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7942000: 0.001171\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7943000: 0.001805\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7944000: 0.001016\n",
      "Training accuracy: 28.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7945000: 0.001486\n",
      "Training accuracy: 45.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7946000: 0.000832\n",
      "Training accuracy: 103.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7947000: 0.000678\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7948000: 0.001236\n",
      "Training accuracy: -45.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7949000: 0.000607\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7950000: 0.000677\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7951000: 0.000489\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7952000: 0.000843\n",
      "Training accuracy: 156.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7953000: 0.001244\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7954000: 0.001159\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7955000: 0.000600\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7956000: 0.000542\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7957000: 0.001030\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7958000: 0.001059\n",
      "Training accuracy: -102.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7959000: 0.000745\n",
      "Training accuracy: 96.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7960000: 0.000734\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7961000: 0.001315\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7962000: 0.000734\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7963000: 0.000976\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7964000: 0.001035\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7965000: 0.000945\n",
      "Training accuracy: 145.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7966000: 0.001025\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7967000: 0.000735\n",
      "Training accuracy: -148.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7968000: 0.000703\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7969000: 0.001071\n",
      "Training accuracy: 42.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7970000: 0.000866\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7971000: 0.001109\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7972000: 0.001132\n",
      "Training accuracy: 36.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7973000: 0.001012\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7974000: 0.001156\n",
      "Training accuracy: 47.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7975000: 0.000945\n",
      "Training accuracy: 112.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7976000: 0.000690\n",
      "Training accuracy: 79.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7977000: 0.001360\n",
      "Training accuracy: -103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7978000: 0.000917\n",
      "Training accuracy: 110.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7979000: 0.000676\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7980000: 0.000499\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7981000: 0.000834\n",
      "Training accuracy: 157.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7982000: 0.001293\n",
      "Training accuracy: 59.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7983000: 0.001153\n",
      "Training accuracy: 30.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7984000: 0.000578\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7985000: 0.000564\n",
      "Training accuracy: 105.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7986000: 0.000944\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7987000: 0.001052\n",
      "Training accuracy: -68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7988000: 0.000747\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7989000: 0.000699\n",
      "Training accuracy: 59.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7990000: 0.001387\n",
      "Training accuracy: 71.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7991000: 0.000616\n",
      "Training accuracy: 90.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7992000: 0.001066\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7993000: 0.001043\n",
      "Training accuracy: 69.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7994000: 0.001121\n",
      "Training accuracy: 116.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7995000: 0.001099\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7996000: 0.000700\n",
      "Training accuracy: -130.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7997000: 0.000940\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7998000: 0.000769\n",
      "Training accuracy: 22.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 7999000: 0.000894\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8000000: 0.001055\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8001000: 0.000897\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8002000: 0.000998\n",
      "Training accuracy: 32.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8003000: 0.000805\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8004000: 0.000868\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8005000: 0.001040\n",
      "Training accuracy: 74.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8006000: 0.000998\n",
      "Training accuracy: -96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8007000: 0.001174\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8008000: 0.000680\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8009000: 0.000501\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8010000: 0.000864\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8011000: 0.001402\n",
      "Training accuracy: 47.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8012000: 0.001159\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8013000: 0.000545\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8014000: 0.000526\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8015000: 0.000981\n",
      "Training accuracy: 116.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8016000: 0.000794\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8017000: 0.001044\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8018000: 0.000619\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8019000: 0.000737\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8020000: 0.000890\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8021000: 0.001122\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8022000: 0.001038\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8023000: 0.001125\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8024000: 0.000895\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8025000: 0.000795\n",
      "Training accuracy: -136.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8026000: 0.001254\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8027000: 0.000709\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8028000: 0.000986\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8029000: 0.001226\n",
      "Training accuracy: 102.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8030000: 0.000794\n",
      "Training accuracy: 32.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8031000: 0.001101\n",
      "Training accuracy: 27.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8032000: 0.000819\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8033000: 0.000763\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8034000: 0.000977\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8035000: 0.000831\n",
      "Training accuracy: -70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8036000: 0.001263\n",
      "Training accuracy: -66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8037000: 0.000636\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8038000: 0.000562\n",
      "Training accuracy: 82.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8039000: 0.000818\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8040000: 0.001357\n",
      "Training accuracy: 26.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8041000: 0.000893\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8042000: 0.000482\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8043000: 0.000669\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8044000: 0.001079\n",
      "Training accuracy: -72.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8045000: 0.000738\n",
      "Training accuracy: -79.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8046000: 0.001052\n",
      "Training accuracy: 84.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8047000: 0.000634\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8048000: 0.000744\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8049000: 0.001781\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8050000: 0.001092\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8051000: 0.001515\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8052000: 0.001114\n",
      "Training accuracy: 77.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8053000: 0.000876\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8054000: 0.000774\n",
      "Training accuracy: -2.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8055000: 0.001262\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8056000: 0.000757\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8057000: 0.000688\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8058000: 0.000979\n",
      "Training accuracy: 109.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8059000: 0.000670\n",
      "Training accuracy: 63.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8060000: 0.000705\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8061000: 0.000845\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8062000: 0.000641\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8063000: 0.000948\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8064000: 0.000864\n",
      "Training accuracy: 5.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8065000: 0.001174\n",
      "Training accuracy: -56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8066000: 0.000675\n",
      "Training accuracy: 104.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8067000: 0.000558\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8068000: 0.000804\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8069000: 0.000895\n",
      "Training accuracy: 55.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8070000: 0.000746\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8071000: 0.000584\n",
      "Training accuracy: 89.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8072000: 0.000704\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8073000: 0.001063\n",
      "Training accuracy: -77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8074000: 0.000727\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8075000: 0.001347\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8076000: 0.000502\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8077000: 0.000854\n",
      "Training accuracy: 115.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8078000: 0.001844\n",
      "Training accuracy: 81.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8079000: 0.001088\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8080000: 0.001275\n",
      "Training accuracy: 15.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8081000: 0.000883\n",
      "Training accuracy: 115.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8082000: 0.000847\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8083000: 0.001123\n",
      "Training accuracy: -6.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8084000: 0.001217\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8085000: 0.000691\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8086000: 0.000667\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8087000: 0.000973\n",
      "Training accuracy: 147.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8088000: 0.000746\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8089000: 0.001029\n",
      "Training accuracy: 32.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8090000: 0.000523\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8091000: 0.000590\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8092000: 0.000894\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8093000: 0.000932\n",
      "Training accuracy: -3.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8094000: 0.001175\n",
      "Training accuracy: -60.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8095000: 0.000738\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8096000: 0.001237\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8097000: 0.000801\n",
      "Training accuracy: 68.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8098000: 0.000735\n",
      "Training accuracy: 71.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8099000: 0.000756\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8100000: 0.000591\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8101000: 0.000824\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8102000: 0.000720\n",
      "Training accuracy: -53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8103000: 0.000687\n",
      "Training accuracy: 65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8104000: 0.001323\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8105000: 0.000545\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8106000: 0.000881\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8107000: 0.001984\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8108000: 0.000781\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8109000: 0.001208\n",
      "Training accuracy: 30.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8110000: 0.000930\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8111000: 0.000672\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8112000: 0.001198\n",
      "Training accuracy: 27.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8113000: 0.001165\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8114000: 0.000832\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8115000: 0.000646\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8116000: 0.001051\n",
      "Training accuracy: 183.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8117000: 0.000673\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8118000: 0.001129\n",
      "Training accuracy: 16.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8119000: 0.000636\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8120000: 0.000460\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8121000: 0.000929\n",
      "Training accuracy: 127.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8122000: 0.000916\n",
      "Training accuracy: 53.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8123000: 0.000863\n",
      "Training accuracy: -60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8124000: 0.000815\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8125000: 0.001307\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8126000: 0.000821\n",
      "Training accuracy: 67.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8127000: 0.000743\n",
      "Training accuracy: 54.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8128000: 0.000851\n",
      "Training accuracy: 81.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8129000: 0.000624\n",
      "Training accuracy: 127.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8130000: 0.000847\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8131000: 0.000769\n",
      "Training accuracy: -188.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8132000: 0.000638\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8133000: 0.001363\n",
      "Training accuracy: 37.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8134000: 0.000869\n",
      "Training accuracy: 56.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8135000: 0.001066\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8136000: 0.002078\n",
      "Training accuracy: 16.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8137000: 0.001149\n",
      "Training accuracy: 2.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8138000: 0.001200\n",
      "Training accuracy: 25.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8139000: 0.000742\n",
      "Training accuracy: 108.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8140000: 0.000668\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8141000: 0.001338\n",
      "Training accuracy: -56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8142000: 0.000947\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8143000: 0.000806\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8144000: 0.000631\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8145000: 0.001158\n",
      "Training accuracy: 149.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8146000: 0.001200\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8147000: 0.001251\n",
      "Training accuracy: 11.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8148000: 0.000613\n",
      "Training accuracy: 83.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8149000: 0.000484\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8150000: 0.000672\n",
      "Training accuracy: 123.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8151000: 0.000979\n",
      "Training accuracy: -92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8152000: 0.000694\n",
      "Training accuracy: -47.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8153000: 0.000814\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8154000: 0.001359\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8155000: 0.000705\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8156000: 0.000744\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8157000: 0.001108\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8158000: 0.000959\n",
      "Training accuracy: 123.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8159000: 0.000869\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8160000: 0.000725\n",
      "Training accuracy: -210.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8161000: 0.000701\n",
      "Training accuracy: 35.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8162000: 0.001098\n",
      "Training accuracy: 42.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8163000: 0.000870\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8164000: 0.001168\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8165000: 0.001806\n",
      "Training accuracy: 27.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8166000: 0.001003\n",
      "Training accuracy: 28.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8167000: 0.001491\n",
      "Training accuracy: 44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8168000: 0.000931\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8169000: 0.000680\n",
      "Training accuracy: 80.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8170000: 0.001304\n",
      "Training accuracy: -71.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8171000: 0.000634\n",
      "Training accuracy: 112.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8172000: 0.000673\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8173000: 0.000493\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8174000: 0.000846\n",
      "Training accuracy: 157.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8175000: 0.001247\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8176000: 0.001159\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8177000: 0.000602\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8178000: 0.000540\n",
      "Training accuracy: 107.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8179000: 0.000995\n",
      "Training accuracy: 114.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8180000: 0.001085\n",
      "Training accuracy: -94.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8181000: 0.000742\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8182000: 0.000736\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8183000: 0.001321\n",
      "Training accuracy: 57.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8184000: 0.000768\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8185000: 0.000984\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8186000: 0.001018\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8187000: 0.000934\n",
      "Training accuracy: 146.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8188000: 0.001002\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8189000: 0.000759\n",
      "Training accuracy: -143.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8190000: 0.000656\n",
      "Training accuracy: 58.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8191000: 0.001110\n",
      "Training accuracy: 18.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8192000: 0.000873\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8193000: 0.001091\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8194000: 0.001005\n",
      "Training accuracy: 43.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8195000: 0.000995\n",
      "Training accuracy: 34.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8196000: 0.000873\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8197000: 0.000953\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8198000: 0.001017\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8199000: 0.001360\n",
      "Training accuracy: -103.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8200000: 0.000882\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8201000: 0.000671\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8202000: 0.000489\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8203000: 0.000834\n",
      "Training accuracy: 157.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8204000: 0.001385\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8205000: 0.001219\n",
      "Training accuracy: 23.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8206000: 0.000568\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8207000: 0.000571\n",
      "Training accuracy: 105.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8208000: 0.000945\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8209000: 0.000885\n",
      "Training accuracy: -63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8210000: 0.000742\n",
      "Training accuracy: 95.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8211000: 0.000699\n",
      "Training accuracy: 60.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8212000: 0.001394\n",
      "Training accuracy: 70.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8213000: 0.000600\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8214000: 0.001061\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8215000: 0.001041\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8216000: 0.001116\n",
      "Training accuracy: 116.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8217000: 0.001064\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8218000: 0.000693\n",
      "Training accuracy: -129.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8219000: 0.000933\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8220000: 0.000767\n",
      "Training accuracy: 22.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8221000: 0.000897\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8222000: 0.000947\n",
      "Training accuracy: 104.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8223000: 0.000882\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8224000: 0.000997\n",
      "Training accuracy: 32.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8225000: 0.000808\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8226000: 0.000830\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8227000: 0.001054\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8228000: 0.000998\n",
      "Training accuracy: -96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8229000: 0.001176\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8230000: 0.000671\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8231000: 0.000504\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8232000: 0.000865\n",
      "Training accuracy: 120.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8233000: 0.001389\n",
      "Training accuracy: 41.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8234000: 0.000849\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8235000: 0.000547\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8236000: 0.000525\n",
      "Training accuracy: 94.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8237000: 0.001028\n",
      "Training accuracy: -19.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8238000: 0.000792\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8239000: 0.001032\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8240000: 0.000606\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8241000: 0.000734\n",
      "Training accuracy: 51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8242000: 0.000904\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8243000: 0.001116\n",
      "Training accuracy: 47.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8244000: 0.001358\n",
      "Training accuracy: 34.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8245000: 0.001126\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8246000: 0.000893\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8247000: 0.000797\n",
      "Training accuracy: -136.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8248000: 0.001243\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8249000: 0.000723\n",
      "Training accuracy: 36.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8250000: 0.001004\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8251000: 0.001210\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8252000: 0.000786\n",
      "Training accuracy: 33.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8253000: 0.001104\n",
      "Training accuracy: 27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8254000: 0.000812\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8255000: 0.000756\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8256000: 0.000969\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8257000: 0.000786\n",
      "Training accuracy: -47.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8258000: 0.001273\n",
      "Training accuracy: -66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8259000: 0.000643\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8260000: 0.000568\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8261000: 0.000801\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8262000: 0.001362\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8263000: 0.000891\n",
      "Training accuracy: 60.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8264000: 0.000479\n",
      "Training accuracy: 61.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8265000: 0.000665\n",
      "Training accuracy: 92.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8266000: 0.001134\n",
      "Training accuracy: -89.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8267000: 0.000734\n",
      "Training accuracy: -79.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8268000: 0.001374\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8269000: 0.000528\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8270000: 0.000681\n",
      "Training accuracy: 127.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8271000: 0.001887\n",
      "Training accuracy: 62.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8272000: 0.001086\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8273000: 0.001524\n",
      "Training accuracy: 33.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8274000: 0.001106\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8275000: 0.000872\n",
      "Training accuracy: 89.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8276000: 0.000769\n",
      "Training accuracy: -2.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8277000: 0.001266\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8278000: 0.000689\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8279000: 0.000690\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8280000: 0.000964\n",
      "Training accuracy: 110.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8281000: 0.000654\n",
      "Training accuracy: 64.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8282000: 0.000706\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8283000: 0.000845\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8284000: 0.000645\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8285000: 0.000938\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8286000: 0.000950\n",
      "Training accuracy: -0.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8287000: 0.001172\n",
      "Training accuracy: -56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8288000: 0.000684\n",
      "Training accuracy: 108.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8289000: 0.000554\n",
      "Training accuracy: 76.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8290000: 0.000815\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8291000: 0.000889\n",
      "Training accuracy: 54.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8292000: 0.000750\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8293000: 0.000593\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8294000: 0.000712\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8295000: 0.001060\n",
      "Training accuracy: -76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8296000: 0.000727\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8297000: 0.001337\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8298000: 0.000495\n",
      "Training accuracy: 72.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8299000: 0.000846\n",
      "Training accuracy: 118.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8300000: 0.001840\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8301000: 0.001084\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8302000: 0.001264\n",
      "Training accuracy: 15.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8303000: 0.000926\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8304000: 0.000845\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8305000: 0.001180\n",
      "Training accuracy: -24.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8306000: 0.001209\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8307000: 0.000690\n",
      "Training accuracy: 72.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8308000: 0.000670\n",
      "Training accuracy: 62.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8309000: 0.000984\n",
      "Training accuracy: 146.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8310000: 0.000746\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8311000: 0.001027\n",
      "Training accuracy: 32.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8312000: 0.000524\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8313000: 0.000490\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8314000: 0.000890\n",
      "Training accuracy: 86.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8315000: 0.000865\n",
      "Training accuracy: 24.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8316000: 0.001155\n",
      "Training accuracy: -71.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8317000: 0.000737\n",
      "Training accuracy: 85.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8318000: 0.001234\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8319000: 0.000816\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8320000: 0.000720\n",
      "Training accuracy: 72.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8321000: 0.000756\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8322000: 0.000602\n",
      "Training accuracy: 128.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8323000: 0.000826\n",
      "Training accuracy: 64.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8324000: 0.000717\n",
      "Training accuracy: -53.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8325000: 0.000665\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8326000: 0.001331\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8327000: 0.000543\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8328000: 0.000871\n",
      "Training accuracy: 117.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8329000: 0.001952\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8330000: 0.000772\n",
      "Training accuracy: 74.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8331000: 0.001213\n",
      "Training accuracy: 29.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8332000: 0.000941\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8333000: 0.000673\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8334000: 0.001185\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8335000: 0.001171\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8336000: 0.000783\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8337000: 0.000631\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8338000: 0.001056\n",
      "Training accuracy: 183.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8339000: 0.000791\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8340000: 0.001136\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8341000: 0.000631\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8342000: 0.000446\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8343000: 0.000601\n",
      "Training accuracy: 132.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8344000: 0.000935\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8345000: 0.000869\n",
      "Training accuracy: -60.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8346000: 0.000823\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8347000: 0.001300\n",
      "Training accuracy: 44.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8348000: 0.000829\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8349000: 0.000757\n",
      "Training accuracy: 54.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8350000: 0.000766\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8351000: 0.000962\n",
      "Training accuracy: 124.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8352000: 0.000847\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8353000: 0.000776\n",
      "Training accuracy: -188.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8354000: 0.000638\n",
      "Training accuracy: 50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8355000: 0.001370\n",
      "Training accuracy: 37.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8356000: 0.000860\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8357000: 0.001063\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8358000: 0.002102\n",
      "Training accuracy: 22.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8359000: 0.001137\n",
      "Training accuracy: 3.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8360000: 0.001518\n",
      "Training accuracy: 22.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8361000: 0.000740\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8362000: 0.000668\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8363000: 0.001341\n",
      "Training accuracy: -56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8364000: 0.000940\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8365000: 0.000792\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8366000: 0.000646\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8367000: 0.001153\n",
      "Training accuracy: 149.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8368000: 0.001204\n",
      "Training accuracy: 67.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8369000: 0.001249\n",
      "Training accuracy: 11.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8370000: 0.000609\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8371000: 0.000509\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8372000: 0.000655\n",
      "Training accuracy: 123.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8373000: 0.001083\n",
      "Training accuracy: -101.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8374000: 0.000693\n",
      "Training accuracy: -47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8375000: 0.000817\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8376000: 0.001357\n",
      "Training accuracy: 45.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8377000: 0.000709\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8378000: 0.000753\n",
      "Training accuracy: 49.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8379000: 0.001175\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8380000: 0.000951\n",
      "Training accuracy: 124.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8381000: 0.000866\n",
      "Training accuracy: 69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8382000: 0.000687\n",
      "Training accuracy: -74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8383000: 0.000708\n",
      "Training accuracy: 35.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8384000: 0.001089\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8385000: 0.000891\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8386000: 0.001149\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8387000: 0.001792\n",
      "Training accuracy: 29.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8388000: 0.001000\n",
      "Training accuracy: 29.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8389000: 0.001169\n",
      "Training accuracy: 48.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8390000: 0.000926\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8391000: 0.000684\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8392000: 0.001346\n",
      "Training accuracy: -86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8393000: 0.000627\n",
      "Training accuracy: 113.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8394000: 0.000675\n",
      "Training accuracy: 82.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8395000: 0.000473\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8396000: 0.000852\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8397000: 0.001255\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8398000: 0.001161\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8399000: 0.000617\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8400000: 0.000538\n",
      "Training accuracy: 107.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8401000: 0.001003\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8402000: 0.001089\n",
      "Training accuracy: -92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8403000: 0.000749\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8404000: 0.000729\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8405000: 0.001320\n",
      "Training accuracy: 61.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8406000: 0.000772\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8407000: 0.000978\n",
      "Training accuracy: 51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8408000: 0.001029\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8409000: 0.000938\n",
      "Training accuracy: 146.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8410000: 0.001001\n",
      "Training accuracy: 65.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8411000: 0.000708\n",
      "Training accuracy: -126.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8412000: 0.000658\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8413000: 0.000773\n",
      "Training accuracy: 22.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8414000: 0.000878\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8415000: 0.001109\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8416000: 0.000890\n",
      "Training accuracy: 49.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8417000: 0.001007\n",
      "Training accuracy: 33.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8418000: 0.000879\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8419000: 0.000944\n",
      "Training accuracy: 112.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8420000: 0.001010\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8421000: 0.001356\n",
      "Training accuracy: -102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8422000: 0.000868\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8423000: 0.000672\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8424000: 0.000492\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8425000: 0.000854\n",
      "Training accuracy: 156.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8426000: 0.001393\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8427000: 0.001216\n",
      "Training accuracy: 24.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8428000: 0.000562\n",
      "Training accuracy: 75.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8429000: 0.000578\n",
      "Training accuracy: 104.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8430000: 0.000944\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8431000: 0.000790\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8432000: 0.000739\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8433000: 0.000683\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8434000: 0.001409\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8435000: 0.000599\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8436000: 0.001077\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8437000: 0.001047\n",
      "Training accuracy: 69.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8438000: 0.001112\n",
      "Training accuracy: 116.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8439000: 0.001047\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8440000: 0.000708\n",
      "Training accuracy: -130.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8441000: 0.000935\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8442000: 0.000687\n",
      "Training accuracy: 30.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8443000: 0.000990\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8444000: 0.000942\n",
      "Training accuracy: 103.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8445000: 0.000876\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8446000: 0.000998\n",
      "Training accuracy: 31.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8447000: 0.000823\n",
      "Training accuracy: 73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8448000: 0.000785\n",
      "Training accuracy: 102.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8449000: 0.001048\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8450000: 0.000952\n",
      "Training accuracy: -78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8451000: 0.001179\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8452000: 0.000668\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8453000: 0.000507\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8454000: 0.000855\n",
      "Training accuracy: 120.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8455000: 0.001382\n",
      "Training accuracy: 41.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8456000: 0.000860\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8457000: 0.000548\n",
      "Training accuracy: 78.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8458000: 0.000534\n",
      "Training accuracy: 93.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8459000: 0.001028\n",
      "Training accuracy: -20.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8460000: 0.000788\n",
      "Training accuracy: -59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8461000: 0.001029\n",
      "Training accuracy: 92.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8462000: 0.000664\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8463000: 0.000736\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8464000: 0.000894\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8465000: 0.001126\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8466000: 0.001366\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8467000: 0.001095\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8468000: 0.000897\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8469000: 0.000796\n",
      "Training accuracy: -136.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8470000: 0.001233\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8471000: 0.000728\n",
      "Training accuracy: 36.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8472000: 0.001010\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8473000: 0.001226\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8474000: 0.000776\n",
      "Training accuracy: 33.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8475000: 0.001120\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8476000: 0.000828\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8477000: 0.000746\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8478000: 0.000978\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8479000: 0.000788\n",
      "Training accuracy: -51.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8480000: 0.001276\n",
      "Training accuracy: -63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8481000: 0.000643\n",
      "Training accuracy: 99.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8482000: 0.000578\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8483000: 0.000805\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8484000: 0.001278\n",
      "Training accuracy: 44.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8485000: 0.000883\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8486000: 0.000475\n",
      "Training accuracy: 63.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8487000: 0.000666\n",
      "Training accuracy: 92.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8488000: 0.001145\n",
      "Training accuracy: -91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8489000: 0.000713\n",
      "Training accuracy: -88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8490000: 0.001369\n",
      "Training accuracy: 82.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8491000: 0.000520\n",
      "Training accuracy: 66.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8492000: 0.000691\n",
      "Training accuracy: 128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8493000: 0.001871\n",
      "Training accuracy: 63.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8494000: 0.001078\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8495000: 0.001531\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8496000: 0.000768\n",
      "Training accuracy: 81.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8497000: 0.000861\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8498000: 0.001100\n",
      "Training accuracy: -5.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8499000: 0.001267\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8500000: 0.000692\n",
      "Training accuracy: 76.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8501000: 0.000684\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8502000: 0.000996\n",
      "Training accuracy: 147.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8503000: 0.000671\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8504000: 0.000753\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8505000: 0.000524\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8506000: 0.000648\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8507000: 0.000929\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8508000: 0.000960\n",
      "Training accuracy: -0.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8509000: 0.001188\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8510000: 0.000692\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8511000: 0.000545\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8512000: 0.000809\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8513000: 0.000885\n",
      "Training accuracy: 55.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8514000: 0.000756\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8515000: 0.000597\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8516000: 0.000846\n",
      "Training accuracy: 58.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8517000: 0.001077\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8518000: 0.000630\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8519000: 0.001338\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8520000: 0.000488\n",
      "Training accuracy: 73.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8521000: 0.000856\n",
      "Training accuracy: 117.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8522000: 0.001840\n",
      "Training accuracy: 81.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8523000: 0.001080\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8524000: 0.001197\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8525000: 0.000926\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8526000: 0.000842\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8527000: 0.001182\n",
      "Training accuracy: -25.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8528000: 0.001209\n",
      "Training accuracy: 101.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8529000: 0.000811\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8530000: 0.000664\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8531000: 0.000984\n",
      "Training accuracy: 145.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8532000: 0.000747\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8533000: 0.001022\n",
      "Training accuracy: 33.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8534000: 0.000532\n",
      "Training accuracy: 87.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8535000: 0.000483\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8536000: 0.000895\n",
      "Training accuracy: 86.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8537000: 0.000829\n",
      "Training accuracy: 39.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8538000: 0.001170\n",
      "Training accuracy: -65.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8539000: 0.000723\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8540000: 0.001234\n",
      "Training accuracy: 91.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8541000: 0.000808\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8542000: 0.000721\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8543000: 0.000765\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8544000: 0.000589\n",
      "Training accuracy: 128.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8545000: 0.000830\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8546000: 0.000718\n",
      "Training accuracy: -51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8547000: 0.000658\n",
      "Training accuracy: 58.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8548000: 0.001311\n",
      "Training accuracy: 68.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8549000: 0.000543\n",
      "Training accuracy: 53.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8550000: 0.000861\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8551000: 0.001999\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8552000: 0.000767\n",
      "Training accuracy: 74.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8553000: 0.001210\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8554000: 0.000943\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8555000: 0.000669\n",
      "Training accuracy: 82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8556000: 0.001173\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8557000: 0.001165\n",
      "Training accuracy: 103.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8558000: 0.000803\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8559000: 0.000630\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8560000: 0.001048\n",
      "Training accuracy: 175.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8561000: 0.000807\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8562000: 0.001134\n",
      "Training accuracy: 16.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8563000: 0.000621\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8564000: 0.000446\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8565000: 0.000602\n",
      "Training accuracy: 130.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8566000: 0.000939\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8567000: 0.000895\n",
      "Training accuracy: -43.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8568000: 0.000819\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8569000: 0.001306\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8570000: 0.000856\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8571000: 0.000749\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8572000: 0.000772\n",
      "Training accuracy: 88.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8573000: 0.000961\n",
      "Training accuracy: 124.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8574000: 0.000843\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8575000: 0.000768\n",
      "Training accuracy: -187.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8576000: 0.000648\n",
      "Training accuracy: 50.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8577000: 0.001386\n",
      "Training accuracy: 37.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8578000: 0.000865\n",
      "Training accuracy: 57.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8579000: 0.001048\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8580000: 0.002089\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8581000: 0.001124\n",
      "Training accuracy: 4.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8582000: 0.001525\n",
      "Training accuracy: 21.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8583000: 0.000734\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8584000: 0.000680\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8585000: 0.001323\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8586000: 0.000944\n",
      "Training accuracy: 97.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8587000: 0.000778\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8588000: 0.000552\n",
      "Training accuracy: 71.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8589000: 0.001154\n",
      "Training accuracy: 148.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8590000: 0.001202\n",
      "Training accuracy: 66.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8591000: 0.001260\n",
      "Training accuracy: 15.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8592000: 0.000599\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8593000: 0.000515\n",
      "Training accuracy: 102.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8594000: 0.000664\n",
      "Training accuracy: 122.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8595000: 0.001075\n",
      "Training accuracy: -101.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8596000: 0.000698\n",
      "Training accuracy: -47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8597000: 0.000816\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8598000: 0.001357\n",
      "Training accuracy: 45.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8599000: 0.000710\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8600000: 0.000640\n",
      "Training accuracy: 55.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8601000: 0.001176\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8602000: 0.000951\n",
      "Training accuracy: 124.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8603000: 0.000857\n",
      "Training accuracy: 70.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8604000: 0.000792\n",
      "Training accuracy: -82.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8605000: 0.000708\n",
      "Training accuracy: 35.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8606000: 0.001091\n",
      "Training accuracy: 42.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8607000: 0.000836\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8608000: 0.001155\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8609000: 0.001887\n",
      "Training accuracy: 22.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8610000: 0.000992\n",
      "Training accuracy: 29.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8611000: 0.001173\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8612000: 0.000938\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8613000: 0.000674\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8614000: 0.001384\n",
      "Training accuracy: -108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8615000: 0.000631\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8616000: 0.000660\n",
      "Training accuracy: 83.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8617000: 0.000489\n",
      "Training accuracy: 99.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8618000: 0.000854\n",
      "Training accuracy: 157.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8619000: 0.001308\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8620000: 0.001198\n",
      "Training accuracy: 7.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8621000: 0.000600\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8622000: 0.000541\n",
      "Training accuracy: 107.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8623000: 0.000995\n",
      "Training accuracy: 114.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8624000: 0.001102\n",
      "Training accuracy: -84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8625000: 0.000752\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8626000: 0.000727\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8627000: 0.001334\n",
      "Training accuracy: 66.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8628000: 0.000769\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8629000: 0.000979\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8630000: 0.001030\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8631000: 0.000938\n",
      "Training accuracy: 146.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8632000: 0.001003\n",
      "Training accuracy: 65.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8633000: 0.000695\n",
      "Training accuracy: -126.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8634000: 0.000671\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8635000: 0.000773\n",
      "Training accuracy: 22.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8636000: 0.000878\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8637000: 0.001110\n",
      "Training accuracy: 65.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8638000: 0.000895\n",
      "Training accuracy: 49.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8639000: 0.001002\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8640000: 0.000873\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8641000: 0.000947\n",
      "Training accuracy: 111.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8642000: 0.001026\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8643000: 0.001012\n",
      "Training accuracy: -99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8644000: 0.000865\n",
      "Training accuracy: 88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8645000: 0.000667\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8646000: 0.000493\n",
      "Training accuracy: 99.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8647000: 0.000842\n",
      "Training accuracy: 122.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8648000: 0.001462\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8649000: 0.001168\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8650000: 0.000571\n",
      "Training accuracy: 74.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8651000: 0.000584\n",
      "Training accuracy: 104.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8652000: 0.000939\n",
      "Training accuracy: 95.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8653000: 0.000786\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8654000: 0.000730\n",
      "Training accuracy: 96.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8655000: 0.000682\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8656000: 0.001412\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8657000: 0.000595\n",
      "Training accuracy: 91.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8658000: 0.001078\n",
      "Training accuracy: 46.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8659000: 0.001042\n",
      "Training accuracy: 67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8660000: 0.001108\n",
      "Training accuracy: 117.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8661000: 0.000897\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8662000: 0.000692\n",
      "Training accuracy: -129.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8663000: 0.000940\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8664000: 0.000711\n",
      "Training accuracy: 37.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8665000: 0.001049\n",
      "Training accuracy: 51.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8666000: 0.000926\n",
      "Training accuracy: 103.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8667000: 0.000940\n",
      "Training accuracy: 27.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8668000: 0.001122\n",
      "Training accuracy: 24.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8669000: 0.000828\n",
      "Training accuracy: 73.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8670000: 0.000799\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8671000: 0.001057\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8672000: 0.000947\n",
      "Training accuracy: -78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8673000: 0.001182\n",
      "Training accuracy: 85.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8674000: 0.000562\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8675000: 0.000498\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8676000: 0.000865\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8677000: 0.001385\n",
      "Training accuracy: 42.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8678000: 0.000876\n",
      "Training accuracy: 49.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8679000: 0.000545\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8680000: 0.000558\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8681000: 0.001123\n",
      "Training accuracy: -27.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8682000: 0.000790\n",
      "Training accuracy: -59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8683000: 0.001014\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8684000: 0.000663\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8685000: 0.000738\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8686000: 0.001232\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8687000: 0.001118\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8688000: 0.001370\n",
      "Training accuracy: 33.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8689000: 0.001109\n",
      "Training accuracy: 77.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8690000: 0.000906\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8691000: 0.000797\n",
      "Training accuracy: -138.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8692000: 0.001233\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8693000: 0.000734\n",
      "Training accuracy: 36.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8694000: 0.001004\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8695000: 0.001234\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8696000: 0.000741\n",
      "Training accuracy: 56.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8697000: 0.001119\n",
      "Training accuracy: 26.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8698000: 0.000822\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8699000: 0.000736\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8700000: 0.000984\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8701000: 0.000861\n",
      "Training accuracy: -70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8702000: 0.001272\n",
      "Training accuracy: -62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8703000: 0.000626\n",
      "Training accuracy: 99.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8704000: 0.000579\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8705000: 0.000812\n",
      "Training accuracy: 80.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8706000: 0.001283\n",
      "Training accuracy: 44.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8707000: 0.000877\n",
      "Training accuracy: 61.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8708000: 0.000527\n",
      "Training accuracy: 46.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8709000: 0.000675\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8710000: 0.001147\n",
      "Training accuracy: -91.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8711000: 0.000710\n",
      "Training accuracy: -88.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8712000: 0.001341\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8713000: 0.000516\n",
      "Training accuracy: 66.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8714000: 0.000689\n",
      "Training accuracy: 125.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8715000: 0.001834\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8716000: 0.001086\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8717000: 0.001524\n",
      "Training accuracy: 33.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8718000: 0.000780\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8719000: 0.000861\n",
      "Training accuracy: 90.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8720000: 0.001107\n",
      "Training accuracy: -6.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8721000: 0.001259\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8722000: 0.000677\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8723000: 0.000682\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8724000: 0.000996\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8725000: 0.000672\n",
      "Training accuracy: 70.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8726000: 0.000763\n",
      "Training accuracy: 55.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8727000: 0.000515\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8728000: 0.000657\n",
      "Training accuracy: 87.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8729000: 0.000920\n",
      "Training accuracy: 91.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8730000: 0.000960\n",
      "Training accuracy: -0.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8731000: 0.001187\n",
      "Training accuracy: -57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8732000: 0.000696\n",
      "Training accuracy: 108.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8733000: 0.000580\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8734000: 0.000811\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8735000: 0.000888\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8736000: 0.000761\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8737000: 0.000597\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8738000: 0.000840\n",
      "Training accuracy: 61.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8739000: 0.001084\n",
      "Training accuracy: -77.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8740000: 0.000632\n",
      "Training accuracy: 56.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8741000: 0.001401\n",
      "Training accuracy: 49.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8742000: 0.000496\n",
      "Training accuracy: 73.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8743000: 0.000857\n",
      "Training accuracy: 117.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8744000: 0.001991\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8745000: 0.001081\n",
      "Training accuracy: 72.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8746000: 0.001187\n",
      "Training accuracy: 31.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8747000: 0.000955\n",
      "Training accuracy: 114.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8748000: 0.000855\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8749000: 0.001079\n",
      "Training accuracy: -17.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8750000: 0.001207\n",
      "Training accuracy: 101.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8751000: 0.000817\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8752000: 0.000664\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8753000: 0.000979\n",
      "Training accuracy: 146.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8754000: 0.000683\n",
      "Training accuracy: 80.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8755000: 0.001019\n",
      "Training accuracy: 34.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8756000: 0.000627\n",
      "Training accuracy: 81.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8757000: 0.000479\n",
      "Training accuracy: 108.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8758000: 0.000902\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8759000: 0.000787\n",
      "Training accuracy: 62.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8760000: 0.001166\n",
      "Training accuracy: -64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8761000: 0.000727\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8762000: 0.001212\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8763000: 0.000792\n",
      "Training accuracy: 68.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8764000: 0.000723\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8765000: 0.000712\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8766000: 0.000596\n",
      "Training accuracy: 128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8767000: 0.000845\n",
      "Training accuracy: 63.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8768000: 0.000724\n",
      "Training accuracy: -51.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8769000: 0.000651\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8770000: 0.001303\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8771000: 0.000553\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8772000: 0.000845\n",
      "Training accuracy: 110.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8773000: 0.001993\n",
      "Training accuracy: 27.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8774000: 0.000789\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8775000: 0.001212\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8776000: 0.000951\n",
      "Training accuracy: 113.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8777000: 0.000674\n",
      "Training accuracy: 82.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8778000: 0.001174\n",
      "Training accuracy: 25.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8779000: 0.001162\n",
      "Training accuracy: 107.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8780000: 0.000803\n",
      "Training accuracy: 86.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8781000: 0.000631\n",
      "Training accuracy: 59.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8782000: 0.001046\n",
      "Training accuracy: 175.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8783000: 0.000811\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8784000: 0.001137\n",
      "Training accuracy: 16.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8785000: 0.000623\n",
      "Training accuracy: 84.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8786000: 0.000444\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8787000: 0.000664\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8788000: 0.000938\n",
      "Training accuracy: 62.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8789000: 0.000903\n",
      "Training accuracy: -44.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8790000: 0.000822\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8791000: 0.001316\n",
      "Training accuracy: 47.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8792000: 0.000848\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8793000: 0.000732\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8794000: 0.000785\n",
      "Training accuracy: 87.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8795000: 0.000950\n",
      "Training accuracy: 125.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8796000: 0.000835\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8797000: 0.000770\n",
      "Training accuracy: -186.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8798000: 0.000640\n",
      "Training accuracy: 51.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8799000: 0.001385\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8800000: 0.000872\n",
      "Training accuracy: 57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8801000: 0.001154\n",
      "Training accuracy: 94.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8802000: 0.002105\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8803000: 0.001123\n",
      "Training accuracy: 5.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8804000: 0.001538\n",
      "Training accuracy: 21.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8805000: 0.000727\n",
      "Training accuracy: 109.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8806000: 0.000698\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8807000: 0.001322\n",
      "Training accuracy: -55.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8808000: 0.000936\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8809000: 0.000761\n",
      "Training accuracy: 79.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8810000: 0.000497\n",
      "Training accuracy: 87.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8811000: 0.001158\n",
      "Training accuracy: 148.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8812000: 0.001133\n",
      "Training accuracy: 85.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8813000: 0.001132\n",
      "Training accuracy: 22.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8814000: 0.000600\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8815000: 0.000509\n",
      "Training accuracy: 102.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8816000: 0.000665\n",
      "Training accuracy: 119.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8817000: 0.001070\n",
      "Training accuracy: -100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8818000: 0.000690\n",
      "Training accuracy: -47.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8819000: 0.000807\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8820000: 0.001350\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8821000: 0.000707\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8822000: 0.000634\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8823000: 0.001171\n",
      "Training accuracy: 68.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8824000: 0.000958\n",
      "Training accuracy: 120.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8825000: 0.000839\n",
      "Training accuracy: 63.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8826000: 0.000743\n",
      "Training accuracy: -150.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8827000: 0.000704\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8828000: 0.001089\n",
      "Training accuracy: 43.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8829000: 0.000838\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8830000: 0.001154\n",
      "Training accuracy: 94.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8831000: 0.001555\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8832000: 0.001013\n",
      "Training accuracy: 36.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8833000: 0.001162\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8834000: 0.000921\n",
      "Training accuracy: 97.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8835000: 0.000666\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8836000: 0.001382\n",
      "Training accuracy: -108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8837000: 0.000631\n",
      "Training accuracy: 112.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8838000: 0.000680\n",
      "Training accuracy: 91.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8839000: 0.000487\n",
      "Training accuracy: 99.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8840000: 0.000846\n",
      "Training accuracy: 157.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8841000: 0.001295\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8842000: 0.001207\n",
      "Training accuracy: 6.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8843000: 0.000600\n",
      "Training accuracy: 73.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8844000: 0.000547\n",
      "Training accuracy: 107.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8845000: 0.000986\n",
      "Training accuracy: 114.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8846000: 0.001038\n",
      "Training accuracy: -65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8847000: 0.000764\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8848000: 0.000726\n",
      "Training accuracy: 66.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8849000: 0.001345\n",
      "Training accuracy: 66.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8850000: 0.000752\n",
      "Training accuracy: 81.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8851000: 0.000976\n",
      "Training accuracy: 51.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8852000: 0.001030\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8853000: 0.000881\n",
      "Training accuracy: 163.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8854000: 0.001094\n",
      "Training accuracy: 59.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8855000: 0.000687\n",
      "Training accuracy: -125.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8856000: 0.000667\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8857000: 0.000790\n",
      "Training accuracy: 22.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8858000: 0.000882\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8859000: 0.001118\n",
      "Training accuracy: 65.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8860000: 0.000890\n",
      "Training accuracy: 49.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8861000: 0.000995\n",
      "Training accuracy: 33.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8862000: 0.000884\n",
      "Training accuracy: 49.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8863000: 0.000936\n",
      "Training accuracy: 113.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8864000: 0.001029\n",
      "Training accuracy: 75.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8865000: 0.001005\n",
      "Training accuracy: -98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8866000: 0.000860\n",
      "Training accuracy: 89.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8867000: 0.000680\n",
      "Training accuracy: 78.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8868000: 0.000490\n",
      "Training accuracy: 99.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8869000: 0.000848\n",
      "Training accuracy: 122.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8870000: 0.001470\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8871000: 0.001166\n",
      "Training accuracy: 47.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8872000: 0.000560\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8873000: 0.000575\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8874000: 0.000966\n",
      "Training accuracy: 103.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8875000: 0.000788\n",
      "Training accuracy: -57.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8876000: 0.000729\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8877000: 0.000680\n",
      "Training accuracy: 56.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8878000: 0.001383\n",
      "Training accuracy: 52.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8879000: 0.000912\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8880000: 0.001079\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8881000: 0.001029\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8882000: 0.001120\n",
      "Training accuracy: 114.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8883000: 0.000912\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8884000: 0.000688\n",
      "Training accuracy: -129.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8885000: 0.000947\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8886000: 0.000755\n",
      "Training accuracy: 48.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8887000: 0.001037\n",
      "Training accuracy: 52.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8888000: 0.001266\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8889000: 0.000799\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8890000: 0.001129\n",
      "Training accuracy: 22.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8891000: 0.000824\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8892000: 0.000775\n",
      "Training accuracy: 78.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8893000: 0.001045\n",
      "Training accuracy: 78.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8894000: 0.000945\n",
      "Training accuracy: -77.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8895000: 0.001197\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8896000: 0.000594\n",
      "Training accuracy: 62.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8897000: 0.000497\n",
      "Training accuracy: 101.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8898000: 0.000864\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8899000: 0.001350\n",
      "Training accuracy: 28.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8900000: 0.000879\n",
      "Training accuracy: 49.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8901000: 0.000483\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8902000: 0.000555\n",
      "Training accuracy: 100.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8903000: 0.001127\n",
      "Training accuracy: -27.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8904000: 0.000804\n",
      "Training accuracy: -60.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8905000: 0.001014\n",
      "Training accuracy: 86.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8906000: 0.000666\n",
      "Training accuracy: 63.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8907000: 0.000768\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8908000: 0.001243\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8909000: 0.001129\n",
      "Training accuracy: 43.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8910000: 0.001371\n",
      "Training accuracy: 33.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8911000: 0.001112\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8912000: 0.000893\n",
      "Training accuracy: 86.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8913000: 0.000802\n",
      "Training accuracy: -138.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8914000: 0.001218\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8915000: 0.000730\n",
      "Training accuracy: 36.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8916000: 0.000997\n",
      "Training accuracy: 64.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8917000: 0.001239\n",
      "Training accuracy: 100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8918000: 0.000759\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8919000: 0.001113\n",
      "Training accuracy: 20.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8920000: 0.000828\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8921000: 0.000725\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8922000: 0.000976\n",
      "Training accuracy: 65.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8923000: 0.000869\n",
      "Training accuracy: -71.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8924000: 0.001282\n",
      "Training accuracy: -67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8925000: 0.000635\n",
      "Training accuracy: 98.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8926000: 0.000584\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8927000: 0.000804\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8928000: 0.001274\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8929000: 0.000877\n",
      "Training accuracy: 61.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8930000: 0.000537\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8931000: 0.000673\n",
      "Training accuracy: 91.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8932000: 0.001093\n",
      "Training accuracy: -78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8933000: 0.000712\n",
      "Training accuracy: -88.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8934000: 0.001345\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8935000: 0.000517\n",
      "Training accuracy: 66.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8936000: 0.000687\n",
      "Training accuracy: 125.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8937000: 0.001823\n",
      "Training accuracy: 79.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8938000: 0.001090\n",
      "Training accuracy: 70.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8939000: 0.001558\n",
      "Training accuracy: 13.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8940000: 0.000821\n",
      "Training accuracy: 94.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8941000: 0.000858\n",
      "Training accuracy: 90.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8942000: 0.001109\n",
      "Training accuracy: -5.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8943000: 0.001260\n",
      "Training accuracy: 86.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8944000: 0.000691\n",
      "Training accuracy: 76.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8945000: 0.000681\n",
      "Training accuracy: 62.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8946000: 0.000888\n",
      "Training accuracy: 151.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8947000: 0.000673\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8948000: 0.000781\n",
      "Training accuracy: 54.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8949000: 0.000515\n",
      "Training accuracy: 88.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8950000: 0.000676\n",
      "Training accuracy: 91.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8951000: 0.000900\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8952000: 0.000964\n",
      "Training accuracy: -0.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8953000: 0.001190\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8954000: 0.000703\n",
      "Training accuracy: 108.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8955000: 0.000590\n",
      "Training accuracy: 92.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8956000: 0.000819\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8957000: 0.000759\n",
      "Training accuracy: 75.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8958000: 0.000778\n",
      "Training accuracy: 63.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8959000: 0.000591\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8960000: 0.000834\n",
      "Training accuracy: 61.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8961000: 0.001079\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8962000: 0.000644\n",
      "Training accuracy: 56.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8963000: 0.001414\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8964000: 0.000537\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8965000: 0.000859\n",
      "Training accuracy: 117.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8966000: 0.001989\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8967000: 0.001094\n",
      "Training accuracy: 70.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8968000: 0.001186\n",
      "Training accuracy: 31.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 8969000: 0.000946\n",
      "Training accuracy: 114.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8970000: 0.000869\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8971000: 0.001035\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8972000: 0.001198\n",
      "Training accuracy: 101.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8973000: 0.000820\n",
      "Training accuracy: 64.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8974000: 0.000662\n",
      "Training accuracy: 63.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8975000: 0.001017\n",
      "Training accuracy: 186.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8976000: 0.000678\n",
      "Training accuracy: 82.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8977000: 0.001002\n",
      "Training accuracy: 25.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8978000: 0.000636\n",
      "Training accuracy: 81.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8979000: 0.000478\n",
      "Training accuracy: 108.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8980000: 0.000897\n",
      "Training accuracy: 87.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8981000: 0.000891\n",
      "Training accuracy: 56.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8982000: 0.001166\n",
      "Training accuracy: -64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8983000: 0.000717\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8984000: 0.001218\n",
      "Training accuracy: 83.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8985000: 0.000800\n",
      "Training accuracy: 67.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8986000: 0.000760\n",
      "Training accuracy: 54.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8987000: 0.000702\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8988000: 0.000596\n",
      "Training accuracy: 128.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8989000: 0.000841\n",
      "Training accuracy: 64.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8990000: 0.000723\n",
      "Training accuracy: -51.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8991000: 0.000650\n",
      "Training accuracy: 50.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8992000: 0.001293\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8993000: 0.000867\n",
      "Training accuracy: 52.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8994000: 0.000996\n",
      "Training accuracy: 105.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8995000: 0.001995\n",
      "Training accuracy: 26.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8996000: 0.000843\n",
      "Training accuracy: 4.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8997000: 0.001213\n",
      "Training accuracy: 25.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8998000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 8999000: 0.000583\n",
      "Training accuracy: 87.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9000000: 0.001174\n",
      "Training accuracy: 25.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9001000: 0.001170\n",
      "Training accuracy: 107.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9002000: 0.000792\n",
      "Training accuracy: 87.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9003000: 0.000632\n",
      "Training accuracy: 59.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9004000: 0.001034\n",
      "Training accuracy: 175.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9005000: 0.000899\n",
      "Training accuracy: 69.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9006000: 0.001155\n",
      "Training accuracy: 15.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9007000: 0.000614\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9008000: 0.000442\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9009000: 0.000664\n",
      "Training accuracy: 123.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9010000: 0.000949\n",
      "Training accuracy: 62.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9011000: 0.000911\n",
      "Training accuracy: -45.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9012000: 0.000821\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9013000: 0.001333\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9014000: 0.000844\n",
      "Training accuracy: 46.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9015000: 0.000732\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9016000: 0.001092\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9017000: 0.000955\n",
      "Training accuracy: 124.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9018000: 0.000837\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9019000: 0.000753\n",
      "Training accuracy: -194.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9020000: 0.000638\n",
      "Training accuracy: 51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9021000: 0.001380\n",
      "Training accuracy: 37.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9022000: 0.000870\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9023000: 0.001141\n",
      "Training accuracy: 94.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9024000: 0.001787\n",
      "Training accuracy: 26.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9025000: 0.001127\n",
      "Training accuracy: 4.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9026000: 0.001534\n",
      "Training accuracy: 21.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9027000: 0.000718\n",
      "Training accuracy: 109.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9028000: 0.000674\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9029000: 0.001321\n",
      "Training accuracy: -51.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9030000: 0.000934\n",
      "Training accuracy: 92.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9031000: 0.000666\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9032000: 0.000499\n",
      "Training accuracy: 87.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9033000: 0.000829\n",
      "Training accuracy: 151.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9034000: 0.001136\n",
      "Training accuracy: 85.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9035000: 0.001120\n",
      "Training accuracy: 23.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9036000: 0.000605\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9037000: 0.000514\n",
      "Training accuracy: 102.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9038000: 0.000983\n",
      "Training accuracy: 115.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9039000: 0.001079\n",
      "Training accuracy: -101.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9040000: 0.000793\n",
      "Training accuracy: -53.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9041000: 0.000782\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9042000: 0.001359\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9043000: 0.000704\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9044000: 0.000633\n",
      "Training accuracy: 57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9045000: 0.001173\n",
      "Training accuracy: 68.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9046000: 0.000926\n",
      "Training accuracy: 146.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9047000: 0.001133\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9048000: 0.000730\n",
      "Training accuracy: -149.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9049000: 0.000703\n",
      "Training accuracy: 35.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9050000: 0.001090\n",
      "Training accuracy: 43.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9051000: 0.000857\n",
      "Training accuracy: 80.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9052000: 0.001131\n",
      "Training accuracy: 57.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9053000: 0.001565\n",
      "Training accuracy: 25.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9054000: 0.001006\n",
      "Training accuracy: 36.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9055000: 0.001165\n",
      "Training accuracy: 47.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9056000: 0.000932\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9057000: 0.000674\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9058000: 0.001381\n",
      "Training accuracy: -104.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9059000: 0.000641\n",
      "Training accuracy: 112.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9060000: 0.000690\n",
      "Training accuracy: 90.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9061000: 0.000490\n",
      "Training accuracy: 98.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9062000: 0.000833\n",
      "Training accuracy: 158.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9063000: 0.001290\n",
      "Training accuracy: 59.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9064000: 0.001190\n",
      "Training accuracy: 7.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9065000: 0.000595\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9066000: 0.000560\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9067000: 0.000996\n",
      "Training accuracy: 113.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9068000: 0.001035\n",
      "Training accuracy: -65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9069000: 0.000757\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9070000: 0.000717\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9071000: 0.001353\n",
      "Training accuracy: 65.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9072000: 0.000755\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9073000: 0.001068\n",
      "Training accuracy: 46.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9074000: 0.001034\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9075000: 0.000873\n",
      "Training accuracy: 164.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9076000: 0.001093\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9077000: 0.000672\n",
      "Training accuracy: -128.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9078000: 0.000982\n",
      "Training accuracy: 53.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9079000: 0.000779\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9080000: 0.000885\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9081000: 0.001109\n",
      "Training accuracy: 66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9082000: 0.000902\n",
      "Training accuracy: 48.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9083000: 0.001003\n",
      "Training accuracy: 32.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9084000: 0.000837\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9085000: 0.000895\n",
      "Training accuracy: 100.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9086000: 0.001041\n",
      "Training accuracy: 74.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9087000: 0.001007\n",
      "Training accuracy: -100.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9088000: 0.000867\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9089000: 0.000670\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9090000: 0.000487\n",
      "Training accuracy: 99.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9091000: 0.000967\n",
      "Training accuracy: 115.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9092000: 0.001469\n",
      "Training accuracy: 30.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9093000: 0.001149\n",
      "Training accuracy: 49.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9094000: 0.000545\n",
      "Training accuracy: 76.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9095000: 0.000561\n",
      "Training accuracy: 97.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9096000: 0.000983\n",
      "Training accuracy: 118.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9097000: 0.000783\n",
      "Training accuracy: -57.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9098000: 0.000722\n",
      "Training accuracy: 96.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9099000: 0.000673\n",
      "Training accuracy: 56.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9100000: 0.001372\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9101000: 0.000902\n",
      "Training accuracy: 88.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9102000: 0.001076\n",
      "Training accuracy: 47.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9103000: 0.001056\n",
      "Training accuracy: 46.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9104000: 0.001120\n",
      "Training accuracy: 115.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9105000: 0.000912\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9106000: 0.000701\n",
      "Training accuracy: -130.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9107000: 0.000953\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9108000: 0.000748\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9109000: 0.001012\n",
      "Training accuracy: 74.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9110000: 0.001266\n",
      "Training accuracy: 100.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9111000: 0.000810\n",
      "Training accuracy: 31.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9112000: 0.001119\n",
      "Training accuracy: 25.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9113000: 0.000815\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9114000: 0.000767\n",
      "Training accuracy: 79.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9115000: 0.001026\n",
      "Training accuracy: 79.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9116000: 0.000939\n",
      "Training accuracy: -77.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9117000: 0.001200\n",
      "Training accuracy: 84.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9118000: 0.000623\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9119000: 0.000508\n",
      "Training accuracy: 99.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9120000: 0.000826\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9121000: 0.001364\n",
      "Training accuracy: 26.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9122000: 0.000979\n",
      "Training accuracy: 42.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9123000: 0.000476\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9124000: 0.000674\n",
      "Training accuracy: 94.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9125000: 0.001128\n",
      "Training accuracy: -28.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9126000: 0.000713\n",
      "Training accuracy: -55.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9127000: 0.001026\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9128000: 0.000649\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9129000: 0.000763\n",
      "Training accuracy: 89.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9130000: 0.001248\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9131000: 0.001082\n",
      "Training accuracy: 67.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9132000: 0.001387\n",
      "Training accuracy: 32.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9133000: 0.001113\n",
      "Training accuracy: 77.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9134000: 0.000895\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9135000: 0.000819\n",
      "Training accuracy: -139.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9136000: 0.001222\n",
      "Training accuracy: 64.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9137000: 0.000729\n",
      "Training accuracy: 36.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9138000: 0.000694\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9139000: 0.001072\n",
      "Training accuracy: 105.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9140000: 0.000756\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9141000: 0.001069\n",
      "Training accuracy: 95.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9142000: 0.000824\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9143000: 0.000651\n",
      "Training accuracy: 84.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9144000: 0.000987\n",
      "Training accuracy: 65.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9145000: 0.000873\n",
      "Training accuracy: -69.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9146000: 0.001274\n",
      "Training accuracy: -66.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9147000: 0.000649\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9148000: 0.000589\n",
      "Training accuracy: 79.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9149000: 0.000801\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9150000: 0.001185\n",
      "Training accuracy: 52.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9151000: 0.000860\n",
      "Training accuracy: 61.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9152000: 0.000572\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9153000: 0.000686\n",
      "Training accuracy: 90.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9154000: 0.001083\n",
      "Training accuracy: -77.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9155000: 0.000717\n",
      "Training accuracy: -88.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9156000: 0.001349\n",
      "Training accuracy: 64.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9157000: 0.000502\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9158000: 0.000679\n",
      "Training accuracy: 125.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9159000: 0.001820\n",
      "Training accuracy: 80.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9160000: 0.001101\n",
      "Training accuracy: 69.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9161000: 0.001251\n",
      "Training accuracy: 16.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9162000: 0.000835\n",
      "Training accuracy: 93.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9163000: 0.000857\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9164000: 0.001104\n",
      "Training accuracy: -5.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9165000: 0.001267\n",
      "Training accuracy: 86.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9166000: 0.000694\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9167000: 0.000684\n",
      "Training accuracy: 62.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9168000: 0.000887\n",
      "Training accuracy: 152.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9169000: 0.000687\n",
      "Training accuracy: 69.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9170000: 0.000776\n",
      "Training accuracy: 55.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9171000: 0.000518\n",
      "Training accuracy: 88.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9172000: 0.000684\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9173000: 0.000901\n",
      "Training accuracy: 82.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9174000: 0.000960\n",
      "Training accuracy: -4.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9175000: 0.001178\n",
      "Training accuracy: -57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9176000: 0.000692\n",
      "Training accuracy: 108.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9177000: 0.000902\n",
      "Training accuracy: 88.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9178000: 0.000816\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9179000: 0.000724\n",
      "Training accuracy: 75.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9180000: 0.000807\n",
      "Training accuracy: 70.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9181000: 0.000589\n",
      "Training accuracy: 90.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9182000: 0.000829\n",
      "Training accuracy: 61.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9183000: 0.000760\n",
      "Training accuracy: -73.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9184000: 0.000638\n",
      "Training accuracy: 56.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9185000: 0.001296\n",
      "Training accuracy: 55.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9186000: 0.000524\n",
      "Training accuracy: 53.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9187000: 0.000862\n",
      "Training accuracy: 117.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9188000: 0.001990\n",
      "Training accuracy: 76.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9189000: 0.001116\n",
      "Training accuracy: 69.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9190000: 0.001192\n",
      "Training accuracy: 31.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9191000: 0.000932\n",
      "Training accuracy: 112.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9192000: 0.000670\n",
      "Training accuracy: 81.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9193000: 0.001147\n",
      "Training accuracy: 51.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9194000: 0.001184\n",
      "Training accuracy: 102.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9195000: 0.000824\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9196000: 0.000641\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9197000: 0.001021\n",
      "Training accuracy: 186.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9198000: 0.000670\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9199000: 0.001015\n",
      "Training accuracy: 25.0%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9200000: 0.000640\n",
      "Training accuracy: 83.7%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9201000: 0.000459\n",
      "Training accuracy: 95.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9202000: 0.000915\n",
      "Training accuracy: 120.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9203000: 0.000895\n",
      "Training accuracy: 52.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9204000: 0.001153\n",
      "Training accuracy: -63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9205000: 0.000708\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9206000: 0.001261\n",
      "Training accuracy: 62.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9207000: 0.000803\n",
      "Training accuracy: 67.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9208000: 0.000749\n",
      "Training accuracy: 54.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9209000: 0.000711\n",
      "Training accuracy: 84.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9210000: 0.000606\n",
      "Training accuracy: 128.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9211000: 0.000830\n",
      "Training accuracy: 64.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9212000: 0.000713\n",
      "Training accuracy: -50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9213000: 0.000650\n",
      "Training accuracy: 50.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9214000: 0.001291\n",
      "Training accuracy: 64.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9215000: 0.000873\n",
      "Training accuracy: 52.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9216000: 0.000986\n",
      "Training accuracy: 106.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9217000: 0.002007\n",
      "Training accuracy: 26.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9218000: 0.001050\n",
      "Training accuracy: 6.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9219000: 0.001209\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9220000: 0.001032\n",
      "Training accuracy: 105.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9221000: 0.000583\n",
      "Training accuracy: 87.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9222000: 0.001166\n",
      "Training accuracy: 23.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9223000: 0.000958\n",
      "Training accuracy: 102.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9224000: 0.000792\n",
      "Training accuracy: 86.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9225000: 0.000635\n",
      "Training accuracy: 63.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9226000: 0.001035\n",
      "Training accuracy: 174.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9227000: 0.001178\n",
      "Training accuracy: 67.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9228000: 0.001264\n",
      "Training accuracy: 10.6%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9229000: 0.000630\n",
      "Training accuracy: 83.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9230000: 0.000466\n",
      "Training accuracy: 96.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9231000: 0.000659\n",
      "Training accuracy: 124.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9232000: 0.000943\n",
      "Training accuracy: 63.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9233000: 0.000902\n",
      "Training accuracy: -44.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9234000: 0.000814\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9235000: 0.001337\n",
      "Training accuracy: 46.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9236000: 0.000721\n",
      "Training accuracy: 53.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9237000: 0.000748\n",
      "Training accuracy: 56.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9238000: 0.001110\n",
      "Training accuracy: 83.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9239000: 0.000956\n",
      "Training accuracy: 124.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9240000: 0.000838\n",
      "Training accuracy: 64.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9241000: 0.000726\n",
      "Training accuracy: -209.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9242000: 0.000648\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9243000: 0.001401\n",
      "Training accuracy: 36.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9244000: 0.000879\n",
      "Training accuracy: 57.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9245000: 0.001148\n",
      "Training accuracy: 94.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9246000: 0.001789\n",
      "Training accuracy: 26.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9247000: 0.001165\n",
      "Training accuracy: 19.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9248000: 0.001488\n",
      "Training accuracy: 44.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9249000: 0.000719\n",
      "Training accuracy: 108.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9250000: 0.000674\n",
      "Training accuracy: 84.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9251000: 0.001312\n",
      "Training accuracy: -50.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9252000: 0.000925\n",
      "Training accuracy: 96.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9253000: 0.000657\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9254000: 0.000477\n",
      "Training accuracy: 88.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9255000: 0.000828\n",
      "Training accuracy: 151.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9256000: 0.001142\n",
      "Training accuracy: 85.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9257000: 0.001128\n",
      "Training accuracy: 20.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9258000: 0.000615\n",
      "Training accuracy: 83.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9259000: 0.000517\n",
      "Training accuracy: 102.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9260000: 0.001027\n",
      "Training accuracy: 95.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9261000: 0.001073\n",
      "Training accuracy: -101.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9262000: 0.000802\n",
      "Training accuracy: -53.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9263000: 0.000759\n",
      "Training accuracy: 60.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9264000: 0.001360\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9265000: 0.000720\n",
      "Training accuracy: 56.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9266000: 0.000635\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9267000: 0.001075\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9268000: 0.000933\n",
      "Training accuracy: 146.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9269000: 0.001019\n",
      "Training accuracy: 65.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9270000: 0.000730\n",
      "Training accuracy: -149.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9271000: 0.000684\n",
      "Training accuracy: 36.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9272000: 0.001083\n",
      "Training accuracy: 43.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9273000: 0.000868\n",
      "Training accuracy: 80.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9274000: 0.001130\n",
      "Training accuracy: 57.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9275000: 0.001551\n",
      "Training accuracy: 25.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9276000: 0.001018\n",
      "Training accuracy: 35.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9277000: 0.001149\n",
      "Training accuracy: 48.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9278000: 0.000934\n",
      "Training accuracy: 112.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9279000: 0.000670\n",
      "Training accuracy: 80.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9280000: 0.001368\n",
      "Training accuracy: -103.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9281000: 0.000926\n",
      "Training accuracy: 109.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9282000: 0.000697\n",
      "Training accuracy: 90.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9283000: 0.000489\n",
      "Training accuracy: 98.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9284000: 0.000835\n",
      "Training accuracy: 158.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9285000: 0.001295\n",
      "Training accuracy: 59.4%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9286000: 0.001185\n",
      "Training accuracy: 7.9%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9287000: 0.000595\n",
      "Training accuracy: 73.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9288000: 0.000551\n",
      "Training accuracy: 106.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9289000: 0.000986\n",
      "Training accuracy: 113.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9290000: 0.001031\n",
      "Training accuracy: -67.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9291000: 0.000758\n",
      "Training accuracy: 96.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9292000: 0.000699\n",
      "Training accuracy: 67.1%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9293000: 0.001350\n",
      "Training accuracy: 65.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9294000: 0.000768\n",
      "Training accuracy: 81.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9295000: 0.001064\n",
      "Training accuracy: 46.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9296000: 0.001036\n",
      "Training accuracy: 70.1%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9297000: 0.001132\n",
      "Training accuracy: 120.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9298000: 0.001092\n",
      "Training accuracy: 59.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9299000: 0.000692\n",
      "Training accuracy: -129.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9300000: 0.000982\n",
      "Training accuracy: 58.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9301000: 0.000768\n",
      "Training accuracy: 24.7%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9302000: 0.000886\n",
      "Training accuracy: 79.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9303000: 0.001098\n",
      "Training accuracy: 67.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9304000: 0.000903\n",
      "Training accuracy: 48.5%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9305000: 0.000995\n",
      "Training accuracy: 32.2%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9306000: 0.000839\n",
      "Training accuracy: 70.3%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9307000: 0.000879\n",
      "Training accuracy: 100.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9308000: 0.001052\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9309000: 0.001007\n",
      "Training accuracy: -100.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9310000: 0.000866\n",
      "Training accuracy: 88.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9311000: 0.000687\n",
      "Training accuracy: 77.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9312000: 0.000478\n",
      "Training accuracy: 100.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9313000: 0.000975\n",
      "Training accuracy: 113.9%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9314000: 0.001471\n",
      "Training accuracy: 30.8%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9315000: 0.001165\n",
      "Training accuracy: 47.8%\n",
      "Validation accuracy: 88.7%\n",
      "Loss at step 9316000: 0.000548\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9317000: 0.000547\n",
      "Training accuracy: 93.2%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9318000: 0.000984\n",
      "Training accuracy: 119.6%\n",
      "Validation accuracy: 88.6%\n",
      "Loss at step 9319000: 0.000791\n",
      "Training accuracy: -57.5%\n",
      "Validation accuracy: 88.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-9233762a82ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# arrays.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     _, l, predictions, valid_predictions = session.run([optimizer, loss, train_prediction, valid_prediction]\n\u001b[0;32m---> 24\u001b[0;31m                                         , feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 200001\n",
    "\n",
    "def accuracy(prediction, price):\n",
    "#  return np.mean(100 - 100*np.abs((prediction - price) / price))\n",
    "  return 100 * np.mean(prediction / price)  \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_info.shape[0] - batch_size)\n",
    "    batch_data = train_info[offset:(offset+batch_size),:]\n",
    "    batch_images = train_images[offset:(offset+batch_size),:]    \n",
    "    batch_labels = train_prices_labels[offset:(offset+batch_size),:]\n",
    "    feed_dict = {tf_train_info: batch_data,tf_train_price:batch_labels,tf_valid_info:valid_info,tf_train_images:batch_images, tf_valid_images:valid_images}\n",
    "    \n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions, valid_predictions = session.run([optimizer, loss, train_prediction, valid_prediction]\n",
    "                                        , feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 1000 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, batch_labels))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_predictions, valid_prices_labels))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFkCAYAAACThxm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e50e490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rsquared on train:  0.666061855861\n",
      "Rsquared on validation:  -1.28025892341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in range(batch_data.shape[0]):\n",
    "    x.append(batch_data[i][1])\n",
    "    y.append(predictions[i][0])\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "result_lr = regr.fit(train_info, train_prices_labels)\n",
    "\n",
    "prices_estimation = result_lr.predict(valid_info)\n",
    "\n",
    "#w,b = polyfit(x, train_prices_labels, 1)\n",
    "\n",
    "#print('weight', w)\n",
    "#print('bias', b)\n",
    "\n",
    "#pred_w = weight\n",
    "#pred_b = bias\n",
    "\n",
    "#pred_y = w*x + b\n",
    "\n",
    "plt.scatter(x, y,color=\"r\")\n",
    "\n",
    "#plt.plot(x, pred_y,color=\"g\")\n",
    "\n",
    "plt.scatter(x, batch_labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print 'Rsquared on train: ', result_lr.score(train_info, train_prices_labels)\n",
    "\n",
    "print 'Rsquared on validation: ', result_lr.score(valid_info, valid_prices_labels)\n",
    "\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', array([-0.02774357]))\n",
      "('bias', array([-0.35934017]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAFkCAYAAACThxm6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117312310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in range(valid_info.shape[0]):\n",
    "    x.append(valid_info[i][1])\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "result_lr = regr.fit(train_info, train_prices_labels)\n",
    "\n",
    "prices_estimation = result_lr.predict(valid_info)\n",
    "\n",
    "w,b = polyfit(x, valid_prices_labels, 1)\n",
    "\n",
    "print('weight', w)\n",
    "print('bias', b)\n",
    "\n",
    "#pred_w = weight\n",
    "#pred_b = bias\n",
    "\n",
    "#pred_y = w*x + b\n",
    "\n",
    "plt.scatter(x,valid_predictions,color=\"r\",alpha=0.5)\n",
    "\n",
    "#plt.plot(x, pred_y,color=\"g\")\n",
    "\n",
    "plt.scatter(x, valid_prices_labels,alpha=0.5)\n",
    "#plt.ylim(range(0,2))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
