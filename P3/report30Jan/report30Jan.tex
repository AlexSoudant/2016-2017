\documentclass[a4paper, 12pt, oneside]{report}
\raggedbottom
\usepackage{setspace}
\onehalfspacing
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\pdfminorversion=6
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[french]{babel}
\usepackage{wasysym}
\usepackage[a4paper,bindingoffset=0in,left=1in,right=1in,top=1in,bottom=1in,footskip=.25in]{geometry}
\usepackage[ansinew]{inputenc}
\usepackage{color}% colour for table cells
\usepackage{multirow}
\definecolor{light-gray}{gray}{0.6}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{sidecap}
\usepackage{caption}
    \captionsetup{font=small,labelfont=bf}
\usepackage{floatrow}
\usepackage{array,multirow}
\floatsetup[table]{capposition=top}
\parindent 0pt	

\usepackage{hyperref}

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Machine Learning For a House pricing Prediction Web Application}} % Article title

\author{
\large
\textsc{Alex Soudant}
\\[2mm] 
\normalsize Ynov Ingésup M1,\\
\normalsize 20 Boulevard Général de Gaulle, 44200 Nantes \\
\\[2mm] 
\normalsize Correspondence: Alex Soudant. E-mail: \href{mailto:alex.soudant@ynov.com}{alex.soudant@ynov.com} % Your email address
\vspace{-5mm}
}

\date{}

\vspace{1cm}

\begin{document}

\maketitle

\vspace*{5cm}
\textbf{report on the 30th of January 2017}
\newline
\newline
\noindent
\normalfont 
Our planned objectives for Week 1 were "Tutorial to scrapy - finding the websites to scrape".
\newline
\newline
In that purpose, I first followed the "first steps" and "basic concepts" sections of the scrapy website. When finished, I wanted to apply this knowledge to a target website containing information about real estates like prices, size and frontside pictures. Therefore, I looked for adequate websites that led to a selection of five real estate letting agencies:

\begin{enumerate}
\item Century21.fr;
\item pap.fr (particulier a particulier);
\item seloger.com;
\item paruvendu.fr;
\item explorimmo.com.
\end{enumerate}

\noindent
I selected these websites for their similarity in terms of webpage organisation which could permit to only tweak a little the scrapy code developed to scrape the information from these different websites.
\newline
However, by trying basic procedures on the seloger.com website I noticed that some of these compagnies have disallowed scrapy in the robot.txt file. After, looking how to adress this inconveniency, I found out that some options in the setup.py file of scrapy can change my signature as a user agent and remove the automatic reading of robots.txt when attempting to scrape a website. Still, this procedure can be seen as bad practice as I am intentionally trying to use a disallowed scraper. Another alternative could be to use a javascript scraper that will log to websites directly through the navigator and therefore is not referenced in the robots.txt file. I found out that PhantomJS could be used as such and could provide me the wanted scaping tools I need. However, after trying a few times the tutorial code to achieve data scrapping I failed to make it work properly. There could be some versioning problems with recent updates that prevent me from learning directly with web available pre-generated example code. 
I also had the time to take a glimpse to selenium$/$webdriverio which looks like a promising framework to scrape from website whith only one dependency to Node.js.
\newline
As I could still train our scraping skills with scrapy, we then used it on the pap.fr website that does not disallow scrapy in robots.txt. I found out that with only a few lines of code I could extract the wanted information from this website main page. However, I still have to implement navigation between pages and ultimatly to be able to visit individual housing sale page to obtain the full description of the associated real estate. Finally I have to find a way to save not only the reference link to the picture but the picture itself when scraping.  
\newline
To conclude, I feel that I explored quite a few directions to take for next week. I can advance on scrapy code if I still can put to work the phantomJS or webdriverio. I do not see any delays that could prevent me from starting the collection of data. My objective is therefore to collect information and pictures of at least a hundred to a thousand real estates which can then be used to run computer vision algorithms after data cleansing and image reformatting.




\end{document}
