{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "filename = \"text8.zip\"\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists advocate social\n",
      "['ons anarchi', 'y eight inc', 'o two one n', 'ard roberts', 'cilities in']\n",
      "['ists advoca', 'ch two zero', 'nine zero t', 's joseph wh', 'ncluding ku']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, size, unrolls):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = size\n",
    "        self._unrolls = unrolls\n",
    "        segment = self._text_size // size\n",
    "        self._cursor = [ offset * segment for offset in range(size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by unrolls new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._unrolls):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, 5, 10)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_text[:30])\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def make_one_hot(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_single_output():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "def construct_verbose(node_count, batch_size, unrolls):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        # Parameters:\n",
    "        # Input gate: input, previous output, and bias.\n",
    "        ix = tf.Variable(tf.truncated_normal([vocabulary_size, node_count], -0.1, 0.1))\n",
    "        im = tf.Variable(tf.truncated_normal([node_count, node_count], -0.1, 0.1))\n",
    "        ib = tf.Variable(tf.zeros([1, node_count]))\n",
    "        # Forget gate: input, previous output, and bias.\n",
    "        fx = tf.Variable(tf.truncated_normal([vocabulary_size, node_count], -0.1, 0.1))\n",
    "        fm = tf.Variable(tf.truncated_normal([node_count, node_count], -0.1, 0.1))\n",
    "        fb = tf.Variable(tf.zeros([1, node_count]))\n",
    "        # Memory cell: input, state and bias.                             \n",
    "        cx = tf.Variable(tf.truncated_normal([vocabulary_size, node_count], -0.1, 0.1))\n",
    "        cm = tf.Variable(tf.truncated_normal([node_count, node_count], -0.1, 0.1))\n",
    "        cb = tf.Variable(tf.zeros([1, node_count]))\n",
    "        # Output gate: input, previous output, and bias.\n",
    "        ox = tf.Variable(tf.truncated_normal([vocabulary_size, node_count], -0.1, 0.1))\n",
    "        om = tf.Variable(tf.truncated_normal([node_count, node_count], -0.1, 0.1))\n",
    "        ob = tf.Variable(tf.zeros([1, node_count]))\n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([node_count, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Definition of the cell computation.\n",
    "        def lstm_cell(i, o, state):\n",
    "            \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "            Note that in this formulation, we omit the various connections between the\n",
    "            previous state and the gates.\"\"\"\n",
    "            input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "            forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "            update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "            return output_gate * tf.tanh(state), state\n",
    "\n",
    "        # Input data.\n",
    "        train_data = list()\n",
    "        for _ in range(unrolls + 1):\n",
    "            train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "        train_inputs = train_data[:unrolls]\n",
    "        train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs:\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "            # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Sampling and validation eval: batch 1, no unrolling.\n",
    "        sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "        saved_sample_output = tf.Variable(tf.zeros([1, node_count]))\n",
    "        saved_sample_state = tf.Variable(tf.zeros([1, node_count]))\n",
    "        reset_sample_state = tf.group(\n",
    "            saved_sample_output.assign(tf.zeros([1, node_count])),\n",
    "            saved_sample_state.assign(tf.zeros([1, node_count]))\n",
    "        )\n",
    "        sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "        with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                      saved_sample_state.assign(sample_state)]):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    return {\n",
    "        \"graph\": graph,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"unrolls\": unrolls,\n",
    "        \"train_data\": train_data,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"loss\": loss,\n",
    "        \"train_prediction\": train_prediction,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"reset_sample_state\": reset_sample_state,\n",
    "        \"sample_input\": sample_input,\n",
    "        \"sample_prediction\": sample_prediction,\n",
    "        \"output_to_input\": make_one_hot,\n",
    "        \"random_output\": random_single_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_lstm(setup, batcher, training, validation, step_count, report_every):\n",
    "    train_batches = batcher(training, setup[\"batch_size\"], setup[\"unrolls\"])\n",
    "    valid_batches = batcher(validation, 1, 1)\n",
    "    with tf.Session(graph=setup[\"graph\"]) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "        mean_loss = 0\n",
    "        for step in range(step_count + 1):\n",
    "            batches = train_batches.next()\n",
    "            feed_dict = dict()\n",
    "            for i in range(setup[\"unrolls\"] + 1):\n",
    "                feed_dict[setup[\"train_data\"][i]] = batches[i]\n",
    "\n",
    "            _, l, predictions, lr = session.run(\n",
    "                [setup[\"optimizer\"], setup[\"loss\"], setup[\"train_prediction\"], setup[\"learning_rate\"]],\n",
    "                feed_dict=feed_dict\n",
    "            )\n",
    "\n",
    "            mean_loss += l\n",
    "            if step % report_every == 0:\n",
    "                if step > 0:\n",
    "                    mean_loss = mean_loss / report_every\n",
    "                # The mean loss is an estimate of the loss over the last few batches.\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "                mean_loss = 0\n",
    "                labels = np.concatenate(list(batches)[1:])\n",
    "                print(predictions.shape)\n",
    "                print(labels)\n",
    "                print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "                if step % (report_every * 10) == 0:\n",
    "                    # Generate some samples.\n",
    "                    print('=' * 80)\n",
    "                    for _ in range(5):\n",
    "                        feed = setup[\"output_to_input\"](setup[\"random_output\"]())\n",
    "                        sentence = characters(feed)[0]\n",
    "                        setup[\"reset_sample_state\"].run()\n",
    "                        for _ in range(79):\n",
    "                            prediction = setup[\"sample_prediction\"].eval({setup[\"sample_input\"]: feed})\n",
    "                            feed = setup[\"output_to_input\"](prediction)\n",
    "                            sentence += characters(feed)[0]\n",
    "                        print(sentence)\n",
    "                    print('=' * 80)\n",
    "                # Measure validation set perplexity.\n",
    "                setup[\"reset_sample_state\"].run()\n",
    "                valid_logprob = 0\n",
    "                for _ in range(len(validation)):\n",
    "                    b = valid_batches.next()\n",
    "                    predictions = setup[\"sample_prediction\"].eval({setup[\"sample_input\"]: b[0]})\n",
    "                    valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-93-b3492f8327e7>:6 in run_lstm.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.292521 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "lt rin potw fx gp ziiqtcynhs  kt n wirpt qbtrra efw tubl  l  t do izinap xkiiftb\n",
      "rlnasxblmbiycnjunegn nbjafdnimi  gb  zsbriirjwxodhe  mg il aiwy feciantfamneyul \n",
      "ygm   rsol cxiiazosx wirrgfsdwvdteesx tcr rrzfotadokpsjqtyp ayrxhqtdgikok g t so\n",
      " ali s lnroqrco cyedtpl  lipq oebozsee  hoenarhudtuisiccnbsresjg rvrytfdz bmpncw\n",
      "vymcyhnppdrcmdf tsljtowbsshiedeoj o iu  tgsyuopnu am nmvhy   sdy lbaxswee qabsih\n",
      "================================================================================\n",
      "Validation set perplexity: 19.89\n",
      "Average loss at step 100: 2.590023 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 10.39\n",
      "Validation set perplexity: 10.45\n",
      "Average loss at step 200: 2.261087 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.84\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 300: 2.109405 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 400: 2.004585 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 500: 1.942525 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.913769 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700: 1.862056 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.820950 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 900: 1.827686 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 1000: 1.827541 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "ben teciled di that ib kakic doasar hob sctitura ic been facthan to res paa wan \n",
      "y one nine ressew tile bnectapln sekm nif sto parleting depelied in kerden from \n",
      "pat beul funine ermaivita and engreaar boy basting oruare a poluf c one arguated\n",
      "uls reputer chulin of the verio ause for roprous bolitar unatibn the benciclo th\n",
      "quative the clited of linisading termunks hay fiatived belocinion rem d parfy pa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1100: 1.775850 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1200: 1.750263 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.730483 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1400: 1.745171 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.741098 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1600: 1.742660 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.714665 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1800: 1.675317 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.647639 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.694337 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "jong is althis memoney tegnt marraty it is imnoty diac despointing oversiby nor \n",
      "chaed unsirinn excered as a pade of tript such tap for wifolated obally orings o\n",
      "le carrorkes by heom in in onty one to vodes knowh refb in experesselegbal whici\n",
      "on works one nine eight four three faper on the relourthares afring oh mauces re\n",
      " stall with notin one zero four zero e sometines hovebsakies get in wand s one n\n",
      "================================================================================\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2100: 1.691422 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2200: 1.678086 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.644535 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2400: 1.659979 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2500: 1.676692 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2600: 1.651892 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2700: 1.663922 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2800: 1.652823 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2900: 1.650826 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3000: 1.650375 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "jeal to logetrizacly powers tuch is resign fory agueje worlow also as frendeists\n",
      "jades the prigently and pari as te alo duri an post accreaser fremon the bein ka\n",
      "g corsono but national is cristual bales worlt and differents achalos farth this\n",
      "tory stronder spbarl he werght one seven seven show lead how coidencts are may e\n",
      "ble in thrie sometom other moderdary with il are all compportef eqame water the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3100: 1.627860 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200: 1.645061 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3300: 1.631307 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3400: 1.669710 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3500: 1.658478 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.671321 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.642072 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3800: 1.643843 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900: 1.638842 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4000: 1.651702 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "ce fumments as modenva a deecting two zero two five zero zero one two ove notes \n",
      "ai and gruma familian centrorma natoass lows of can might his peecent langra in \n",
      "vericabur dayli quanth hall counced in game unstem of orle of temptes and and pn\n",
      "f with zero feet appearations attidraugh of one nine five zero zero zero zero ze\n",
      "ther redepentic homeala wamer central it ckine us of indiance and gording grocar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.629593 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.634527 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4300: 1.613059 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.611788 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.616722 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.616934 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4700: 1.624415 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4800: 1.633333 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4900: 1.631549 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5000: 1.610246 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "ex as elects true cypond ovingry gathina war of hishinese foreet not was not jan\n",
      "s earliesn to botin accorded fromen his despice restrifion constituted one tery \n",
      "quessionate this be had towar frenaire maups one zrumer ba grantes blacks he gam\n",
      "ited by the diach for three nine eight eight smilificil was worve to his have b \n",
      "chndarger the pact throe he undendio it serwan spenotions for hands very ockwary\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.598428 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.588341 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5300: 1.574184 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.580973 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.564167 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.577335 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5700: 1.568247 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.575402 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.572427 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.546528 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "ty at the laccl iveander ow n bargar renet yoandon as that of poduchtion distain\n",
      "s in propose in the become vicopispure don that is badiogs demoditical turnal ga\n",
      "waiting bearch ungleat actres as goboc nown limin nine for the gaopsern many hea\n",
      "tic elect made fursonomm he langencish of two six the working ounjoy this espari\n",
      "xies blark a zamas national toic tspess as usiss m hakens the leving of the lay \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.558539 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.537730 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.544260 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.541679 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.555995 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6600: 1.595881 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6700: 1.580193 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.605736 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.579713 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.572761 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "zer out by jonda nucalto timal v a spirht one nine two one two five two recurrat\n",
      "winck resolves are c guals westima deften row use yoe for the fincly massans and\n",
      "s in it for councrach from one zero one nitting that pelatics fert list the fock\n",
      "y neglaal recuard b enermen in englorism of the chriside for his eegrum and elec\n",
      "and one kish far intial provery action the herest sounddlwuss of replact of a v \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "verbose_setup = construct_verbose(node_count=64, batch_size=64, unrolls=10)\n",
    "\n",
    "run_lstm(verbose_setup, BatchGenerator, train_text, valid_text, step_count=7000, report_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_compact(node_count, batch_size, unrolls):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        gate_count = 4\n",
    "        # Parameters:\n",
    "        # Gates: input, previous output, and bias.\n",
    "        input_weights = tf.Variable(tf.truncated_normal([vocabulary_size, node_count * gate_count], -0.1, 0.1))\n",
    "        output_weights = tf.Variable(tf.truncated_normal([node_count, node_count * gate_count], -0.1, 0.1))\n",
    "        bias = tf.Variable(tf.zeros([1, node_count * gate_count]))\n",
    "        # Variables saving state across unrollings.\n",
    "        saved_output = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        saved_state = tf.Variable(tf.zeros([batch_size, node_count]), trainable=False)\n",
    "        # Classifier weights and biases.\n",
    "        w = tf.Variable(tf.truncated_normal([node_count, vocabulary_size], -0.1, 0.1))\n",
    "        b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        # Definition of the cell computation.\n",
    "        def lstm_cell(i, o, state):\n",
    "            \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "            Note that in this formulation, we omit the various connections between the\n",
    "            previous state and the gates.\"\"\"\n",
    "            values = tf.split(1, gate_count, tf.matmul(i, input_weights) + tf.matmul(o, output_weights) + bias)\n",
    "            input_gate = tf.sigmoid(values[0])\n",
    "            forget_gate = tf.sigmoid(values[1])\n",
    "            update = values[2]\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            output_gate = tf.sigmoid(values[3])\n",
    "            return output_gate * tf.tanh(state), state\n",
    "\n",
    "        # Input data.\n",
    "        train_data = list()\n",
    "        for _ in range(unrolls + 1):\n",
    "            train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "        train_inputs = train_data[:unrolls]\n",
    "        train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = saved_state\n",
    "        for i in train_inputs:\n",
    "            output, state = lstm_cell(i, output, state)\n",
    "            outputs.append(output)\n",
    "\n",
    "        # State saving across unrollings.\n",
    "        with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "            # Classifier.\n",
    "            logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "        # Optimizer.\n",
    "        global_step = tf.Variable(0)\n",
    "        learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "        optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "        # Predictions.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "        # Sampling and validation eval: batch 1, no unrolling.\n",
    "        sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "        saved_sample_output = tf.Variable(tf.zeros([1, node_count]))\n",
    "        saved_sample_state = tf.Variable(tf.zeros([1, node_count]))\n",
    "        reset_sample_state = tf.group(\n",
    "            saved_sample_output.assign(tf.zeros([1, node_count])),\n",
    "            saved_sample_state.assign(tf.zeros([1, node_count]))\n",
    "        )\n",
    "        sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "        with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                      saved_sample_state.assign(sample_state)]):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    \n",
    "    return {\n",
    "        \"graph\": graph,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"unrolls\": unrolls,\n",
    "        \"train_data\": train_data,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"loss\": loss,\n",
    "        \"train_prediction\": train_prediction,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"reset_sample_state\": reset_sample_state,\n",
    "        \"sample_input\": sample_input,\n",
    "        \"sample_prediction\": sample_prediction,\n",
    "        \"output_to_input\": make_one_hot,\n",
    "        \"random_output\": random_single_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-93-b3492f8327e7>:6 in run_lstm.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296253 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "lfnfewalkvsoekr e eg jeset lj ty n t rn uizplwxiizyxeazkmucollas cu  nto s pe ci\n",
      "txrtlhsxo ewzr k dnqdk pn a anigcd e nut tbos oed  aueoellatnibdnsyo vvnd pjnhud\n",
      "pdttfor  cj gbvogvfsei f  loidfi  p aifa e murzrpewg fgnbrspmejpahhoitdgttjswa w\n",
      "vaaltvoevkye on   xnvtj koczambntwhwyfiat esroa  usiith wrm ek qekzfl rizchmacx \n",
      "gspoaqi mkx ttnu i z bmslaecnhbddil int p twvodamni tfdsyngeubof zvlbvlbrvlebeoe\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 100: 2.598242 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 10.43\n",
      "Validation set perplexity: 10.31\n",
      "Average loss at step 200: 2.264170 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.99\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 300: 2.097771 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 400: 1.993353 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 500: 1.934046 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.905810 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.853493 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.815533 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.826053 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.822181 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "zer agains the cadrem a sint simpera diver the counting hidis copper tempulin th\n",
      "wal limmas that beproped necommentes along only to strate staticas sesean mar ma\n",
      "itty the pucal the nentuan nigevo spruasidation procidity uperat yarcele actband\n",
      "er his hore semenis squtria one nine eight nine eight count usam ppland anablets\n",
      "famated of as s cuty is to the jradth celts tinly afficion other knathorod in a \n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1100: 1.773144 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.753127 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.732521 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1400: 1.741630 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1500: 1.734352 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.744094 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1700: 1.707435 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1800: 1.665948 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1900: 1.642286 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2000: 1.692420 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "with midy orfiasns montgules anarchics repextropy fanchum sodiinson peeces only \n",
      "nepleb in zero not wordable language praptics any mayorm unister sold the schasi\n",
      "lusting and the evis of dispraying and from for from hasin constinuing hegp in m\n",
      "vee onzee of the thougher alled the putoreneenal no externald successon in natre\n",
      "am onch onten four zero zero zero in one nine zero six seven zero eight three th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2100: 1.684797 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2200: 1.672977 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.644694 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2400: 1.659281 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.681113 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2600: 1.649613 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2700: 1.657550 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.647320 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.650052 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3000: 1.649753 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 6.11\n",
      "================================================================================\n",
      "finse lomis carlations throush remation of lonist is and for to trual zandsor wi\n",
      "guens mail dividation apringered in the cleet a photion to be his ipations in th\n",
      "ple conseeter to tikely undov a trussy instroush intend s billing the germainty \n",
      "k of threes signs permoptes leashy history of honts at is to the sdanchannish or\n",
      "side fo m game benthers as rourers which histy hore setring desslaze panters rec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3100: 1.624195 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3200: 1.646695 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.633128 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3400: 1.666269 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.652852 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3600: 1.669068 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.643963 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.639580 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3900: 1.637013 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.650672 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "mariand of vicrotabova was there nagus freads manne by in appearentan ordernied \n",
      "noms team includiers euro galf organe kore egro d stanso and in aver and comport\n",
      "zents the site ngardo is the norder there at cincondalan for alop other one nine\n",
      "mmalist have anzides inflands sucter one plizablablations ey calemed that one ei\n",
      "st six acting bugge inlow vick moved be means and in and develater bylan filios \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4100: 1.631040 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.634746 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4300: 1.609655 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.607357 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4500: 1.611623 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4600: 1.615156 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.621225 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.631753 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4900: 1.625988 learning rate: 10.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.604216 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "bobutric india subservent be yohn oppond westay his releaser hendsen due to six \n",
      "den best bitter seecraluls thebethong the rboly one nine two restentraching econ\n",
      "cayed by vest rushail cornotons indiar write his as one nine eight about ober fe\n",
      "x reseat partiated hits interasery a singer one eight birtho the atigorg tive on\n",
      "stin sometime el hai diantemmbic consedvation was suncepted imbolitters a entars\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.599899 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5200: 1.592921 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.578034 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5400: 1.576043 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.567655 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.577844 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.572687 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.579512 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.581735 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.545022 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.37\n",
      "================================================================================\n",
      "tias on the lock kersnoving actustive and corrs strongs over of a viced chanded \n",
      "jous in encicest since from the codering checito new l penerame in the m western\n",
      "xing often sayab to war alvie undic od albusha topon war largest jund be on used\n",
      "broo impneistic of dispressing and use reado his escorders indequre a unay book \n",
      "n develiof itala not flemtorian a imptor a livize school drags and togriginch de\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.563284 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.534037 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.539389 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.539392 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.556489 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6600: 1.592459 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.580084 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.601905 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6900: 1.577956 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.577044 learning rate: 1.000000\n",
      "(640, 27)\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "more hurevafroop in admict notictly partidal of puncenose by a flow light and in\n",
      "y in pencionsly stander of exampincianntertiat in accoin officance it the modele\n",
      "estention on on for the laturing governman weilsila extentantier fealleh of a me\n",
      "kleryat than the anzing a defen for concressing of the sning objactes is there h\n",
      "as confined of he one nine two three two one four johnet as legs one nine zero s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n"
     ]
    }
   ],
   "source": [
    "compact_setup = construct_compact(node_count=64, batch_size=64, unrolls=10)\n",
    "\n",
    "run_lstm(compact_setup, BatchGenerator, train_text, valid_text, step_count=7000, report_every=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists advocate social\n",
      "['ons anarchi', 'y eight inc', 'o two one n', 'ard roberts', 'cilities in']\n",
      "['ists advoca', 'ch two zero', 'nine zero t', 's joseph wh', 'ncluding ku']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGenerator(train_text, 5, 10)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(train_text[:30])\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGeneratorBigram(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size / batch_size\n",
    "    # list of offsets within batch\n",
    "    self._cursor = [ offset * segment for offset in xrange(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int)  # id of char to be embedded\n",
    "    for b in xrange(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]]) # get id of a char\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size  # move cursor\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in xrange(self._num_unrollings):\n",
    "      batches.append(self._next_batch())  # add id of char for 1 to num_unrollings\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigrambatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into string\n",
    "  representation.\n",
    "  \"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, [id2char(c) for c in b])]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigram(valid_text, 1, 1) # returns batch size 1, +1 unrolling\n",
    "train_labels = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_labels = BatchGenerator(valid_text, 1, 1) # returns batch size 1, +1 unrolling \n",
    "\n",
    "\n",
    "print(bigrambatches2string(train_batches.next()))\n",
    "print(bigrambatches2string(train_batches.next()))\n",
    "print(bigrambatches2string(valid_batches.next()))\n",
    "print(bigrambatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 128, 64]\n",
      "logits [640, 27]\n",
      "labels [640, 27]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "#vocabulary_size = (len(string.ascii_lowercase) + 1)**2\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  ## Parameters:\n",
    "  fico_x = tf.Variable(tf.truncated_normal([4, embedding_size, num_nodes], -0.1, 0.1))\n",
    "  print(fico_x.get_shape().as_list())\n",
    "  fico_m = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fico_b = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Embedding Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=False)\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "    \"\"\"                   \n",
    "    i_list = tf.pack([i, i, i, i])\n",
    "    o_list = tf.pack([o, o, o, o])\n",
    "                          \n",
    "    ins = tf.batch_matmul(i_list, fico_x)\n",
    "    outs = tf.batch_matmul(o_list, fico_m)\n",
    "    \n",
    "    h_x = ins + outs + fico_b\n",
    "\n",
    "    forget_gate = tf.sigmoid(h_x[0,:,:])\n",
    "\n",
    "    input_gate = tf.sigmoid(h_x[1,:,:])\n",
    "    update = tf.tanh(h_x[2,:,:])\n",
    "    state = forget_gate*state + input_gate*update\n",
    "    \n",
    "    output_gate = tf.sigmoid(h_x[3,:,:])\n",
    "    \n",
    "    h = output_gate * tf.tanh(state)\n",
    "    return h, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_data_y = list()\n",
    "  for _ in xrange(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))  # removed ohe of char\n",
    "    train_data_y.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))  # uses ohe of char\n",
    "  train_labels = train_data_y[1:]\n",
    "  \n",
    "  # Embedded input data\n",
    "  encoded_inputs = list()\n",
    "  for bigram in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram)\n",
    "    encoded_inputs.append(embed)\n",
    "  train_inputs = encoded_inputs[:num_unrollings]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    print('logits', logits.get_shape().as_list())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "    print('labels', tf.concat(0, train_labels).get_shape().as_list())\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=False)  ## orig 10.0, 5000, 0.1, True\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1]) # removed ohe of char\n",
    "  sample_input_emb = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_emb, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-101-1befe6826d9f>:13 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "==========\n",
      "Average loss at step 0 = 3.30546188354 \n",
      "learning rate: 10.0\n",
      "Minibatch perplexity: 27.26\n",
      "================================================================================\n",
      "uk a tlnd rlf pgnnrntuceryvlsbb twgximhdixbsnnsa otayftdwe vsea te mjxrsluaaeo e\n",
      "ujjmnkenpebzla ceg ahpo o nnfss zabygdwavr s tiadhrbnso a  mtaooeheydno soshgbss\n",
      "t n ktawpiau rbla t nvtexbrwpagrtsnwm nycrqwdu u anfxzrjvlnhy v vy  a j  cnzktan\n",
      "qrbe  sfhmrk  l   mepi abo jatf wlf rquioiohl d mwsxmgnye qwmbvcxgwrvbrgiprm t o\n",
      "kosjmhhsjwo  itnpgwvbfgczhgiefadhi o dtemlhs wcvrhicf cbqhcxbtthnrczjpdeaxo xhrl\n",
      "================================================================================\n",
      "Validation set perplexity: 18.79\n",
      "------------------------------\n",
      "Average loss at step 500 = 9.92722220778 \n",
      "learning rate: 7.94328\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.94\n",
      "------------------------------\n",
      "Average loss at step 1000 = 8.84984380364 \n",
      "learning rate: 6.30957\n",
      "Minibatch perplexity: 5.91\n",
      "================================================================================\n",
      "i lesitation bhat statc the mearge one one two zero th ethre two monian kazas a \n",
      "legat maritality to kulctan the at homeative of civil micturive batle commum a b\n",
      "thel hind the arf in include distanders the and larto the wyhil co nication aya \n",
      "ry conced it the bolated mukacture was golder to are atering the raiton mikhs el\n",
      "y as kmencalite has infinituind was rixides that high mail nine nine over konsto\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "------------------------------\n",
      "Average loss at step 1500 = 8.45089272738 \n",
      "learning rate: 5.01187\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.07\n",
      "------------------------------\n",
      "Average loss at step 2000 = 8.24617858291 \n",
      "learning rate: 3.98107\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "s aner one nine engy three no s compelied linumoniced ofter tradage heborlap hea\n",
      "centries zero zero zero powesason mack was dislace studing the unbhilic of locul\n",
      "ver of the unit airmum defined the lain to have by to dieligion fact fromant gre\n",
      "ate two exist of the induction dacayclistten to the is force gork of amainual we\n",
      "us peral of communion of maname i heather ins the in group in one eight stilting\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "------------------------------\n",
      "Average loss at step 2500 = 8.13867776752 \n",
      "learning rate: 3.16228\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.38\n",
      "------------------------------\n",
      "Average loss at step 3000 = 8.0884043479 \n",
      "learning rate: 2.51189\n",
      "Minibatch perplexity: 5.78\n",
      "================================================================================\n",
      "gin s pattro storges lemarform one by and mahavan of the games arex weden mort s\n",
      "qually the inconnect y stil pader to rolestwint my servity loca fact and way a p\n",
      "it and don sequent six germataiz gram upture a s mork halp yarnynation to the bo\n",
      "pat diging dieh when by peoply legame beer was dread eproter in a carsmation rev\n",
      "berian take of a syrede aciston abloods landmet domans and the publiar regarch a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "------------------------------\n",
      "Average loss at step 3500 = 8.08294803977 \n",
      "learning rate: 1.99526\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.33\n",
      "------------------------------\n",
      "Average loss at step 4000 = 8.1140245235 \n",
      "learning rate: 1.58489\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "x six zero four two tradiguanoly not playafications in theres on milelt by music\n",
      " nation s retend todoth birrands of john workout also and decificonators for the\n",
      "yantware kink eurogram of the earte a fom it algided rolator in use smallay at w\n",
      "way black river four s rejectived on hexiss on the directions liberation contran\n",
      " stances and world dources clased aleechnows haderoth nelpate or boxing five occ\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "------------------------------\n",
      "Average loss at step 4500 = 7.99187282205 \n",
      "learning rate: 1.25893\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.29\n",
      "------------------------------\n",
      "Average loss at step 5000 = 8.04201383114 \n",
      "learning rate: 1.0\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ka suffiet of is except rops in the of commish used sootest defistination wels a\n",
      "human convention to seven nonciation of region of sconx froninic to zero three f\n",
      "jances of gland outher isartith name simes logazemed in tuccause e isa rupistmat\n",
      "s the mode seen day gro alon s his composence hand saw mermiter how sakical libe\n",
      "lost and visit vologis whites couthourum back groonkbeal was teal as in the ran \n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n",
      "------------------------------\n",
      "Average loss at step 5500 = 7.90461976171 \n",
      "learning rate: 0.794328\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.28\n",
      "------------------------------\n",
      "Average loss at step 6000 = 7.84907930851 \n",
      "learning rate: 0.630957\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "pleneds the formest the rough theughlim are for that zero four jen but orga bd o\n",
      "f an a years kaymary provination to be follow beer obisholisadio mays few reposi\n",
      "fined it in the souther and way to the serm niceramirally wenest song their the \n",
      "le were astrist reqvenlate dixextry spire sen game cauto cal cornition tradie is\n",
      "ker dramin from a yearly simissed tradition objintabs brutionaded with mussifien\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "------------------------------\n",
      "Average loss at step 6500 = 7.73921801567 \n",
      "learning rate: 0.501187\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.22\n",
      "------------------------------\n",
      "Average loss at step 7000 = 7.92613475323 \n",
      "learning rate: 0.398107\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "jecting palethereakal many range purber the pressumente the charactels j himsuta\n",
      "ach celitieses to of argeritizs jequallers s are ort arme he frag a far effectio\n",
      "an stand race airounhamaging untilar soon west thoughry on when priving one nine\n",
      "e the timater a hume mornial to much wightly corcluday years that theoro subser \n",
      "y of has which that the elop notans is a members one eight nine eight zero zero \n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "------------------------------\n",
      "1.73433958292 minutes elapsed\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# training and validation batches\n",
    "train_batches = BatchGeneratorBigram(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigram(valid_text, 1, 1) # returns batch size 1, +1 unrolling\n",
    "train_batches_y = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_y = BatchGenerator(valid_text, 1, 1) # returns batch size 1, +1 unrolling \n",
    "\n",
    "num_steps = 7001  ## orig 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "t0 = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized\\n==========')\n",
    "  mean_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    batches_y = train_batches_y.next()\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    for i in xrange(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      feed_dict[train_data_y[i]] = batches_y[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % (5.*summary_frequency) == 0:  ## orig 2.5*summary_frequency\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step', step, '=', mean_loss, '\\nlearning rate:', lr)\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches_y)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in xrange(5):\n",
    "          #feed = sample(random_distribution())  # random vector\n",
    "          feed = np.random.randint(27, size=[1])#.astype('int32')\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = id2char(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in xrange(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)  # get ohe of predicted proba\n",
    "            feed = np.array([np.argmax(feed)])  # get id of predicted char\n",
    "            sentence += id2char(feed)  # add predicted char\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in xrange(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        b_y = valid_batches_y.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b_y[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "      print('-' * 30)\n",
    "# show how much time elapsed\n",
    "print((time.time()-t0)/60., 'minutes elapsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/Users/alex/anaconda2/lib/python27.zip', '/Users/alex/anaconda2/lib/python2.7', '/Users/alex/anaconda2/lib/python2.7/plat-darwin', '/Users/alex/anaconda2/lib/python2.7/plat-mac', '/Users/alex/anaconda2/lib/python2.7/plat-mac/lib-scriptpackages', '/Users/alex/anaconda2/lib/python2.7/lib-tk', '/Users/alex/anaconda2/lib/python2.7/lib-old', '/Users/alex/anaconda2/lib/python2.7/lib-dynload', '/Users/alex/anaconda2/lib/python2.7/site-packages', '/Users/alex/anaconda2/lib/python2.7/site-packages/Sphinx-1.4.6-py2.7.egg', '/Users/alex/anaconda2/lib/python2.7/site-packages/aeosa', '/Users/alex/anaconda2/lib/python2.7/site-packages/setuptools-27.2.0-py2.7.egg', '/Users/alex/anaconda2/lib/python2.7/site-packages/IPython/extensions', '/Users/alex/.ipython', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models', '/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(tf.__path__[0] + '/models')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.models.rnn.translate.seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in xrange(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "        #if np.all(is_finished):\n",
    "            #break\n",
    "    print(decoder_inputs)\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = (len(words) / batch_size) * batch_size\n",
    "    correct = 0\n",
    "    for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in xrange(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 135, in seq2seq_f\n    dtype=dtype)\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 171, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 172, in __init__\n    softmax_loss_function=softmax_loss_function)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-16ac2dcd9de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time reverse_text(15000)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-eca1df7923fd>\u001b[0m in \u001b[0;36mreverse_text\u001b[0;34m(nb_steps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreverse_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-131-a72cdcac077c>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    \u001b[0mlearning_rate_decay_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                    \u001b[0muse_lstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                    forward_only=False)\n\u001b[0m",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor, use_lstm, num_samples, forward_only, dtype)\u001b[0m\n\u001b[1;32m    170\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m           \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m           softmax_loss_function=softmax_loss_function)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# Gradients and SGD update operation for training the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36mmodel_with_buckets\u001b[0;34m(encoder_inputs, decoder_inputs, targets, weights, buckets, seq2seq, softmax_loss_function, per_example_loss, name)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                                          reuse=True if j > 0 else None):\n\u001b[1;32m   1120\u001b[0m         bucket_outputs, _ = seq2seq(encoder_inputs[:bucket[0]],\n\u001b[0;32m-> 1121\u001b[0;31m                                     decoder_inputs[:bucket[1]])\n\u001b[0m\u001b[1;32m   1122\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mper_example_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    169\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m           \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq2seq_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m           softmax_loss_function=softmax_loss_function)\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.pyc\u001b[0m in \u001b[0;36mseq2seq_f\u001b[0;34m(encoder_inputs, decoder_inputs, do_decode)\u001b[0m\n\u001b[1;32m    133\u001b[0m           \u001b[0moutput_projection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m           \u001b[0mfeed_previous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_decode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m           dtype=dtype)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# Feeds for inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/seq2seq.pyc\u001b[0m in \u001b[0;36membedding_attention_seq2seq\u001b[0;34m(encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size, num_heads, output_projection, feed_previous, dtype, scope, initial_state_attention)\u001b[0m\n\u001b[1;32m    815\u001b[0m         embedding_size=embedding_size)\n\u001b[1;32m    816\u001b[0m     encoder_outputs, encoder_state = rnn.rnn(\n\u001b[0;32m--> 817\u001b[0;31m         encoder_cell, encoder_inputs, dtype=dtype)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;31m# First calculate a concatenation of encoder outputs to put attention on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m    224\u001b[0m             state_size=cell.state_size)\n\u001b[1;32m    225\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m       \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvarscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# pylint: disable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;31m# pylint: enable=cell-var-from-loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             dtype=data_type)\n\u001b[0m\u001b[1;32m    754\u001b[0m         embedded = embedding_ops.embedding_lookup(\n\u001b[1;32m    755\u001b[0m             embedding, array_ops.reshape(inputs, [-1]))\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m   1022\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    848\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    344\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    329\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    630\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 632\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    633\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable embedding_attention_seq2seq/RNN/EmbeddingWrapper/embedding already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 135, in seq2seq_f\n    dtype=dtype)\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 171, in <lambda>\n    lambda x, y: seq2seq_f(x, y, False),\n  File \"/Users/alex/anaconda2/lib/python2.7/site-packages/tensorflow/models/rnn/translate/seq2seq_model.py\", line 172, in __init__\n    softmax_loss_function=softmax_loss_function)\n"
     ]
    }
   ],
   "source": [
    "%time reverse_text(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
